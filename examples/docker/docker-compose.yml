services:
  haiku-rag:
    build:
      context: ../..
      dockerfile: docker/Dockerfile
    container_name: haiku-rag
    ports:
      - "8000:8000"  # A2A server
      - "8001:8001"  # MCP server
    volumes:
      - ./data:/data  # Persist database
      - ./docs:/docs  # Mount documents directory for monitoring
    environment:
      # Database
      - HAIKU_RAG_DB_PATH=/data/haiku.rag.lancedb

      # File monitoring
      - MONITOR_DIRECTORIES=/docs

      # Set the Ollama base url for Ollama defaults
      - OLLAMA_BASE_URL=http://host.docker.internal:11434

      # Embeddings provider (choose one)
      # For OpenAI:
      # - EMBEDDINGS_PROVIDER=openai
      # - EMBEDDINGS_MODEL=text-embedding-3-small
      # - OPENAI_API_KEY=your-key-here

      # For VoyageAI:
      # - EMBEDDINGS_PROVIDER=voyageai
      # - EMBEDDINGS_MODEL=voyage-3
      # - VOYAGE_API_KEY=your-key-here

      # QA provider (uses Pydantic AI)
      # - QA_PROVIDER=ollama

      # For OpenAI:
      # - QA_PROVIDER=openai
      # - QA_MODEL=gpt-4o-mini
      # - OPENAI_API_KEY=your-key-here

      # Reranking (optional)
      # - RERANKING_PROVIDER=mxbai
      # - RERANKING_MODEL=mixedbread-ai/mxbai-rerank-large-v1
      # - RERANKING_BASE_URL=http://host.docker.internal:11434

      # Research agent (optional, defaults to QA provider/model)
      # - RESEARCH_PROVIDER=openai
      # - RESEARCH_MODEL=gpt-4o

      # Environment
      - ENV=production

    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
