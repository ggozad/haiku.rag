{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"haiku.rag","text":"<p><code>haiku.rag</code> is a Retrieval-Augmented Generation (RAG) library built to work with LanceDB as a local vector database. It uses LanceDB for storing embeddings and performs semantic (vector) search as well as full-text search combined through native hybrid search with Reciprocal Rank Fusion. Both open-source (Ollama, MixedBread AI) as well as commercial (OpenAI, VoyageAI) embedding providers are supported.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Local LanceDB: No need to run additional servers</li> <li>Support for various embedding providers: Ollama, VoyageAI, OpenAI or add your own</li> <li>Native Hybrid Search: Vector search combined with full-text search using native LanceDB RRF reranking</li> <li>Reranking: Optional result reranking with MixedBread AI or Cohere</li> <li>Question Answering: Built-in QA agents using Ollama, OpenAI, or Anthropic</li> <li>File monitoring: Automatically index files when run as a server</li> <li>Extended file format support: Parse 40+ file formats including PDF, DOCX, HTML, Markdown, code files and more. Or add a URL!</li> <li>MCP server: Exposes functionality as MCP tools</li> <li>A2A agent: Conversational agent with context and multi-turn dialogue support</li> <li>CLI commands: Access all functionality from your terminal</li> <li>Add sources from text, files, or URLs, optionally with a human\u2011readable title</li> <li>Python client: Call <code>haiku.rag</code> from your own python applications</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install haiku.rag:</p> <pre><code>uv pip install haiku.rag\n</code></pre> <p>Use from Python:</p> <pre><code>from haiku.rag.client import HaikuRAG\n\nasync with HaikuRAG(\"database.lancedb\") as client:\n    # Add a document\n    doc = await client.create_document(\"Your content here\")\n\n    # Search documents\n    results = await client.search(\"query\")\n\n    # Ask questions\n    answer = await client.ask(\"Who is the author of haiku.rag?\")\n</code></pre> <p>Or use the CLI:</p> <pre><code>haiku-rag add \"Your document content\"\nhaiku-rag add \"Your document content\" --meta author=alice\nhaiku-rag add-src /path/to/document.pdf --title \"Q3 Financial Report\" --meta source=manual\nhaiku-rag search \"query\"\nhaiku-rag ask \"Who is the author of haiku.rag?\"\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting started - Tutorial</li> <li>Installation - Install haiku.rag with different providers</li> <li>Configuration - Environment variables and settings</li> <li>CLI - Command line interface usage</li> <li>Server - File monitoring and server mode</li> <li>MCP - Model Context Protocol integration</li> <li>A2A - Agent-to-Agent conversational protocol</li> <li>Python - Python API reference</li> <li>Agents - QA agent and multi-agent research</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"a2a/","title":"Agent-to-Agent (A2A) Protocol","text":"<p>The A2A server exposes <code>haiku.rag</code> as a conversational agent using the Agent-to-Agent protocol. Unlike the MCP server which provides stateless tools, the A2A agent maintains conversation history and context across multiple turns.</p>"},{"location":"a2a/#features","title":"Features","text":"<ul> <li>Conversational Context: Maintains full conversation history including tool calls and results</li> <li>Multi-turn Dialogue: Supports follow-up questions with pronoun resolution (\"he\", \"it\", \"that document\")</li> <li>Intelligent Search: Performs single or multiple searches depending on question complexity</li> <li>Source Citations: Always includes sources with both titles and URIs</li> <li>Full Document Retrieval: Can fetch complete documents on request</li> <li>Multiple Skills: Exposes three distinct skills with appropriate artifacts:</li> <li><code>document-qa</code>: Conversational question answering (default)</li> <li><code>document-search</code>: Semantic search with structured results</li> <li><code>document-retrieve</code>: Fetch complete documents by URI</li> </ul>"},{"location":"a2a/#starting-a2a-server","title":"Starting A2A Server","text":"<pre><code>haiku-rag serve --a2a\n</code></pre> <p>Server options: - <code>--a2a-host</code> - Host to bind to (default: 127.0.0.1) - <code>--a2a-port</code> - Port to bind to (default: 8000)</p> <p>Example: <pre><code>haiku-rag serve --a2a --a2a-host 0.0.0.0 --a2a-port 8080\n</code></pre></p>"},{"location":"a2a/#interactive-a2a-client","title":"Interactive A2A Client","text":"<p>Note</p> <p>The interactive A2A client is an excellent way to do conversational research with <code>haiku.rag</code>.</p> <p>Test and interact with haiku.rag's A2A server using the built-in interactive client:</p> <pre><code>haiku-rag a2aclient\n</code></pre> <p>Client options: - <code>--url</code> - Base URL of the A2A server (default: http://localhost:8000)</p> <p>Example: <pre><code># Connect to local server\nhaiku-rag a2aclient\n\n# Connect to remote server\nhaiku-rag a2aclient --url https://example.com:8000\n</code></pre></p> <p>The interactive client provides:</p> <ul> <li>Rich markdown rendering of agent responses</li> <li>Conversation context across multiple turns</li> <li>Agent card discovery and display</li> <li>Compact artifact summaries</li> </ul>"},{"location":"a2a/#requirements","title":"Requirements","text":"<p>A2A support requires the <code>a2a</code> extra:</p> <pre><code>uv pip install 'haiku.rag[a2a]'\n</code></pre>"},{"location":"a2a/#python-usage","title":"Python Usage","text":"<pre><code>from pathlib import Path\nfrom haiku.rag.a2a import create_a2a_app\nimport uvicorn\n\n# Create A2A app\napp = create_a2a_app(Path(\"database.lancedb\"))\n\n# Run with uvicorn\nuvicorn.run(app, host=\"127.0.0.1\", port=8000)\n</code></pre> <p>This installs the <code>fasta2a</code> package and its dependencies.</p>"},{"location":"a2a/#architecture","title":"Architecture","text":"<p>The A2A agent uses:</p> <ul> <li>FastA2A: Python framework implementing the A2A protocol</li> <li>Pydantic AI: Agent framework with tool support</li> <li>In-Memory Storage: Context and message history storage (persists during server lifetime)</li> <li>Conversation State: Full pydantic-ai message history serialized in A2A context</li> </ul>"},{"location":"a2a/#message-history","title":"Message History","text":"<p>The agent stores the complete conversation state including:</p> <ul> <li>User prompts</li> <li>Agent responses</li> <li>Tool calls and their arguments</li> <li>Tool return values</li> </ul> <p>This enables the agent to:</p> <ul> <li>Reference previous searches</li> <li>Understand pronouns and context</li> <li>Maintain coherent multi-turn conversations</li> </ul>"},{"location":"a2a/#context-management","title":"Context Management","text":"<p>Each conversation is identified by a <code>context_id</code>. All messages within the same context share conversation history. This allows the agent to:</p> <ul> <li>Remember what was discussed</li> <li>Track which documents were already found</li> <li>Provide contextual follow-up answers</li> </ul>"},{"location":"a2a/#skills","title":"Skills","text":"<p>The agent exposes three skills:</p> <ul> <li>document-qa (default): Conversational question answering including follow-ups and multi-turn dialogue</li> <li>document-search: Direct semantic search returning formatted results</li> <li>document-retrieve: Fetch complete document content by URI</li> </ul>"},{"location":"a2a/#artifacts","title":"Artifacts","text":"<p>All operations create artifacts for traceability:</p> <ul> <li> <p>search_results: Created for each <code>search_documents</code> tool call</p> </li> <li> <p>Contains query and array of SearchResult objects (content, score, document_title, document_uri)</p> </li> <li> <p>document: Created for each <code>get_full_document</code> tool call</p> </li> <li> <p>Contains complete document text</p> </li> <li> <p>qa_result: Created for all document-qa operations</p> </li> <li> <p>Contains question, answer, and skill identifier</p> </li> <li>Always created for Q&amp;A, even when answering from conversation history without tools</li> </ul>"},{"location":"a2a/#memory-management","title":"Memory Management","text":"<p>To prevent memory growth, the server uses LRU (Least Recently Used) eviction:</p> <ul> <li>Maximum 1000 contexts kept in memory (configurable via <code>a2a.max_contexts</code>)</li> <li>When limit exceeded, least recently used contexts are automatically evicted</li> </ul> <p>Configure in <code>haiku.rag.yaml</code>: <pre><code>a2a:\n  max_contexts: 1000\n</code></pre></p>"},{"location":"a2a/#security","title":"Security","text":"<p>By default, the A2A agent runs without authentication. For production deployments, you should add authentication.</p>"},{"location":"a2a/#adding-authentication","title":"Adding Authentication","text":"<p>The <code>create_a2a_app()</code> function accepts optional security parameters that declare authentication requirements in the agent card:</p> <pre><code>from haiku.rag.a2a import create_a2a_app\n\napp = create_a2a_app(\n    db_path,\n    security_schemes={\n        \"apiKeyAuth\": {\n            \"type\": \"apiKey\",\n            \"in\": \"header\",\n            \"name\": \"X-API-Key\",\n            \"description\": \"API key authentication\",\n        }\n    },\n    security=[{\"apiKeyAuth\": []}],\n)\n</code></pre> <p>This populates the agent card at <code>/.well-known/agent-card.json</code> so other agents can discover your authentication requirements.</p>"},{"location":"a2a/#security-examples","title":"Security Examples","text":"<p>Three working examples are provided in <code>examples/a2a-security/</code>:</p> <ol> <li>API Key (<code>apikey_example.py</code>) - Simple header-based authentication</li> <li>OAuth2 GitHub (<code>oauth2_github.py</code>) - GitHub Personal Access Token authentication</li> <li>OAuth2 Enterprise (<code>oauth2_example.py</code>) - Full OAuth2 with JWT verification</li> </ol> <p>Each example shows:</p> <ul> <li>How to declare security in the agent card</li> <li>How to implement authentication middleware</li> <li>How to verify credentials</li> </ul>"},{"location":"agents/","title":"Agents","text":""},{"location":"agents/#agents","title":"Agents","text":"<p>Three agentic flows are provided by haiku.rag:</p> <ul> <li>Simple QA Agent \u2014 a focused question answering agent</li> <li>Deep QA Agent \u2014 multi-agent question decomposition for complex questions</li> <li>Research Multi\u2011Agent \u2014 a multi\u2011step, analyzable research workflow</li> </ul> <p>For an interactive example using Pydantic AI and AG-UI, see the Interactive Research Assistant example (demo video). The demo uses a knowledge base containing haiku.rag's code and documentation.</p>"},{"location":"agents/#simple-qa-agent","title":"Simple QA Agent","text":"<p>The simple QA agent answers a single question using the knowledge base. It retrieves relevant chunks, optionally expands context around them, and asks the model to answer strictly based on that context.</p> <p>Key points:</p> <ul> <li>Uses a single <code>search_documents</code> tool to fetch relevant chunks</li> <li>Can be run with or without inline citations in the prompt (citations prefer   document titles when present, otherwise URIs)</li> <li>Returns a plain string answer</li> </ul> <p>Python usage:</p> <pre><code>from haiku.rag.client import HaikuRAG\nfrom haiku.rag.qa.agent import QuestionAnswerAgent\n\nclient = HaikuRAG(path_to_db)\n\n# Choose a provider and model (see Configuration for env defaults)\nagent = QuestionAnswerAgent(\n    client=client,\n    provider=\"openai\",  # or \"ollama\", \"vllm\", etc.\n    model=\"gpt-4o-mini\",\n    use_citations=False,  # set True to bias prompt towards citing sources\n)\n\nanswer = await agent.answer(\"What is climate change?\")\nprint(answer)\n</code></pre>"},{"location":"agents/#deep-qa-agent","title":"Deep QA Agent","text":"<p>Deep QA is a multi-agent system that decomposes complex questions into sub-questions, answers them in batches, evaluates sufficiency, and iterates if needed before synthesizing a final answer. It's lighter than the full research workflow but more powerful than the simple QA agent.</p> <pre><code>---\ntitle: Deep QA graph\n---\nstateDiagram-v2\n  DeepQAPlanNode --&gt; DeepQASearchDispatchNode\n  DeepQASearchDispatchNode --&gt; DeepQADecisionNode\n  DeepQADecisionNode --&gt; DeepQASearchDispatchNode\n  DeepQADecisionNode --&gt; DeepQASynthesizeNode\n  DeepQASynthesizeNode --&gt; [*]</code></pre> <p>Key nodes:</p> <ul> <li>Plan: Decomposes the question into focused sub-questions</li> <li>Search (batched): Answers sub-questions in parallel batches (respects max_concurrency)</li> <li>Decision: Evaluates if we have sufficient information or need another iteration</li> <li>Synthesize: Generates the final comprehensive answer</li> </ul> <p>Key differences from Research:</p> <ul> <li>Simpler evaluation: Uses sufficiency check (not confidence + insight analysis)</li> <li>Direct answers: Returns just the answer (not a full research report)</li> <li>Question-focused: Optimized for answering specific questions, not open-ended research</li> <li>Supports citations: Can include inline source citations like <code>[document.md]</code></li> <li>Configurable iterations: Control max_iterations (default: 2) and max_concurrency (default: 3)</li> </ul> <p>CLI usage:</p> <pre><code># Deep QA without citations\nhaiku-rag ask \"What are the main features of haiku.rag?\" --deep\n\n# Deep QA with citations\nhaiku-rag ask \"What are the main features of haiku.rag?\" --deep --cite\n</code></pre> <p>Python usage:</p> <pre><code>from haiku.rag.client import HaikuRAG\nfrom haiku.rag.qa.deep.dependencies import DeepQAContext\nfrom haiku.rag.qa.deep.graph import build_deep_qa_graph\nfrom haiku.rag.qa.deep.nodes import DeepQAPlanNode\nfrom haiku.rag.qa.deep.state import DeepQADeps, DeepQAState\n\nasync with HaikuRAG(path_to_db) as client:\n    graph = build_deep_qa_graph()\n    context = DeepQAContext(\n        original_question=\"What are the main features of haiku.rag?\",\n        use_citations=True\n    )\n    state = DeepQAState(\n        context=context,\n        max_sub_questions=3,\n        max_iterations=2,\n        max_concurrency=3\n    )\n    deps = DeepQADeps(client=client)\n\n    result = await graph.run(\n        start_node=DeepQAPlanNode(provider=\"openai\", model=\"gpt-4o-mini\"),\n        state=state,\n        deps=deps\n    )\n\n    print(result.output.answer)\n    print(result.output.sources)\n</code></pre>"},{"location":"agents/#research-graph","title":"Research Graph","text":"<p>The research workflow is implemented as a typed pydantic\u2011graph. It plans, searches (in parallel batches), evaluates, and synthesizes into a final report \u2014 with clear stop conditions and shared state.</p> <pre><code>---\ntitle: Research graph\n---\nstateDiagram-v2\n  PlanNode --&gt; SearchDispatchNode\n  SearchDispatchNode --&gt; AnalyzeInsightsNode\n  AnalyzeInsightsNode --&gt; DecisionNode\n  DecisionNode --&gt; SearchDispatchNode\n  DecisionNode --&gt; SynthesizeNode\n  SynthesizeNode --&gt; [*]</code></pre> <p>Key nodes:</p> <ul> <li>Plan: builds up to 3 standalone sub\u2011questions (uses an internal presearch tool)</li> <li>Search (batched): answers sub\u2011questions using the KB with minimal, verbatim context</li> <li>Analyze: aggregates fresh insights, updates gaps, and suggests new sub-questions</li> <li>Decision: checks sufficiency/confidence thresholds and chooses whether to iterate</li> <li>Synthesize: generates a final structured report</li> </ul> <p>Primary models:</p> <ul> <li><code>SearchAnswer</code> \u2014 one per sub\u2011question (query, answer, context, sources)</li> <li><code>InsightRecord</code> / <code>GapRecord</code> \u2014 structured tracking of findings and open issues</li> <li><code>InsightAnalysis</code> \u2014 output of the analysis stage (insights, gaps, commentary)</li> <li><code>EvaluationResult</code> \u2014 insights, new questions, sufficiency, confidence</li> <li><code>ResearchReport</code> \u2014 final report (title, executive summary, findings, conclusions, \u2026)</li> </ul> <p>CLI usage:</p> <pre><code>haiku-rag research \"How does haiku.rag organize and query documents?\" \\\n  --max-iterations 2 \\\n  --confidence-threshold 0.8 \\\n  --max-concurrency 3 \\\n  --verbose\n</code></pre> <p>Python usage (blocking result):</p> <pre><code>from haiku.rag.client import HaikuRAG\nfrom haiku.rag.research import (\n    PlanNode,\n    ResearchContext,\n    ResearchDeps,\n    ResearchState,\n    build_research_graph,\n)\n\nasync with HaikuRAG(path_to_db) as client:\n    graph = build_research_graph()\n    question = \"What are the main drivers and trends of global temperature anomalies since 1990?\"\n    state = ResearchState(\n        context=ResearchContext(original_question=question),\n        max_iterations=2,\n        confidence_threshold=0.8,\n        max_concurrency=2,\n    )\n    deps = ResearchDeps(client=client)\n\n    result = await graph.run(\n        PlanNode(provider=\"openai\", model=\"gpt-4o-mini\"),\n        state=state,\n        deps=deps,\n    )\n\n    report = result.output\n    print(report.title)\n    print(report.executive_summary)\n</code></pre> <p>Python usage (streamed events):</p> <pre><code>from haiku.rag.client import HaikuRAG\nfrom haiku.rag.research import (\n    PlanNode,\n    ResearchContext,\n    ResearchDeps,\n    ResearchState,\n    build_research_graph,\n    stream_research_graph,\n)\n\nasync with HaikuRAG(path_to_db) as client:\n    graph = build_research_graph()\n    question = \"What are the main drivers and trends of global temperature anomalies since 1990?\"\n    state = ResearchState(\n        context=ResearchContext(original_question=question),\n        max_iterations=2,\n        confidence_threshold=0.8,\n        max_concurrency=2,\n    )\n    deps = ResearchDeps(client=client)\n\n    async for event in stream_research_graph(\n        graph,\n        PlanNode(provider=\"openai\", model=\"gpt-4o-mini\"),\n        state,\n        deps,\n    ):\n        if event.type == \"log\":\n            iteration = event.state.iterations if event.state else state.iterations\n            print(f\"[{iteration}] {event.message}\")\n        elif event.type == \"report\":\n            print(\"\\nResearch complete!\\n\")\n            print(event.report.title)\n            print(event.report.executive_summary)\n</code></pre>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>We use the repliqa dataset for the evaluation of <code>haiku.rag</code>.</p> <p>You can perform your own evaluations with the Typer CLI in <code>src/evaluations/benchmark.py</code>, for example <code>cd src &amp;&amp; python -m evaluations.benchmark repliqa</code>. The evaluation flow is orchestrated with <code>pydantic-evals</code>, which we leverage for dataset management, scoring, and report generation.</p>"},{"location":"benchmarks/#configuration","title":"Configuration","text":"<p>The benchmark script accepts a <code>--config</code> option to specify a custom <code>haiku.rag.yaml</code> configuration file:</p> <pre><code>cd src &amp;&amp; python -m evaluations.benchmark repliqa --config /path/to/haiku.rag.yaml\n</code></pre> <p>If no config file is specified, the script will search for a config file in the standard locations: 1. <code>./haiku.rag.yaml</code> (current directory) 2. User config directory 3. Falls back to default configuration</p> <p>You can also use command-line options: - <code>--skip-db</code> - Skip updating the evaluation database - <code>--skip-retrieval</code> - Skip retrieval benchmark - <code>--skip-qa</code> - Skip QA benchmark - <code>--qa-limit N</code> - Limit number of QA cases to evaluate</p>"},{"location":"benchmarks/#recall","title":"Recall","text":"<p>In order to calculate recall, we load the <code>News Stories</code> from <code>repliqa_3</code> (1035 documents) and index them. Subsequently, we run a search over the <code>question</code> field for each row of the dataset and check whether we match the document that answers the question. Questions for which the answer cannot be found in the documents are ignored.</p> <p>The recall obtained is ~0.79 for matching in the top result, raising to ~0.91 for the top 3 results with the \"bare\" default settings (Ollama <code>qwen3</code>, <code>mxbai-embed-large</code> embeddings, no reranking).</p> Embedding Model Document in top 1 Document in top 3 Reranker Ollama / <code>qwen3-embedding</code> 0.81 0.95 None Ollama / <code>qwen3-embedding</code> 0.91 0.98 <code>mxbai-rerank-base-v2</code> Ollama / <code>mxbai-embed-large</code> 0.79 0.91 None Ollama / <code>mxbai-embed-large</code> 0.90 0.95 <code>mxbai-rerank-base-v2</code> Ollama / <code>nomic-embed-text-v1.5</code> 0.74 0.90 None"},{"location":"benchmarks/#questionanswer-evaluation","title":"Question/Answer evaluation","text":"<p>Again using the same dataset, we use a QA agent to answer the question. <code>pydantic-evals</code> runs each case and coordinates an LLM judge (Ollama <code>qwen3</code>) to determine whether the answer is correct. The obtained accuracy is as follows:</p> Embedding Model QA Model Accuracy Reranker Ollama / <code>qwen3-embedding.</code> Ollama / <code>gpt-oss</code> 0.93 None Ollama / <code>mxbai-embed-large</code> Ollama / <code>qwen3</code> 0.85 None Ollama / <code>mxbai-embed-large</code> Ollama / <code>qwen3</code> 0.87 <code>mxbai-rerank-base-v2</code> Ollama / <code>mxbai-embed-large</code> Ollama / <code>qwen3:0.6b</code> 0.28 None <p>Note the significant degradation when very small models are used such as <code>qwen3:0.6b</code>.</p>"},{"location":"benchmarks/#wix-dataset","title":"Wix dataset","text":"<p>We also track retrieval performance on WixQA, a dataset of real customer support questions paired with curated answers from Wix. The benchmark follows the evaluation protocol described in the WixQA paper and gives us a view into how the system handles conversational, product-specific support queries.</p> <p>For retrieval evaluation, we index the reference answer passages shipped with the dataset and run retrieval against each user question. Each sample supplies one or more relevant passage URIs. We track two complementary metrics:</p> <ul> <li>Recall@K: Fraction of relevant documents retrieved in top K results. Measures coverage.</li> <li>Success@K: Fraction of queries with at least one relevant document in top K. Most relevant for RAG, where finding one good document is often sufficient.</li> </ul>"},{"location":"benchmarks/#recallk-results","title":"Recall@K Results","text":"Embedding Model Recall@1 Recall@3 Recall@5 Reranker <code>qwen3-embedding</code> 0.31 0.48 0.54 None <code>qwen3-embedding</code> 0.36 0.57 0.68 <code>mxbai-rerank-base-v2</code> <code>qwen3-embedding</code> 0.36 0.58 0.67 <code>zeroentropy</code>"},{"location":"benchmarks/#successk-results","title":"Success@K Results","text":"Embedding Model Success@1 Success@3 Success@5 Reranker <code>qwen3-embedding</code> 0.36 0.54 0.62 None <code>qwen3-embedding</code> 0.42 0.66 0.76 <code>mxbai-rerank-base-v2</code> <code>qwen3-embedding</code> 0.41 0.66 0.76 <code>zeroentropy</code>"},{"location":"benchmarks/#qa-accuracy","title":"QA Accuracy","text":"<p>And for QA accuracy,</p> Embedding Model QA Model Accuracy Reranker <code>qwen3-embedding</code> <code>gpt-oss</code> 0.75 <code>mxbai-rerank-base-v2</code>"},{"location":"cli/","title":"Command Line Interface","text":"<p>The <code>haiku-rag</code> CLI provides complete document management functionality.</p> <p>Note</p> <p>All commands support:</p> <ul> <li><code>--db</code> - Specify custom database path</li> <li><code>-h</code> - Show help for specific command</li> </ul> <p>Example: <pre><code>haiku-rag list --db /path/to/custom.db\nhaiku-rag add -h\n</code></pre></p>"},{"location":"cli/#document-management","title":"Document Management","text":""},{"location":"cli/#list-documents","title":"List Documents","text":"<pre><code>haiku-rag list\n</code></pre>"},{"location":"cli/#add-documents","title":"Add Documents","text":"<p>From text: <pre><code>haiku-rag add \"Your document content here\"\n\n# Attach metadata (repeat --meta for multiple entries)\nhaiku-rag add \"Your document content here\" --meta author=alice --meta topic=notes\n</code></pre></p> <p>From file or URL: <pre><code>haiku-rag add-src /path/to/document.pdf\nhaiku-rag add-src https://example.com/article.html\n\n# Optionally set a human\u2011readable title stored in the DB schema\nhaiku-rag add-src /mnt/data/doc1.pdf --title \"Q3 Financial Report\"\n\n# Optionally attach metadata (repeat --meta). Values use JSON parsing if possible:\n# numbers, booleans, null, arrays/objects; otherwise kept as strings.\nhaiku-rag add-src /mnt/data/doc1.pdf --meta source=manual --meta page_count=12 --meta published=true\n</code></pre></p> <p>From directory (recursively adds all supported files): <pre><code>haiku-rag add-src /path/to/documents/\n</code></pre></p> <p>Note</p> <p>When adding a directory, the same content filters configured for file monitoring are applied. This means <code>ignore_patterns</code> and <code>include_patterns</code> from your configuration will be used to filter which files are added.</p> <p>Note</p> <p>As you add documents to <code>haiku.rag</code> the database keeps growing. By default, LanceDB supports versioning of your data. Create/update operations are atomic\u2011feeling: if anything fails during chunking or embedding, the database rolls back to the pre\u2011operation snapshot using LanceDB table versioning. You can optimize and compact the database by running the vacuum command.</p>"},{"location":"cli/#get-document","title":"Get Document","text":"<pre><code>haiku-rag get 3f4a...   # document ID\n</code></pre>"},{"location":"cli/#delete-document","title":"Delete Document","text":"<pre><code>haiku-rag delete 3f4a...   # document ID\nhaiku-rag rm 3f4a...       # alias\n</code></pre> <p>Use this when you want to change things like the embedding model or chunk size for example.</p>"},{"location":"cli/#search","title":"Search","text":"<p>Basic search: <pre><code>haiku-rag search \"machine learning\"\n</code></pre></p> <p>With options: <pre><code>haiku-rag search \"python programming\" --limit 10\n</code></pre></p> <p>With filters (filter by document properties): <pre><code># Filter by URI pattern\nhaiku-rag search \"neural networks\" --filter \"uri LIKE '%arxiv%'\"\n\n# Filter by exact title\nhaiku-rag search \"transformers\" --filter \"title = 'Deep Learning Guide'\"\n\n# Combine multiple conditions\nhaiku-rag search \"AI\" --filter \"uri LIKE '%.pdf' AND title LIKE '%paper%'\"\n</code></pre></p>"},{"location":"cli/#question-answering","title":"Question Answering","text":"<p>Ask questions about your documents: <pre><code>haiku-rag ask \"Who is the author of haiku.rag?\"\n</code></pre></p> <p>Ask questions with citations showing source documents: <pre><code>haiku-rag ask \"Who is the author of haiku.rag?\" --cite\n</code></pre></p> <p>Use deep QA for complex questions (multi-agent decomposition): <pre><code>haiku-rag ask \"What are the main features and architecture of haiku.rag?\" --deep --cite\n</code></pre></p> <p>Show verbose output with deep QA: <pre><code>haiku-rag ask \"What are the main features and architecture of haiku.rag?\" --deep --verbose\n</code></pre></p> <p>The QA agent will search your documents for relevant information and provide a comprehensive answer. With <code>--cite</code>, responses include citations showing which documents were used. With <code>--deep</code>, the question is decomposed into sub-questions that are answered in parallel before synthesizing a final answer. With <code>--verbose</code> (only with <code>--deep</code>), you'll see the planning, searching, evaluation, and synthesis steps as they happen. When available, citations use the document title; otherwise they fall back to the URI.</p>"},{"location":"cli/#research","title":"Research","text":"<p>Run the multi-step research graph:</p> <pre><code>haiku-rag research \"How does haiku.rag organize and query documents?\" \\\n  --max-iterations 2 \\\n  --confidence-threshold 0.8 \\\n  --max-concurrency 3 \\\n  --verbose\n</code></pre> <p>Flags: - <code>--max-iterations, -n</code>: maximum search/evaluate cycles (default: 3) - <code>--confidence-threshold</code>: stop once evaluation confidence meets/exceeds this (default: 0.8) - <code>--max-concurrency</code>: number of sub-questions searched in parallel each iteration (default: 3) - <code>--verbose</code>: show planning, searching previews, evaluation summary, and stop reason</p> <p>When <code>--verbose</code> is set the CLI also consumes the internal research stream, printing every <code>log</code> event as agents progress through planning, search, evaluation, and synthesis. If you build your own integration, call <code>stream_research_graph</code> to access the same <code>log</code>, <code>report</code>, and <code>error</code> events and render them however you like while the graph is running.</p>"},{"location":"cli/#server","title":"Server","text":"<p>Start services (requires at least one flag): <pre><code># MCP server only (HTTP transport)\nhaiku-rag serve --mcp\n\n# MCP server (stdio transport)\nhaiku-rag serve --mcp --stdio\n\n# A2A server only\nhaiku-rag serve --a2a\n\n# File monitoring only\nhaiku-rag serve --monitor\n\n# All services\nhaiku-rag serve --monitor --mcp --a2a\n\n# Custom ports\nhaiku-rag serve --mcp --mcp-port 9000 --a2a --a2a-port 9001\n</code></pre></p> <p>See Server Mode for details on available services.</p>"},{"location":"cli/#a2a-interactive-client","title":"A2A Interactive Client","text":"<p>Connect to and chat with haiku.rag's A2A server:</p> <pre><code># Connect to local server\nhaiku-rag a2aclient\n\n# Connect to remote server\nhaiku-rag a2aclient --url https://example.com:8000\n</code></pre> <p>The interactive client provides: - Rich markdown rendering of agent responses - Multi-turn conversation with context - Agent card discovery and display - Compact artifact summaries</p> <p>See A2A documentation for more details.</p>"},{"location":"cli/#settings","title":"Settings","text":"<p>View current configuration settings: <pre><code>haiku-rag settings\n</code></pre></p>"},{"location":"cli/#maintenance","title":"Maintenance","text":""},{"location":"cli/#info-read-only","title":"Info (Read-only)","text":"<p>Display database metadata without upgrading or modifying it:</p> <pre><code>haiku-rag info [--db /path/to/your.lancedb]\n</code></pre> <p>Shows: - path to the database - stored haiku.rag version (from settings) - embeddings provider/model and vector dimension - number of documents - table versions per table (documents, chunks)</p> <p>At the end, a separate \u201cVersions\u201d section lists runtime package versions: - haiku.rag - lancedb - docling</p>"},{"location":"cli/#vacuum-optimize-and-cleanup","title":"Vacuum (Optimize and Cleanup)","text":"<p>Reduce disk usage by optimizing and pruning old table versions across all tables:</p> <pre><code>haiku-rag vacuum\n</code></pre> <p>Automatic Cleanup: Vacuum runs automatically in the background after document operations. By default, it removes versions older than 60 seconds (configurable via <code>storage.vacuum_retention_seconds</code>), preserving recent versions for concurrent connections. Manual vacuum can be useful for cleanup after bulk operations or to free disk space immediately.</p>"},{"location":"cli/#rebuild-database","title":"Rebuild Database","text":"<p>Rebuild the database by deleting all chunks &amp; embeddings and re-indexing all documents. This is useful when want to switch embeddings provider or model:</p> <pre><code>haiku-rag rebuild\n</code></pre>"},{"location":"cli/#download-models","title":"Download Models","text":"<p>Download required runtime models:</p> <pre><code>haiku-rag download-models\n</code></pre> <p>This command: - Downloads Docling OCR/conversion models (no-op if already present). - Pulls Ollama models referenced in your configuration (embeddings, QA, research, rerank).</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Configuration is done through YAML configuration files.</p> <p>Note</p> <p>If you create a db with certain settings and later change them, <code>haiku.rag</code> will detect incompatibilities (for example, if you change embedding provider) and will exit. You can rebuild the database to apply the new settings, see Rebuild Database.</p>"},{"location":"configuration/#getting-started","title":"Getting Started","text":"<p>Generate a configuration file with defaults:</p> <pre><code>haiku-rag init-config\n</code></pre> <p>This creates a <code>haiku.rag.yaml</code> file in your current directory with all available settings.</p> <p>Deprecation Notice</p> <p>Environment variable configuration via <code>.env</code> files is deprecated and will be removed in future versions. Please migrate to YAML configuration.</p> <p>To migrate from environment variables (<code>.env</code> file):</p> <pre><code>haiku-rag init-config --from-env\n</code></pre>"},{"location":"configuration/#configuration-file-locations","title":"Configuration File Locations","text":"<p><code>haiku.rag</code> searches for configuration files in this order:</p> <ol> <li>Path specified via <code>--config</code> flag: <code>haiku-rag --config /path/to/config.yaml &lt;command&gt;</code></li> <li><code>./haiku.rag.yaml</code> (current directory)</li> <li>Platform-specific user directory:<ul> <li>Linux: <code>~/.local/share/haiku.rag/config.yaml</code></li> <li>macOS: <code>~/Library/Application Support/haiku.rag/config.yaml</code></li> <li>Windows: <code>C:/Users/&lt;USER&gt;/AppData/Roaming/haiku.rag/config.yaml</code></li> </ul> </li> </ol>"},{"location":"configuration/#minimal-configuration","title":"Minimal Configuration","text":"<p>A minimal configuration file with defaults:</p> <pre><code># haiku.rag.yaml\nenvironment: production\n\nembeddings:\n  provider: ollama\n  model: qwen3-embedding\n  vector_dim: 4096\n\nqa:\n  provider: ollama\n  model: gpt-oss\n</code></pre>"},{"location":"configuration/#complete-configuration-example","title":"Complete Configuration Example","text":"<pre><code># haiku.rag.yaml\nenvironment: production\n\nstorage:\n  data_dir: \"\"  # Empty = use default platform location\n  disable_autocreate: false\n  vacuum_retention_seconds: 60\n\nmonitor:\n  directories:\n    - /path/to/documents\n    - /another/path\n  ignore_patterns: []  # Gitignore-style patterns to exclude\n  include_patterns: []  # Gitignore-style patterns to include\n\nlancedb:\n  uri: \"\"  # Empty for local, or db://, s3://, az://, gs://\n  api_key: \"\"\n  region: \"\"\n\nembeddings:\n  provider: ollama\n  model: qwen3-embedding\n  vector_dim: 4096\n\nreranking:\n  provider: \"\"  # Empty to disable, or mxbai, cohere, zeroentropy, vllm\n  model: \"\"\n\nqa:\n  provider: ollama\n  model: gpt-oss\n\nresearch:\n  provider: \"\"  # Empty to use qa settings\n  model: \"\"\n\nprocessing:\n  chunk_size: 256\n  context_chunk_radius: 0\n  markdown_preprocessor: \"\"\n\nproviders:\n  ollama:\n    base_url: http://localhost:11434\n\n  vllm:\n    embeddings_base_url: \"\"\n    rerank_base_url: \"\"\n    qa_base_url: \"\"\n    research_base_url: \"\"\n\na2a:\n  max_contexts: 1000\n</code></pre>"},{"location":"configuration/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>When using haiku.rag as a Python library, you can pass configuration directly to the <code>HaikuRAG</code> client:</p> <pre><code>from haiku.rag.config import AppConfig\nfrom haiku.rag.client import HaikuRAG\n\n# Create custom configuration\ncustom_config = AppConfig(\n    qa={\"provider\": \"openai\", \"model\": \"gpt-4o\"},\n    embeddings={\"provider\": \"ollama\", \"model\": \"qwen3-embedding\"},\n    processing={\"chunk_size\": 512}\n)\n\n# Pass configuration to the client\nclient = HaikuRAG(config=custom_config)\n</code></pre> <p>If you don't pass a config, the client uses the global configuration loaded from your YAML file or defaults.</p> <p>This is useful for: - Jupyter notebooks - Python scripts - Testing with different configurations - Applications that need multiple clients with different configurations</p>"},{"location":"configuration/#file-monitoring","title":"File Monitoring","text":"<p>Set directories to monitor for automatic indexing:</p> <pre><code>monitor:\n  directories:\n    - /path/to/documents\n    - /another_path/to/documents\n</code></pre>"},{"location":"configuration/#filtering-monitored-files","title":"Filtering Monitored Files","text":"<p>Use gitignore-style patterns to control which files are monitored:</p> <pre><code>monitor:\n  directories:\n    - /path/to/documents\n\n  # Exclude specific files or directories\n  ignore_patterns:\n    - \"*draft*\"         # Ignore files with \"draft\" in the name\n    - \"temp/\"           # Ignore temp directory\n    - \"**/archive/**\"   # Ignore all archive directories\n    - \"*.backup\"        # Ignore backup files\n\n  # Only include specific files (whitelist mode)\n  include_patterns:\n    - \"*.md\"            # Only markdown files\n    - \"*.pdf\"           # Only PDF files\n    - \"**/docs/**\"      # Only files in docs directories\n</code></pre> <p>How patterns work:</p> <ol> <li>Extension filtering - Only supported file types are considered</li> <li>Include patterns - If specified, only matching files are included (whitelist)</li> <li>Ignore patterns - Matching files are excluded (blacklist)</li> <li>Combining both - Include patterns are applied first, then ignore patterns</li> </ol> <p>Common patterns:</p> <pre><code># Only monitor markdown documentation, but ignore drafts\nmonitor:\n  include_patterns:\n    - \"*.md\"\n  ignore_patterns:\n    - \"*draft*\"\n    - \"*WIP*\"\n\n# Monitor all supported files except in specific directories\nmonitor:\n  ignore_patterns:\n    - \"node_modules/\"\n    - \".git/\"\n    - \"**/test/**\"\n    - \"**/temp/**\"\n</code></pre> <p>Patterns follow gitignore syntax: - <code>*</code> matches anything except <code>/</code> - <code>**</code> matches zero or more directories - <code>?</code> matches any single character - <code>[abc]</code> matches any character in the set</p>"},{"location":"configuration/#embedding-providers","title":"Embedding Providers","text":"<p>If you use Ollama, you can use any pulled model that supports embeddings.</p>"},{"location":"configuration/#ollama-default","title":"Ollama (Default)","text":"<pre><code>embeddings:\n  provider: ollama\n  model: mxbai-embed-large\n  vector_dim: 1024\n</code></pre> <p>The Ollama base URL can be configured via environment variable or config file:</p> <pre><code># Via environment variable (recommended)\nexport OLLAMA_BASE_URL=http://localhost:11434\n</code></pre> <p>Or in your config file:</p> <pre><code>providers:\n  ollama:\n    base_url: http://localhost:11434\n</code></pre> <p>If neither is set, it defaults to <code>http://localhost:11434</code>.</p>"},{"location":"configuration/#voyageai","title":"VoyageAI","text":"<p>If you want to use VoyageAI embeddings you will need to install <code>haiku.rag</code> with the VoyageAI extras:</p> <pre><code>uv pip install haiku.rag[voyageai]\n</code></pre> <pre><code>embeddings:\n  provider: voyageai\n  model: voyage-3.5\n  vector_dim: 1024\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export VOYAGE_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/#openai","title":"OpenAI","text":"<p>OpenAI embeddings are included in the default installation:</p> <pre><code>embeddings:\n  provider: openai\n  model: text-embedding-3-small  # or text-embedding-3-large\n  vector_dim: 1536\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export OPENAI_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/#vllm","title":"vLLM","text":"<p>For high-performance local inference, you can use vLLM to serve embedding models with OpenAI-compatible APIs:</p> <pre><code>embeddings:\n  provider: vllm\n  model: mixedbread-ai/mxbai-embed-large-v1\n  vector_dim: 512\n\nproviders:\n  vllm:\n    embeddings_base_url: http://localhost:8000\n</code></pre> <p>Note: You need to run a vLLM server separately with an embedding model loaded.</p>"},{"location":"configuration/#question-answering-providers","title":"Question Answering Providers","text":"<p>Configure which LLM provider to use for question answering. Any provider and model supported by Pydantic AI can be used.</p>"},{"location":"configuration/#ollama-default_1","title":"Ollama (Default)","text":"<pre><code>qa:\n  provider: ollama\n  model: gpt-oss\n</code></pre> <p>The Ollama base URL can be configured via the <code>OLLAMA_BASE_URL</code> environment variable, config file, or defaults to <code>http://localhost:11434</code>:</p> <pre><code>export OLLAMA_BASE_URL=http://localhost:11434\n</code></pre> <p>Or in your config file:</p> <pre><code>providers:\n  ollama:\n    base_url: http://localhost:11434\n</code></pre>"},{"location":"configuration/#openai_1","title":"OpenAI","text":"<p>OpenAI QA is included in the default installation:</p> <pre><code>qa:\n  provider: openai\n  model: gpt-4o-mini  # or gpt-4, gpt-3.5-turbo, etc.\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export OPENAI_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/#anthropic","title":"Anthropic","text":"<p>Anthropic QA is included in the default installation:</p> <pre><code>qa:\n  provider: anthropic\n  model: claude-3-5-haiku-20241022  # or claude-3-5-sonnet-20241022, etc.\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export ANTHROPIC_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/#vllm_1","title":"vLLM","text":"<p>For high-performance local inference:</p> <pre><code>qa:\n  provider: vllm\n  model: Qwen/Qwen3-4B  # Any model with tool support in vLLM\n\nproviders:\n  vllm:\n    qa_base_url: http://localhost:8002\n</code></pre> <p>Note: You need to run a vLLM server separately with a model that supports tool calling loaded. Consult the specific model's documentation for proper vLLM serving configuration.</p>"},{"location":"configuration/#other-providers","title":"Other Providers","text":"<p>Any provider supported by Pydantic AI can be used. Examples:</p> <pre><code># Google Gemini\nqa:\n  provider: gemini\n  model: gemini-1.5-flash\n\n# Groq\nqa:\n  provider: groq\n  model: llama-3.3-70b-versatile\n\n# Mistral\nqa:\n  provider: mistral\n  model: mistral-small-latest\n</code></pre> <p>See the Pydantic AI documentation for the complete list of supported providers and models.</p>"},{"location":"configuration/#reranking","title":"Reranking","text":"<p>Reranking improves search quality by re-ordering the initial search results using specialized models. When enabled, the system retrieves more candidates (3x the requested limit) and then reranks them to return the most relevant results.</p> <p>Reranking is disabled by default (<code>provider: \"\"</code>) for faster searches. You can enable it by configuring one of the providers below.</p>"},{"location":"configuration/#mixedbread-ai","title":"MixedBread AI","text":"<p>For MxBAI reranking, install with mxbai extras:</p> <pre><code>uv pip install haiku.rag[mxbai]\n</code></pre> <p>Then configure:</p> <pre><code>reranking:\n  provider: mxbai\n  model: mixedbread-ai/mxbai-rerank-base-v2\n</code></pre>"},{"location":"configuration/#cohere","title":"Cohere","text":"<p>Install with cohere extras:</p> <pre><code>uv pip install haiku.rag[cohere]\n</code></pre> <p>Then configure:</p> <pre><code>reranking:\n  provider: cohere\n  model: rerank-v3.5\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export CO_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/#zero-entropy","title":"Zero Entropy","text":"<p>Install with zeroentropy extras:</p> <pre><code>uv pip install haiku.rag[zeroentropy]\n</code></pre> <p>Then configure:</p> <pre><code>reranking:\n  provider: zeroentropy\n  model: zerank-1  # Currently the only available model\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export ZEROENTROPY_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/#vllm_2","title":"vLLM","text":"<p>For high-performance local reranking using dedicated reranking models:</p> <pre><code>reranking:\n  provider: vllm\n  model: mixedbread-ai/mxbai-rerank-base-v2\n\nproviders:\n  vllm:\n    rerank_base_url: http://localhost:8001\n</code></pre> <p>Note: vLLM reranking uses the <code>/rerank</code> API endpoint. You need to run a vLLM server separately with a reranking model loaded. Consult the specific model's documentation for proper vLLM serving configuration.</p>"},{"location":"configuration/#other-settings","title":"Other Settings","text":""},{"location":"configuration/#database-and-storage","title":"Database and Storage","text":"<p>By default, <code>haiku.rag</code> uses a local LanceDB database:</p> <pre><code>storage:\n  data_dir: /path/to/data  # Empty = use default platform location\n</code></pre> <p>For remote storage, use the <code>lancedb</code> settings with various backends:</p> <pre><code># LanceDB Cloud\nlancedb:\n  uri: db://your-database-name\n  api_key: your-api-key\n  region: us-west-2  # optional\n\n# Amazon S3\nlancedb:\n  uri: s3://my-bucket/my-table\n# Use AWS credentials or IAM roles\n\n# Azure Blob Storage\nlancedb:\n  uri: az://my-container/my-table\n# Use Azure credentials\n\n# Google Cloud Storage\nlancedb:\n  uri: gs://my-bucket/my-table\n# Use GCP credentials\n\n# HDFS\nlancedb:\n  uri: hdfs://namenode:port/path/to/table\n</code></pre> <p>Authentication is handled through standard cloud provider credentials (AWS CLI, Azure CLI, gcloud, etc.) or by setting <code>api_key</code> for LanceDB Cloud.</p> <p>Note: Table optimization is automatically handled by LanceDB Cloud (<code>db://</code> URIs) and is disabled for better performance. For object storage backends (S3, Azure, GCS), optimization is still performed locally.</p>"},{"location":"configuration/#disable-database-auto-creation","title":"Disable database auto-creation","text":"<p>By default, haiku.rag creates the local LanceDB directory and required tables on first use. To prevent accidental database creation and fail fast if a database hasn't been set up yet:</p> <pre><code>storage:\n  disable_autocreate: true\n</code></pre> <p>When enabled, for local paths, haiku.rag errors if the LanceDB directory does not exist, and it will not create parent directories.</p>"},{"location":"configuration/#document-processing","title":"Document Processing","text":"<pre><code>processing:\n  # Chunk size for document processing\n  chunk_size: 256\n\n  # Number of adjacent chunks to include before/after retrieved chunks for context\n  # 0 = no expansion (default), 1 = include 1 chunk before and after, etc.\n  # When expanded chunks overlap or are adjacent, they are automatically merged\n  # into single chunks with continuous content to eliminate duplication\n  context_chunk_radius: 0\n\n  # Optional dotted path or file path to a callable that preprocesses\n  # markdown content before chunking\n  markdown_preprocessor: \"\"\n\nstorage:\n  # Vacuum retention threshold (seconds) for automatic cleanup\n  # When documents are added/updated, old table versions older than this are removed\n  # Default: 60 seconds (safe for concurrent connections)\n  # Set to 0 for aggressive cleanup (removes all old versions immediately)\n  vacuum_retention_seconds: 60\n</code></pre>"},{"location":"configuration/#markdown-preprocessor","title":"Markdown Preprocessor","text":"<p>Optionally preprocess Markdown before chunking by pointing to a callable that receives and returns Markdown text. This is useful for normalizing content, stripping boilerplate, or applying custom transformations before chunk boundaries are computed.</p> <pre><code>processing:\n  # A callable path in one of these formats:\n  # - package.module:func\n  # - package.module.func\n  # - /abs/or/relative/path/to/file.py:func\n  markdown_preprocessor: my_pkg.preprocess:clean_md\n</code></pre> <p>Note</p> <ul> <li>The function signature should be <code>def clean_md(text: str) -&gt; str</code> or <code>async def clean_md(text: str) -&gt; str</code>.</li> <li>If the function raises or returns a non-string, haiku.rag logs a warning and proceeds without preprocessing.</li> <li>The preprocessor affects only the chunking pipeline. The stored document content remains unchanged.</li> </ul> <p>Example implementation:</p> <pre><code># my_pkg/preprocess.py\ndef clean_md(text: str) -&gt; str:\n    # strip HTML comments and collapse multiple blank lines\n    lines = [line for line in text.splitlines() if not line.strip().startswith(\"&lt;!--\")]\n    out = []\n    for line in lines:\n        if line.strip() == \"\" and (out and out[-1] == \"\"):\n            continue\n        out.append(line)\n    return \"\\n\".join(out)\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#basic-installation","title":"Basic Installation","text":"<pre><code>uv pip install haiku.rag\n</code></pre> <p>This includes support for: - Ollama (default embedding provider using <code>mxbai-embed-large</code>) - OpenAI (GPT models for QA and embeddings) - Anthropic (Claude models for QA) - vLLM (high-performance local inference for embeddings, QA, and reranking)</p>"},{"location":"installation/#provider-specific-installation","title":"Provider-Specific Installation","text":"<p>For additional providers, install with extras:</p>"},{"location":"installation/#embedding-providers","title":"Embedding Providers","text":""},{"location":"installation/#voyageai","title":"VoyageAI","text":"<pre><code>uv pip install haiku.rag[voyageai]\n</code></pre>"},{"location":"installation/#reranking-providers","title":"Reranking Providers","text":""},{"location":"installation/#mixedbread-ai","title":"MixedBread AI","text":"<pre><code>uv pip install haiku.rag[mxbai]\n</code></pre>"},{"location":"installation/#cohere","title":"Cohere","text":"<pre><code>uv pip install haiku.rag[cohere]\n</code></pre>"},{"location":"installation/#zero-entropy","title":"Zero Entropy","text":"<pre><code>uv pip install haiku.rag[zeroentropy]\n</code></pre>"},{"location":"installation/#vllm-setup","title":"vLLM Setup","text":"<p>vLLM requires no additional installation - it works with the base haiku.rag package. However, you need to run vLLM servers separately:</p> <pre><code># Install vLLM\npip install vllm\n\n# Serve an embedding model\nvllm serve mixedbread-ai/mxbai-embed-large-v1 --port 8000\n\n# Serve a model for QA (requires tool calling support)\nvllm serve Qwen/Qwen3-4B --port 8002 --enable-auto-tool-choice --tool-call-parser hermes\n\n# Serve a model for reranking\nvllm serve mixedbread-ai/mxbai-rerank-base-v2 --hf_overrides '{\"architectures\": [\"Qwen2ForSequenceClassification\"],\"classifier_from_token\": [\"0\", \"1\"], \"method\": \"from_2_way_softmax\"}' --port 8001\n</code></pre> <p>Then configure haiku.rag to use the vLLM servers. Create a <code>haiku.rag.yaml</code> file:</p> <pre><code>embeddings:\n  provider: vllm\n  model: mixedbread-ai/mxbai-embed-large-v1\n  vector_dim: 512\n\nqa:\n  provider: vllm\n  model: Qwen/Qwen3-4B\n\nreranking:\n  provider: vllm\n  model: mixedbread-ai/mxbai-rerank-base-v2\n\nproviders:\n  vllm:\n    embeddings_base_url: http://localhost:8000\n    qa_base_url: http://localhost:8002\n    rerank_base_url: http://localhost:8001\n</code></pre> <p>See Configuration for all available options.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12+</li> <li>Ollama (for default embeddings)</li> <li>vLLM server (for vLLM provider)</li> </ul>"},{"location":"installation/#pre-download-models-optional","title":"Pre-download Models (Optional)","text":"<p>You can prefetch all required runtime models before first use:</p> <pre><code>haiku-rag download-models\n</code></pre> <p>This will download Docling models and pull any Ollama models referenced by your current configuration.</p>"},{"location":"installation/#docker","title":"Docker","text":"<pre><code>docker pull ghcr.io/ggozad/haiku.rag:latest\n</code></pre> <p>Run the container with all services:</p> <pre><code>docker run -p 8000:8000 -p 8001:8001 -v $(pwd)/data:/data ghcr.io/ggozad/haiku.rag:latest\n</code></pre> <p>This starts the MCP server on port 8001 and A2A server on port 8000, with data persisted to <code>./data</code>.</p>"},{"location":"mcp/","title":"Model Context Protocol (MCP)","text":"<p>The MCP server exposes <code>haiku.rag</code> as MCP tools for compatible MCP clients.</p>"},{"location":"mcp/#available-tools","title":"Available Tools","text":""},{"location":"mcp/#document-management","title":"Document Management","text":"<ul> <li><code>add_document_from_file</code> - Add documents from local file paths</li> <li><code>add_document_from_url</code> - Add documents from URLs</li> <li><code>add_document_from_text</code> - Add documents from raw text content</li> <li><code>get_document</code> - Retrieve specific documents by ID</li> <li><code>list_documents</code> - List all documents with pagination</li> <li><code>delete_document</code> - Delete documents by ID</li> </ul>"},{"location":"mcp/#search","title":"Search","text":"<ul> <li><code>search_documents</code> - Search documents using hybrid search (vector + full-text)</li> </ul>"},{"location":"mcp/#starting-mcp-server","title":"Starting MCP Server","text":"<p>The MCP server starts automatically with the serve command and supports Streamable HTTP and stdio transports:</p> <pre><code># Default streamable HTTP transport\nhaiku-rag serve\n\n# stdio transport (for Claude Desktop)\nhaiku-rag serve --stdio\n</code></pre>"},{"location":"python/","title":"Python API","text":"<p>Use <code>haiku.rag</code> directly in your Python applications.</p>"},{"location":"python/#basic-usage","title":"Basic Usage","text":"<pre><code>from pathlib import Path\nfrom haiku.rag.client import HaikuRAG\n\n# Use as async context manager (recommended)\nasync with HaikuRAG(\"Path(path/to/database.lancedb\")) as client:\n    # Your code here\n    pass\n</code></pre>"},{"location":"python/#document-management","title":"Document Management","text":""},{"location":"python/#creating-documents","title":"Creating Documents","text":"<p>From text: <pre><code>doc = await client.create_document(\n    content=\"Your document content here\",\n    uri=\"doc://example\",\n    title=\"My Example Document\",  # optional human\u2011readable title\n    metadata={\"source\": \"manual\", \"topic\": \"example\"}\n)\n</code></pre></p> <p>With custom externally generated chunks: <pre><code>from haiku.rag.store.models.chunk import Chunk\n\n# Create custom chunks with optional embeddings\nchunks = [\n    Chunk(\n        content=\"This is the first chunk\",\n        metadata={\"section\": \"intro\"}\n    ),\n    Chunk(\n        content=\"This is the second chunk\",\n        metadata={\"section\": \"body\"},\n        embedding=[0.1] * 1024  # Optional pre-computed embedding\n    ),\n]\n\ndoc = await client.create_document(\n    content=\"Full document content\",\n    uri=\"doc://custom\",\n    metadata={\"source\": \"manual\"},\n    chunks=chunks  # Use provided chunks instead of auto-generating\n)\n</code></pre></p> <p>From file: <pre><code>doc = await client.create_document_from_source(\n    \"path/to/document.pdf\", title=\"Project Brief\"\n)\n</code></pre></p> <p>From URL: <pre><code>doc = await client.create_document_from_source(\n    \"https://example.com/article.html\", title=\"Example Article\"\n)\n</code></pre></p>"},{"location":"python/#retrieving-documents","title":"Retrieving Documents","text":"<p>By ID: <pre><code>doc = await client.get_document_by_id(1)\n</code></pre></p> <p>By URI: <pre><code>doc = await client.get_document_by_uri(\"file:///path/to/document.pdf\")\n</code></pre></p> <p>List all documents: <pre><code>docs = await client.list_documents(limit=10, offset=0)\n</code></pre></p>"},{"location":"python/#updating-documents","title":"Updating Documents","text":"<pre><code>doc.content = \"Updated content\"\nawait client.update_document(doc)\n</code></pre>"},{"location":"python/#deleting-documents","title":"Deleting Documents","text":"<pre><code>await client.delete_document(doc.id)\n</code></pre>"},{"location":"python/#rebuilding-the-database","title":"Rebuilding the Database","text":"<pre><code>async for doc_id in client.rebuild_database():\n    print(f\"Processed document {doc_id}\")\n</code></pre>"},{"location":"python/#maintenance","title":"Maintenance","text":"<p>Run maintenance to optimize storage and prune old table versions:</p> <pre><code>await client.vacuum()\n</code></pre> <p>This compacts tables and removes historical versions to keep disk usage in check. It\u2019s safe to run anytime, for example after bulk imports or periodically in long\u2011running apps.</p>"},{"location":"python/#atomic-writes-and-rollback","title":"Atomic Writes and Rollback","text":"<p>Document create and update operations take a snapshot of table versions before any write and automatically roll back to that snapshot if something fails (for example, during chunking or embedding). This restores both the <code>documents</code> and <code>chunks</code> tables to their pre\u2011operation state using LanceDB\u2019s table versioning.</p> <ul> <li>Applies to: <code>create_document(...)</code>, <code>create_document_from_source(...)</code>, <code>update_document(...)</code>, and internal rebuild/update flows.</li> <li>Scope: Both document rows and all associated chunks are rolled back together.</li> <li>Vacuum: Running <code>vacuum()</code> later prunes old versions for disk efficiency; rollbacks occur immediately during the failing operation and are not impacted.</li> </ul>"},{"location":"python/#searching-documents","title":"Searching Documents","text":"<p>The search method performs native hybrid search (vector + full-text) using LanceDB with optional reranking for improved relevance:</p> <p>Basic hybrid search (default): <pre><code>results = await client.search(\"machine learning algorithms\", limit=5)\nfor chunk, score in results:\n    print(f\"Score: {score:.3f}\")\n    print(f\"Content: {chunk.content}\")\n    print(f\"Document ID: {chunk.document_id}\")\n</code></pre></p> <p>Search with different search types: <pre><code># Vector search only\nresults = await client.search(\n    query=\"machine learning\",\n    limit=5,\n    search_type=\"vector\"\n)\n\n# Full-text search only\nresults = await client.search(\n    query=\"machine learning\",\n    limit=5,\n    search_type=\"fts\"\n)\n\n# Hybrid search (default - combines vector + fts with native LanceDB RRF)\nresults = await client.search(\n    query=\"machine learning\",\n    limit=5,\n    search_type=\"hybrid\"\n)\n\n# Process results\nfor chunk, relevance_score in results:\n    print(f\"Relevance: {relevance_score:.3f}\")\n    print(f\"Content: {chunk.content}\")\n    print(f\"From document: {chunk.document_id}\")\n    print(f\"Document URI: {chunk.document_uri}\")\n    print(f\"Document Title: {chunk.document_title}\")  # when available\n    print(f\"Document metadata: {chunk.document_meta}\")\n</code></pre></p>"},{"location":"python/#filtering-search-results","title":"Filtering Search Results","text":"<p>Filter search results to only include chunks from documents matching specific criteria:</p> <pre><code># Filter by document URI pattern\nresults = await client.search(\n    query=\"machine learning\",\n    limit=5,\n    filter=\"uri LIKE '%arxiv%'\"\n)\n\n# Filter by exact document title\nresults = await client.search(\n    query=\"neural networks\",\n    limit=5,\n    filter=\"title = 'Deep Learning Guide'\"\n)\n\n# Combine multiple filter conditions\nresults = await client.search(\n    query=\"AI research\",\n    limit=5,\n    filter=\"uri LIKE '%.pdf' AND title LIKE '%paper%'\"\n)\n\n# Filter with any search type\nresults = await client.search(\n    query=\"transformers\",\n    limit=5,\n    search_type=\"vector\",\n    filter=\"uri LIKE '%huggingface%'\"\n)\n</code></pre> <p>Note: Filters apply to document properties only. Available columns for filtering: - <code>id</code> - Document ID - <code>uri</code> - Document URI/URL - <code>title</code> - Document title (if set) - <code>created_at</code>, <code>updated_at</code> - Timestamps - <code>metadata</code> - Document metadata (as string, use LIKE for pattern matching)</p>"},{"location":"python/#expanding-search-context","title":"Expanding Search Context","text":"<p>Expand search results with adjacent chunks for more complete context:</p> <pre><code># Get initial search results\nsearch_results = await client.search(\"machine learning\", limit=3)\n\n# Expand with adjacent chunks using config setting\nexpanded_results = await client.expand_context(search_results)\n\n# Or specify a custom radius\nexpanded_results = await client.expand_context(search_results, radius=2)\n\n# The expanded results contain chunks with combined content from adjacent chunks\nfor chunk, score in expanded_results:\n    print(f\"Expanded content: {chunk.content}\")  # Now includes before/after chunks\n</code></pre> <p>Smart Merging: When expanded chunks overlap or are adjacent within the same document, they are automatically merged into single chunks with continuous content. This eliminates duplication and provides coherent text blocks. The merged chunk uses the highest relevance score from the original chunks.</p> <p>This is automatically used by the QA system when <code>processing.context_chunk_radius &gt; 0</code> (configured in <code>haiku.rag.yaml</code>) to provide better answers with more complete context.</p>"},{"location":"python/#question-answering","title":"Question Answering","text":"<p>Ask questions about your documents:</p> <pre><code>answer = await client.ask(\"Who is the author of haiku.rag?\")\nprint(answer)\n</code></pre> <p>Ask questions with citations showing source documents:</p> <pre><code>answer = await client.ask(\"Who is the author of haiku.rag?\", cite=True)\nprint(answer)\n</code></pre> <p>Customize the QA agent's behavior with a custom system prompt:</p> <pre><code>custom_prompt = \"\"\"You are a technical support expert for WIX.\nAnswer questions based on the knowledge base documents provided.\nBe concise and helpful.\"\"\"\n\nanswer = await client.ask(\n    \"How do I create a blog?\",\n    system_prompt=custom_prompt\n)\n</code></pre> <p>The QA agent will search your documents for relevant information and use the configured LLM to generate a comprehensive answer. With <code>cite=True</code>, responses include citations showing which documents were used as sources. Citations prefer the document title when present, otherwise they use the URI.</p> <p>The QA provider and model are configured in <code>haiku.rag.yaml</code> or can be passed directly to the client (see Configuration).</p> <p>See also: Agents for details on the QA agent and the multi\u2011agent research workflow.</p>"},{"location":"server/","title":"Server Mode","text":"<p>The server provides automatic file monitoring, MCP functionality, and A2A agent support.</p>"},{"location":"server/#starting-the-server","title":"Starting the Server","text":"<p>The <code>serve</code> command requires at least one service flag. You can enable file monitoring, MCP server, A2A server, or any combination:</p>"},{"location":"server/#mcp-server-only","title":"MCP Server Only","text":"<pre><code>haiku-rag serve --mcp\n</code></pre> <p>Transport options: - Default - Streamable HTTP transport on port 8001 - <code>--stdio</code> - Standard input/output transport - <code>--mcp-port</code> - Custom port (default: 8001)</p>"},{"location":"server/#a2a-server-only","title":"A2A Server Only","text":"<pre><code>haiku-rag serve --a2a\n</code></pre> <p>Options: - <code>--a2a-host</code> - Host to bind to (default: 127.0.0.1) - <code>--a2a-port</code> - Port to bind to (default: 8000)</p> <p>See A2A documentation for details on the conversational agent.</p>"},{"location":"server/#file-monitoring-only","title":"File Monitoring Only","text":"<pre><code>haiku-rag serve --monitor\n</code></pre>"},{"location":"server/#all-services","title":"All Services","text":"<pre><code>haiku-rag serve --monitor --mcp --a2a\n</code></pre> <p>This will start file monitoring, MCP server on port 8001, and A2A server on port 8000.</p>"},{"location":"server/#file-monitoring","title":"File Monitoring","text":"<p>Configure directories to monitor in your <code>haiku.rag.yaml</code>:</p> <pre><code>monitor:\n  directories:\n    - /path/to/documents\n    - /another/path\n</code></pre> <p>Then start the server:</p> <pre><code>haiku-rag serve --monitor\n</code></pre>"},{"location":"server/#monitoring-features","title":"Monitoring Features","text":"<ul> <li>Startup: Scans all monitored directories and adds new files</li> <li>File Added/Modified: Automatically parses and updates documents</li> <li>File Deleted: Removes corresponding documents from database</li> </ul>"},{"location":"server/#filtering-files","title":"Filtering Files","text":"<p>You can filter which files to monitor using gitignore-style patterns:</p> <pre><code>monitor:\n  directories:\n    - /path/to/documents\n\n  # Ignore patterns (exclude files)\n  ignore_patterns:\n    - \"*draft*\"         # Ignore draft files\n    - \"temp/\"           # Ignore temp directory\n    - \"**/archive/**\"   # Ignore archive directories\n\n  # Include patterns (whitelist files)\n  include_patterns:\n    - \"*.md\"           # Only markdown files\n    - \"**/docs/**\"     # Files in docs directories\n</code></pre> <p>Pattern behavior: - Extension filtering is applied first (only supported file types) - Include patterns create a whitelist (if specified) - Ignore patterns exclude files - Both can be combined for fine-grained control</p>"},{"location":"server/#supported-formats","title":"Supported Formats","text":"<p>The server can parse 40+ file formats including: - PDF documents - Microsoft Office (DOCX, XLSX, PPTX) - HTML and Markdown - Plain text files - Code files (Python, JavaScript, etc.) - Images (processed via OCR) - And more...</p> <p>URLs are also supported for web content.</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This tutorial quickstart instructions for getting familiar with <code>haiku.rag</code>. This tutorial is intended for people who are familiar with command line and Python, but not different AI ecosystem tools.</p> <p>The tutorial covers:</p> <ul> <li>RAG and embeddings basics</li> <li>Install <code>haiku.rag</code> Python package</li> <li>Set up environment variables for running <code>haiku.rag</code></li> <li>Adding and retrieving items</li> <li>Inspecting the database</li> </ul> <p>The tutorial uses OpenAI API service - no local installation needed and will work on computers with any amount of RAM and GPU. The OpenAI API is pay-as-you-go, so you need to top it up with at least ~$5 when creating the API key.</p>"},{"location":"tutorial/#introduction","title":"Introduction","text":"<p>Embeddings serve as the foundational bridge between unstructured text data and computational efficiency in AI systems, particularly within Retrieval-Augmented Generation (RAG) frameworks that enhance Large Language Models (LLMs). At their core, embeddings are dense vector representations of words, sentences, or documents, created by models like those from OpenAI, which encode semantic and contextual meaning into numerical forms. In a RAG pipeline, these embeddings enable the system to index and store vast amounts of information in a vector database, allowing for rapid similarity searches. When a user query is posed, its embedding is generated and compared against the stored vectors using metrics like cosine similarity to retrieve the most relevant documents or chunks of text. This retrieval step addresses a key limitation of standalone LLMs, which rely on pre-trained knowledge that can be outdated, incomplete, or prone to hallucinations, by providing external, grounded context to inform the model's output.</p> <p>You technically can create vector embeddings yourself without relying on pre-built models like OpenAI's, but it's impractical for most users due to the complexity and resources involved in training or implementing from scratch. Generating embeddings requires a neural network architecture (e.g., transformer-based) trained on massive datasets to learn semantic relationships, which demands significant computational power (GPUs/TPUs), expertise in machine learning, and access to billions of text examples for fine-tuning.</p> <p><code>haiku.rag</code> is a Python library allowing you to set up your own embeddings database, pipeline to feed into it and query it with different LLM providers and related services.</p>"},{"location":"tutorial/#setup","title":"Setup","text":"<p>First, get an OpenAI API key.</p> <p>Install <code>haiku.rag</code> Python package using uv or your favourite Python package manager:</p> <pre><code># Python 3.12+ needed\nuv pip install haiku.rag\n</code></pre> <p>Configure haiku.rag to use OpenAI. Create a <code>haiku.rag.yaml</code> file:</p> <pre><code>embeddings:\n  provider: openai\n  model: text-embedding-3-small  # or text-embedding-3-large\n  vector_dim: 1536\n\nqa:\n  provider: openai\n  model: gpt-4o-mini  # or gpt-4o, gpt-4, etc.\n</code></pre> <p>Set your OpenAI API key as an environment variable (API keys should not be stored in the YAML file):</p> <pre><code>export OPENAI_API_KEY=\"&lt;your OpenAI API key&gt;\"\n</code></pre> <p>For the list of available OpenAI models and their vector dimensions, see the OpenAI documentation.</p> <p>See Configuration for all available options.</p>"},{"location":"tutorial/#adding-the-first-documents","title":"Adding the first documents","text":"<p>Now you can add some pieces of text in the database:</p> <pre><code>haiku-rag add \"Python is the best programming language in the world, because it is flexible, with robust ecosystem, open source licensing and thousands of contributors\"\nhaiku-rag add \"JavaScript is a popular programming language, but has a lot of warts\"\nhaiku-rag add \"PHP is a bad programming language, because of spotted security history, horrible syntax and declining popularity\"\n</code></pre> <p>What will happen</p> <ul> <li>The piece of text is send to OpenAI <code>/embeddings</code> API service</li> <li>OpenAI translates the free form text to RAG embedding vectors needed for the retrieval</li> <li>The vector values will be stored in a local database</li> </ul> <p>Now you can view your LanceDB database, and the embeddings it is configured for:</p> <pre><code>haiku-rag info\n</code></pre> <p>You should get the back the information:</p> <pre><code>haiku.rag database info\n  path: /Users/moo/Library/Application Support/haiku.rag/haiku.rag.lancedb\n  haiku.rag version (db): 0.12.1\n  embeddings: openai/text-embedding-3-small (dim: 1536)\n  documents: 4\n  versions (documents): 9\n  versions (chunks): 10\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nVersions\n  haiku.rag: 0.12.1\n  lancedb: 0.25.2\n  docling: 2.57.0\n</code></pre>"},{"location":"tutorial/#asking-questions-and-retrieving-information","title":"Asking questions and retrieving information","text":"<p>Now we can use OpenAI LLMs to retrieve information from our embeddings database.</p> <p>In this example, we connect to a remote OpenAI API.</p> <p>Behind the scenes pydantic-ai query is created using <code>OpenAIChatModel.request()</code>.</p> <p>The easiest way to do this is <code>ask</code> CLI command:</p> <pre><code>haiku-rag ask \"What is the best programming language in the world\"\n</code></pre> <pre><code>Question: What is the best programming language in the world\n\nAnswer:\nAccording to the document, Python is considered the best programming language in the world due to its flexibility, robust ecosystem, open-source licensing, and thousands of contributors.\n</code></pre>"},{"location":"tutorial/#programmatic-interaction-in-python","title":"Programmatic interaction in Python","text":"<p>You can interact with Haiku RAG from Python in a similar manner as you can from the command line. Here we use Haiku RAG with the interactive Python command prompt (REPL).</p> <p>First we need to install <code>ipython</code>, as built-in Python REPL does not support async blocks.</p> <pre><code>uv pip install ipython\n</code></pre> <p>Run IPython:</p> <pre><code>ipython\n</code></pre> <p>Then copy paste in the snippet (you can use %cpaste command):</p> <pre><code>import sys\nimport logging\nfrom haiku.rag.client import HaikuRAG\n\n# Increase logging verbosity so we see what happens behind the scenes,\n# and check that the logger works\nlogging.basicConfig(\n  stream=sys.stdout,\n  level=logging.DEBUG,\n  format=\"%(name)s - %(levelname)s - %(message)s\",\n)\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nlogger.debug(\"AGI here we come\")\n\n# Uses LanceDB database from default storage location\nasync with HaikuRAG() as client:\n    answer = await client.ask(\"What is the best programming language in the world?\")\n    print(answer)\n</code></pre> <p>You should see:</p> <pre><code>2025-10-18 17:05:49,611 - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Sat, 18 Oct 2025 14:05:49 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'xxx', 'openai-processing-ms': '788', 'openai-project': 'xxx', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '1050', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '199603', 'x-ratelimit-reset-requests': '14.981s', 'x-ratelimit-reset-tokens': '119ms', 'x-request-id': 'req_9651a3691a144dd388e97066ad67a49c', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '990897b6f8d270d7-ARN', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n2025-10-18 17:05:49,611 - DEBUG - request_id: req_9651a3691a144dd388e97066ad67a49c\n\nAccording to the document, Python is considered the best programming language in the world due to its flexibility, robust ecosystem, open-source licensing, and support from thousands of contributors.\n</code></pre>"},{"location":"tutorial/#complex-documents","title":"Complex documents","text":"<p>Haiku RAG can also handle types beyond plain text.</p> <p>Here we add research papers about Python from arxiv using URL retriever.</p> <pre><code># Better Python Programming for all: With the focus on Maintainability\nhaiku-rag add-src --meta collection=\"Interesting Python papers\" \"https://arxiv.org/pdf/2408.09134\"\n\n# Interoperability From OpenTelemetry to Kieker: Demonstrated as Export from the Astronomy Shop\nhaiku-rag add-src --meta collection=\"Interesting Python papers\" \"https://arxiv.org/pdf/2510.11179\"\n</code></pre> <p>Then we can query this:</p> <pre><code>haiku-rag ask \"Who wrote a paper about OpenTelemetry interoperability, and what was his take\"\n</code></pre> <p>We should get something along the lines:</p> <pre><code>Answer:\nDavid Georg Reichelt from Lancaster University wrote a paper titled \"Interoperability From OpenTelemetry to Kieker: Demonstrated as Export from the Astronomy Shop.\" In his work, he indicates that there is a structural difference between Kieker\u2019s synchronous traces and OpenTelemetry\u2019s asynchronous traces, leading to  limited compatibility between the two systems. This highlights the challenges of interoperability in observability frameworks.\n</code></pre> <p>We can also add offline files, like PDFs. Here we add a local file to ensure OpenAI does not cheat - a file we know that should not be very well known in Internet:</p> <pre><code># This static file is supplied in haiku.rag repo\nhaiku-rag add-src \"examples/samples/PyCon Finland 2025 Schedule.html\"\n</code></pre> <p>And then:</p> <pre><code>haiku-rag ask \"Who were presenting talks in Pycon Finland 2025? Can you give at least five different people.\"\n</code></pre> <pre><code>The following people are presenting talks at PyCon Finland 2025:\n\n 1 Jeremy Mayeres - Talk: The Limits of Imagination: An Open Source Journey\n 2 Aroma Rodrigues - Talk: Python and Rust, a Perfect Pairing\n 3 Andreas Jung - Talk: Guillotina Volto: A New Backend for Volto\n 4 Daniel Vahla - Talk: Experiences with AI in Software Projects\n 5 Andreas Jung (also presenting another talk) - Talk: Debugging Python\n</code></pre>"},{"location":"tutorial/#reseting-the-embeddings-database","title":"Reseting the embeddings database","text":"<p>If you change your embeddings provider (OpenAI -&gt; ollama) or its parameters, you need to delete the LanceDB database and add the documents again:</p> <pre><code>rm -rf \"/Users/moo/Library/Application Support/haiku.rag/haiku.rag.lancedb\"\n</code></pre>"},{"location":"tutorial/#configuration","title":"Configuration","text":"<p>See Configuration page for complete documentation on YAML configuration and all available options.</p>"}]}