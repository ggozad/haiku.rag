{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"haiku.rag","text":"<p>Agentic RAG built on LanceDB, Pydantic AI, and Docling.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Hybrid search \u2014 Vector + full-text with Reciprocal Rank Fusion</li> <li>Question answering \u2014 QA agents with citations (page numbers, section headings)</li> <li>Reranking \u2014 MxBAI, Cohere, Zero Entropy, or vLLM</li> <li>Research agents \u2014 Multi-agent workflows via pydantic-graph: plan, search, evaluate, synthesize</li> <li>RLM agent \u2014 Complex analytical tasks via sandboxed Python code execution (aggregation, computation, multi-document analysis)</li> <li>Conversational RAG \u2014 Chat TUI and web application for multi-turn conversations with session memory</li> <li>Document structure \u2014 Stores full DoclingDocument, enabling structure-aware context expansion</li> <li>Multiple providers \u2014 Embeddings: Ollama, OpenAI, VoyageAI, LM Studio, vLLM. QA/Research: any model supported by Pydantic AI</li> <li>Local-first \u2014 Embedded LanceDB, no servers required. Also supports S3, GCS, Azure, and LanceDB Cloud</li> <li>CLI &amp; Python API \u2014 Full functionality from command line or code</li> <li>MCP server \u2014 Expose as tools for AI assistants (Claude Desktop, etc.)</li> <li>Visual grounding \u2014 View chunks highlighted on original page images</li> <li>File monitoring \u2014 Watch directories and auto-index on changes</li> <li>Time travel \u2014 Query the database at any historical point with <code>--before</code></li> <li>Inspector \u2014 TUI for browsing documents, chunks, and search results</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install haiku.rag:</p> <pre><code>uv pip install haiku.rag\n</code></pre> <p>Use from Python:</p> <pre><code>from haiku.rag.client import HaikuRAG\n\nasync with HaikuRAG(\"database.lancedb\", create=True) as client:\n    # Add a document\n    doc = await client.create_document(\"Your content here\")\n\n    # Search documents\n    results = await client.search(\"query\")\n\n    # Ask questions (returns answer and citations)\n    answer, citations = await client.ask(\"Who is the author of haiku.rag?\")\n</code></pre> <p>Or use the CLI:</p> <pre><code>haiku-rag add \"Your document content\"\nhaiku-rag add \"Your document content\" --meta author=alice\nhaiku-rag add-src /path/to/document.pdf --title \"Q3 Financial Report\" --meta source=manual\nhaiku-rag search \"query\"\nhaiku-rag ask \"Who is the author of haiku.rag?\"\nhaiku-rag chat  # Interactive conversation mode\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting started - Tutorial</li> <li>Installation - Install haiku.rag with different providers</li> <li>Architecture - System overview and data flow</li> <li>Configuration - Environment variables and settings</li> <li>CLI - Command line interface usage</li> <li>Python - Python API reference</li> <li>Custom Pipelines - Build custom processing workflows</li> <li>Agents - QA, chat, and research agents</li> <li>RLM Agent - Complex analytical tasks via code execution</li> <li>Applications - Chat TUI, web app, and inspector</li> <li>Server - File monitoring and server mode</li> <li>MCP - Model Context Protocol integration</li> <li>Remote processing - Remote document processing with docling-serve</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"apps/","title":"Applications","text":"<p>Three interactive applications for working with your knowledge base.</p>"},{"location":"apps/#chat-tui","title":"Chat TUI","text":"<p>Conversational RAG from the terminal with streaming responses and session memory.</p> <p>Note</p> <p>Requires the <code>tui</code> extra: <code>pip install haiku.rag-slim[tui]</code> (included in full <code>haiku.rag</code> package)</p>"},{"location":"apps/#usage","title":"Usage","text":"<pre><code>haiku-rag chat\nhaiku-rag chat --db /path/to/database.lancedb\n</code></pre>"},{"location":"apps/#interface","title":"Interface","text":"<p>The chat interface provides:</p> <ul> <li>Streaming responses with real-time tool execution indicators</li> <li>Expandable citations showing source document, pages, and headings</li> <li>Session memory for context-aware follow-up questions</li> <li>Visual grounding to inspect chunk source locations</li> </ul> <p></p> <p>Demo: Chatting with an agent over 1000 arXiv papers. Shows context building (3:00), citations with visual grounding (3:20), and document listing/retrieval.</p>"},{"location":"apps/#command-palette","title":"Command Palette","text":"<p>Press <code>Ctrl+P</code> to open the command palette:</p> Command Description View state View the current session state Filter documents Select documents to restrict searches Show database info View document/chunk counts and storage info Visual grounding View chunk source location in document Clear chat Clear chat history and reset session"},{"location":"apps/#session-management","title":"Session Management","text":"<ul> <li>Conversation history is maintained in memory for the session</li> <li>Previous Q/A pairs are automatically used as context for follow-up questions via the <code>ask</code> tool</li> <li>Citations are tracked per response and can be inspected</li> <li>Document filter restricts all searches to selected documents</li> <li>Clearing chat resets session state</li> </ul>"},{"location":"apps/#web-application","title":"Web Application","text":"<p>Browser-based conversational RAG with a CopilotKit frontend.</p>"},{"location":"apps/#features","title":"Features","text":"<ul> <li>Streaming chat with real-time tool execution visibility</li> <li>Expandable citations with source documents, pages, and headings</li> <li>Visual grounding to view chunk source locations in documents</li> <li>Document filter to restrict searches to selected documents</li> <li>Session state view for inspecting accumulated Q&amp;A history, citations, and documents</li> </ul>"},{"location":"apps/#quick-start","title":"Quick Start","text":"<pre><code>cd app\ndocker compose -f docker-compose.dev.yml up -d --build\n</code></pre> <ul> <li>Frontend: http://localhost:3000</li> <li>Backend: http://localhost:8001</li> </ul>"},{"location":"apps/#architecture","title":"Architecture","text":"<ul> <li>Backend: Starlette server with pydantic-ai <code>AGUIAdapter</code></li> <li>Frontend: Next.js with CopilotKit</li> <li>Protocol: AG-UI for streaming chat</li> </ul>"},{"location":"apps/#configuration","title":"Configuration","text":"<p>Create a <code>.env</code> file in the <code>app/</code> directory:</p> <pre><code># API Keys (at least one required)\nANTHROPIC_API_KEY=your-anthropic-key\nOPENAI_API_KEY=your-openai-key\n\n# Database path\nDB_PATH=/path/to/your/haiku.rag.lancedb\n\n# Optional: Ollama base URL (if using local models)\nOLLAMA_BASE_URL=http://localhost:11434\n\n# Optional: Logfire for observability\nLOGFIRE_TOKEN=your-logfire-token\n</code></pre> <p>For full configuration, mount a <code>haiku.rag.yaml</code> file:</p> <pre><code># app/haiku.rag.yaml\nqa:\n  model:\n    provider: anthropic\n    name: claude-sonnet-4-20250514\n</code></pre>"},{"location":"apps/#api-endpoints","title":"API Endpoints","text":"Endpoint Method Description <code>/v1/chat/stream</code> POST AG-UI chat streaming <code>/api/documents</code> GET List all documents <code>/api/info</code> GET Database statistics <code>/api/visualize/{chunk_id}</code> GET Visual grounding images (base64) <code>/health</code> GET Health check"},{"location":"apps/#development","title":"Development","text":"<p>Hot reload: The backend reloads automatically on file changes. For frontend changes:</p> <pre><code>docker compose -f docker-compose.dev.yml up -d --build frontend\n</code></pre> <p>Logfire debugging: If <code>LOGFIRE_TOKEN</code> is set, LLM calls are traced and available in the Logfire dashboard.</p>"},{"location":"apps/#inspector","title":"Inspector","text":"<p>TUI for browsing documents, chunks, and search results.</p> <p>Note</p> <p>Requires the <code>tui</code> extra: <code>pip install haiku.rag-slim[tui]</code> (included in full <code>haiku.rag</code> package)</p>"},{"location":"apps/#usage_1","title":"Usage","text":"<pre><code>haiku-rag inspect\nhaiku-rag inspect --db /path/to/database.lancedb\n</code></pre>"},{"location":"apps/#interface_1","title":"Interface","text":"<p>Three panels display your data:</p> <ul> <li>Documents (left) - All documents in the database</li> <li>Chunks (top right) - Chunks for the selected document</li> <li>Detail View (bottom right) - Full content and metadata</li> </ul> <p></p>"},{"location":"apps/#navigation","title":"Navigation","text":"Key Action <code>Tab</code> Cycle between panels <code>\u2191</code> / <code>\u2193</code> Navigate lists <code>/</code> Open search modal <code>c</code> Context expansion modal (when viewing a chunk) <code>v</code> Visual grounding modal (when viewing a chunk) <code>q</code> Quit <p>Mouse: Click to select, scroll to view content.</p>"},{"location":"apps/#search","title":"Search","text":"<p>Press <code>/</code> to open the full-screen search modal:</p> <ul> <li>Enter your query and press <code>Enter</code> to search</li> <li>Left panel: Search results with relevance scores <code>[0.95] content preview</code></li> <li>Right panel: Full chunk content and metadata</li> <li>Use <code>\u2191</code> / <code>\u2193</code> to navigate results</li> <li>Press <code>Enter</code> on a result to navigate to that document/chunk</li> <li>Press <code>Esc</code> to close search</li> </ul> <p>Search uses hybrid (vector + full-text) search across all chunks.</p>"},{"location":"apps/#context-expansion","title":"Context Expansion","text":"<p>Press <code>c</code> while viewing a chunk to see the expanded context that would be provided to the QA agent:</p> <ul> <li>Type-aware expansion: tables, code blocks, and lists expand to their complete structures</li> <li>Text content expands based on <code>search.context_radius</code> setting</li> <li>Includes metadata like source document, content type, and relevance score</li> </ul>"},{"location":"apps/#visual-grounding","title":"Visual Grounding","text":"<p>Visual grounding shows exactly where a chunk appears in the original document by highlighting its bounding box on the page image. This helps verify chunk boundaries and understand how content was extracted.</p> <p>Press <code>v</code> while viewing a chunk to see page images with the chunk's location highlighted:</p> <ul> <li>Bounding boxes show the exact region of the page that maps to the chunk</li> <li>Use <code>\u2190</code> / <code>\u2192</code> arrow keys to navigate between pages when a chunk spans multiple pages</li> <li>Press <code>Esc</code> to close the modal</li> </ul> <p></p>"},{"location":"apps/#requirements","title":"Requirements","text":"<ul> <li>Page images: Documents must be processed with Docling's page image extraction enabled (default for PDFs)</li> <li>Terminal image support: Your terminal must support inline images (e.g., iTerm2, WezTerm, Kitty). Terminals without image support will show a placeholder message.</li> <li>DoclingDocument storage: Text-only documents (plain text, markdown added via <code>add</code>) don't have visual grounding available</li> </ul> <p>Tip</p> <p>You can also view visual grounding from the command line with <code>haiku-rag visualize &lt;chunk_id&gt;</code>. See CLI documentation for details.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>High-level overview of haiku.rag components and data flow.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<pre><code>flowchart TB\n    subgraph Sources[\"Document Sources\"]\n        Files[Files]\n        URLs[URLs]\n        Text[Text]\n    end\n\n    subgraph Processing[\"Processing Pipeline\"]\n        Converter[Converter]\n        Chunker[Chunker]\n        Embedder[Embedder]\n    end\n\n    subgraph Storage[\"Storage Layer\"]\n        LanceDB[(LanceDB)]\n    end\n\n    subgraph Agents[\"Agent Layer\"]\n        QA[QA Agent]\n        Skill[RAG Skill]\n        Research[Research Graph]\n        RLM[RLM Agent]\n    end\n\n    subgraph Apps[\"Applications\"]\n        CLI[CLI]\n        ChatTUI[Chat TUI]\n        WebApp[Web App]\n        Inspector[Inspector]\n        MCP[MCP Server]\n    end\n\n    Sources --&gt; Converter\n    Converter --&gt; Chunker\n    Chunker --&gt; Embedder\n    Embedder --&gt; LanceDB\n\n    LanceDB --&gt; Agents\n    Agents --&gt; Apps</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#storage-layer","title":"Storage Layer","text":"<p>LanceDB provides vector storage with full-text search capabilities:</p> <ul> <li>DocumentRecord - Document metadata and full content</li> <li>ChunkRecord - Text chunks with embeddings and structural metadata</li> <li>SettingsRecord - Database configuration and version info</li> </ul> <p>Repositories handle CRUD operations:</p> <ul> <li><code>DocumentRepository</code> - Create, read, update, delete documents</li> <li><code>ChunkRepository</code> - Chunk management and hybrid search</li> <li><code>SettingsRepository</code> - Configuration persistence</li> </ul>"},{"location":"architecture/#processing-pipeline","title":"Processing Pipeline","text":"<pre><code>flowchart LR\n    Source[Source] --&gt; Converter\n    Converter --&gt; DoclingDoc[DoclingDocument]\n    DoclingDoc --&gt; Chunker\n    Chunker --&gt; Chunks[Chunks]\n    Chunks --&gt; Embedder\n    Embedder --&gt; Vectors[Vectors]\n    Vectors --&gt; DB[(LanceDB)]</code></pre> <p>Converters transform sources into DoclingDocuments:</p> <ul> <li><code>docling-local</code> - Local Docling processing</li> <li><code>docling-serve</code> - Remote processing via docling-serve</li> </ul> <p>Chunkers split documents into semantic chunks:</p> <ul> <li>Preserves document structure (tables, lists, code blocks)</li> <li>Maintains provenance (page numbers, headings)</li> <li>Configurable chunk size</li> </ul> <p>Embedders generate vector representations:</p> Provider Models Ollama nomic-embed-text, mxbai-embed-large OpenAI text-embedding-3-small, text-embedding-3-large VoyageAI voyage-3, voyage-code-3 vLLM Any compatible model LM Studio Any compatible model"},{"location":"architecture/#agent-layer","title":"Agent Layer","text":"<p>Three agent types and a RAG skill for different use cases:</p> <pre><code>flowchart TB\n    subgraph QA[\"QA Agent\"]\n        Q1[Question] --&gt; S1[Search]\n        S1 --&gt; A1[Answer]\n    end\n\n    subgraph Skill[\"RAG Skill\"]\n        Q2[Question] --&gt; Tools[Tool Selection]\n        Tools --&gt; S2[Search / Ask / Analyze]\n        S2 --&gt; A2[Answer]\n        A2 --&gt; State[RAG State]\n        State -.-&gt; Q2\n    end\n\n    subgraph Research[\"Research Graph\"]\n        Q3[Question] --&gt; Plan[Plan Next]\n        Plan --&gt; SearchOne[Search One]\n        SearchOne --&gt; Eval[Evaluate]\n        Eval --&gt;|Continue| Plan\n        Eval --&gt;|Done| Synthesize[Synthesize]\n    end\n\n    subgraph RLM[\"RLM Agent\"]\n        Q4[Question] --&gt; Code[Write Code]\n        Code --&gt; Execute[Execute]\n        Execute --&gt; Examine[Examine Results]\n        Examine --&gt;|Iterate| Code\n        Examine --&gt;|Done| A4[Answer]\n    end</code></pre> <p>QA Agent - Single-turn question answering:</p> <ul> <li>Searches for relevant chunks</li> <li>Expands context around results</li> <li>Generates answer with optional citations</li> </ul> <p>RAG Skill - Multi-turn conversational RAG via haiku.skills:</p> <ul> <li>Bundles search, list_documents, get_document, ask, analyze, and research tools</li> <li>Managed <code>RAGState</code> for session state (citations, QA history, document filters)</li> <li>Integrates with any pydantic-ai agent via <code>SkillToolset</code></li> <li>Powers both the Chat TUI and web application</li> </ul> <p>Research Graph - Iterative research workflow:</p> <ul> <li>Proposes one question at a time, evaluates the answer, then decides whether to continue</li> <li>Prior answers let the planner skip redundant searches</li> <li>Synthesizes structured report</li> </ul> <p>RLM Agent - Complex analytical tasks via code execution:</p> <ul> <li>Writes Python code to explore the knowledge base</li> <li>Executes in sandboxed environment</li> <li>Handles aggregation, computation, multi-document analysis</li> <li>Iterates until answer is found</li> </ul>"},{"location":"architecture/#applications","title":"Applications","text":"Application Interface Use Case CLI Command line Scripts, one-off queries, batch processing Chat TUI Terminal Interactive conversations Web App Browser Team collaboration, visual interface Inspector Terminal Database exploration, debugging MCP Server Protocol AI assistant integration"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#document-ingestion","title":"Document Ingestion","text":"<pre><code>sequenceDiagram\n    participant User\n    participant CLI\n    participant Converter\n    participant Chunker\n    participant Embedder\n    participant DB as LanceDB\n\n    User-&gt;&gt;CLI: add-src document.pdf\n    CLI-&gt;&gt;Converter: Convert to DoclingDocument\n    Converter--&gt;&gt;CLI: DoclingDocument\n    CLI-&gt;&gt;Chunker: Split into chunks\n    Chunker--&gt;&gt;CLI: Chunks with metadata\n    CLI-&gt;&gt;Embedder: Generate embeddings\n    Embedder--&gt;&gt;CLI: Vectors\n    CLI-&gt;&gt;DB: Store document + chunks\n    DB--&gt;&gt;User: Document ID</code></pre>"},{"location":"architecture/#search-and-qa","title":"Search and QA","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Agent\n    participant Embedder\n    participant DB as LanceDB\n    participant LLM\n\n    User-&gt;&gt;Agent: Ask question\n    Agent-&gt;&gt;Embedder: Embed query\n    Embedder--&gt;&gt;Agent: Query vector\n    Agent-&gt;&gt;DB: Hybrid search\n    DB--&gt;&gt;Agent: Relevant chunks\n    Agent-&gt;&gt;Agent: Expand context\n    Agent-&gt;&gt;LLM: Generate answer\n    LLM--&gt;&gt;Agent: Answer + citations\n    Agent--&gt;&gt;User: Response</code></pre>"},{"location":"architecture/#configuration","title":"Configuration","text":"<p>Configuration flows through the system:</p> <pre><code>CLI args \u2192 Environment variables \u2192 haiku.rag.yaml \u2192 Defaults\n</code></pre> <p>Key configuration areas:</p> <ul> <li>Storage - Database path, vacuum settings</li> <li>Embeddings - Provider, model, dimensions</li> <li>Processing - Chunk size, converter, chunker</li> <li>Search - Limits, context expansion</li> <li>QA/Research - Model, iterations, concurrency</li> <li>Providers - Ollama, vLLM, docling-serve URLs</li> </ul> <p>See Configuration for details.</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>We evaluate <code>haiku.rag</code> on several datasets to measure both retrieval quality and question-answering accuracy.</p>"},{"location":"benchmarks/#running-evaluations","title":"Running Evaluations","text":"<p>You can run evaluations with the <code>evaluations</code> CLI:</p> <pre><code>evaluations run repliqa\nevaluations run wix\n</code></pre> <p>The evaluation flow is orchestrated with <code>pydantic-evals</code>, which we leverage for dataset management, scoring, and report generation.</p>"},{"location":"benchmarks/#pre-built-databases","title":"Pre-built Databases","text":"<p>Building evaluation databases from scratch can take a long time, especially for large datasets like OpenRAG Bench. Pre-built databases are available on HuggingFace:</p> <pre><code># Download a specific dataset\nevaluations download repliqa\n\n# Download all datasets\nevaluations download all\n\n# Force re-download (overwrite existing)\nevaluations download repliqa --force\n</code></pre> <p>Available datasets:</p> Dataset Size <code>repliqa</code> ~30MB <code>hotpotqa</code> ~331MB <code>wix</code> ~511MB <code>open_rag_bench</code> ~14GB <p>After downloading, run benchmarks with <code>--skip-db</code> to use the pre-built database:</p> <pre><code>evaluations run repliqa --skip-db\n</code></pre>"},{"location":"benchmarks/#configuration","title":"Configuration","text":"<p>The benchmark script accepts several options:</p> <pre><code>evaluations run repliqa --config /path/to/haiku.rag.yaml --db /path/to/custom.lancedb\n</code></pre> <p>Options:</p> <ul> <li><code>--config PATH</code> - Specify a custom <code>haiku.rag.yaml</code> configuration file</li> <li><code>--db PATH</code> - Override the database path (default: platform-specific user data directory)</li> <li><code>--skip-db</code> - Skip updating the evaluation database</li> <li><code>--skip-retrieval</code> - Skip retrieval benchmark</li> <li><code>--skip-qa</code> - Skip QA benchmark</li> <li><code>--limit N</code> - Limit number of test cases</li> <li><code>--name NAME</code> - Override the evaluation name</li> </ul> <p>If no config file is specified, the script searches standard locations: <code>./haiku.rag.yaml</code>, user config directory, then falls back to defaults.</p>"},{"location":"benchmarks/#methodology","title":"Methodology","text":""},{"location":"benchmarks/#retrieval-metrics","title":"Retrieval Metrics","text":"<p>Mean Reciprocal Rank (MRR) - Used when each query has exactly one relevant document.</p> <ul> <li>For each query, find the rank (position) of the first relevant document in top-K results</li> <li>Reciprocal rank = <code>1/rank</code> (e.g., rank 3 \u2192 1/3 \u2248 0.333)</li> <li>If not found in top-K, score is 0</li> <li>MRR is the mean across all queries</li> <li>Range: 0 (never found) to 1 (always at rank 1)</li> </ul> <p>Mean Average Precision (MAP) - Used when queries have multiple relevant documents.</p> <ul> <li>For each relevant document at position k, calculate precision@k = (relevant docs in top k) / k</li> <li>Average Precision (AP) = mean of these precision values / total relevant documents</li> <li>MAP is the mean of AP scores across all queries</li> <li>Range: 0 to 1; rewards ranking relevant documents higher</li> </ul>"},{"location":"benchmarks/#qa-accuracy","title":"QA Accuracy","text":"<p>For question-answering evaluation, <code>pydantic-evals</code> coordinates an LLM judge (Ollama <code>qwen3</code>) to determine whether answers are correct. Accuracy is the fraction of correctly answered questions.</p>"},{"location":"benchmarks/#repliqa","title":"RepliQA","text":"<p>RepliQA contains synthetic news stories with question-answer pairs. We use <code>News Stories</code> from <code>repliqa_3</code> (1035 documents). Each question has exactly one relevant document, so we use MRR for retrieval evaluation.</p> <p>Results from v0.19.6</p>"},{"location":"benchmarks/#retrieval-mrr","title":"Retrieval (MRR)","text":"Embedding Model MRR Reranker Ollama / <code>qwen3-embedding:8b</code> 0.91 -"},{"location":"benchmarks/#qa-accuracy_1","title":"QA Accuracy","text":"Embedding Model QA Model Accuracy Reranker Ollama / <code>qwen3-embedding:4b</code> Ollama / <code>gpt-oss</code> - no thinking 0.82 None Ollama / <code>qwen3-embedding:8b</code> Ollama / <code>gpt-oss</code> - thinking 0.89 None Ollama / <code>mxbai-embed-large</code> Ollama / <code>qwen3</code> - thinking 0.85 None Ollama / <code>mxbai-embed-large</code> Ollama / <code>qwen3</code> - thinking 0.87 <code>mxbai-rerank-base-v2</code> Ollama / <code>mxbai-embed-large</code> Ollama / <code>qwen3:0.6b</code> 0.28 None <p>Note the significant degradation when very small models are used such as <code>qwen3:0.6b</code>.</p>"},{"location":"benchmarks/#wix","title":"Wix","text":"<p>WixQA contains real customer support questions paired with curated answers from Wix. The benchmark follows the evaluation protocol from the WixQA paper. Each query can have multiple relevant passages, so we use MAP for retrieval evaluation.</p> <p>We benchmark both the plain text version (HTML stripped, no structure) and HTML version. Since HTML chunks are small (typically a phrase), we use <code>chunk_radius=2</code> to expand context.</p> <p>Results from v0.27.2</p>"},{"location":"benchmarks/#retrieval-map","title":"Retrieval (MAP)","text":"Embedding Model Chunk size MAP Reranker Notes <code>qwen3-embedding:4b</code> 256 0.34 None html, <code>chunk-radius=2</code> <code>qwen3-embedding:4b</code> 256 0.39 <code>mxbai-rerank-base-v2</code> html, <code>chunk-radius=2</code> <code>qwen3-embedding:4b</code> 256 0.43 None plain text, <code>chunk-radius=0</code> <code>qwen3-embedding:4b</code> 512 0.45 None plain text, <code>chunk-radius=0</code>"},{"location":"benchmarks/#qa-accuracy_2","title":"QA Accuracy","text":"Embedding Model Chunk size QA Model Accuracy Notes <code>qwen3-embedding:4b</code> 256 <code>gpt-oss:20b</code> - thinking 0.82 html, <code>chunk-radius=2</code> <code>qwen3-embedding:4b</code> 256 <code>gpt-oss:20b</code> - no thinking 0.80 html, <code>chunk-radius=2</code> <code>qwen3-embedding:4b</code> 256 <code>gpt-oss:20b</code> - no thinking 0.83 html, <code>chunk-radius=2</code>, <code>jinaai/jina-reranker-v3</code>"},{"location":"benchmarks/#hotpotqa","title":"HotpotQA","text":"<p>HotpotQA is a multi-hop question answering dataset requiring reasoning over multiple Wikipedia paragraphs. Each question requires evidence from 2+ documents, making it ideal for testing retrieval and reasoning capabilities. We use MAP for retrieval evaluation since queries have multiple relevant documents.</p> <p>Results from v0.20.2</p>"},{"location":"benchmarks/#retrieval-map_1","title":"Retrieval (MAP)","text":"Embedding Model MAP Reranker <code>qwen3-embedding:4b</code> 0.69 none"},{"location":"benchmarks/#qa-accuracy_3","title":"QA Accuracy","text":"Embedding Model QA Model Accuracy <code>qwen3-embedding:4b</code> <code>gpt-oss:20b</code> - thinking 0.86"},{"location":"benchmarks/#openrag-bench-orb","title":"OpenRAG Bench (ORB)","text":"<p>OpenRAG Bench contains ArXiv research papers with multimodal question-answering pairs. Queries include both text-based and image-based questions, testing retrieval over visual content like figures, charts, and diagrams. We use MAP for retrieval evaluation since each query maps to one relevant document.</p> <p>Multimodal processing: Picture descriptions are generated using a Vision Language Model (VLM) during document conversion, making embedded images searchable via text queries. See Picture Description configuration.</p> <p>Results from v0.26.8</p>"},{"location":"benchmarks/#retrieval-map_2","title":"Retrieval (MAP)","text":"Embedding Model MAP VLM <code>qwen3-embedding:4b</code> 0.9626 Ollama / ministral-3"},{"location":"benchmarks/#qa-accuracy_4","title":"QA Accuracy","text":"Embedding Model QA Model Accuracy VLM <code>qwen3-embedding:4b</code> <code>gpt-oss:20b</code> - no thinking 0.912 Ollama / ministral-3"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#0322-2026-02-28","title":"0.32.2 - 2026-02-28","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Compatibility with haiku.skills 0.5.1: Replaced removed <code>SkillToolset.system_prompt</code> with <code>build_system_prompt(toolset.skill_catalog)</code> across chat TUI, backend app, and examples</li> <li>Minimum dependency: Bumped <code>haiku.skills</code> requirement to <code>&gt;=0.5.1</code></li> <li>Chat model default: Chat TUI and backend app now use the configured QA model instead of hardcoded <code>openai:gpt-4o</code></li> </ul>"},{"location":"changelog/#0321-2026-02-26","title":"0.32.1 - 2026-02-26","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Automatic title generation: Documents can now have titles auto-generated during ingestion via <code>processing.auto_title: true</code>. Uses two-tier extraction: structural metadata from DoclingDocument (HTML <code>&lt;title&gt;</code>, h1, section headers) first, with LLM fallback via configurable <code>processing.title_model</code></li> <li><code>generate_title()</code>: Public method on <code>HaikuRAG</code> to generate a title for an existing document on demand</li> <li><code>rebuild --title-only</code>: New rebuild mode that generates titles only for untitled documents without re-chunking or re-embedding</li> <li><code>add --title</code>: CLI option to set a title when adding text documents</li> </ul>"},{"location":"changelog/#0320-2026-02-24","title":"0.32.0 - 2026-02-24","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>RLM sandbox: Replaced Docker-based code execution with pydantic-monty, a minimal secure Python interpreter written in Rust. Eliminates Docker as a runtime dependency for RLM with sub-millisecond sandbox startup</li> <li>RLM sandbox functions: Added <code>get_chunk(chunk_id)</code> for retrieving chunk content and metadata from search results. <code>get_docling_document(document_id)</code> now returns the full document structure as a JSON dict. All sandbox functions now require <code>await</code></li> <li><code>RLMConfig</code>: Removed <code>docker_image</code> and <code>docker_memory_limit</code> fields</li> </ul>"},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>RLM sandbox regex functions: <code>regex_findall</code>, <code>regex_sub</code>, <code>regex_search</code>, <code>regex_split</code> for pattern matching without LLM calls</li> <li><code>HaikuRAG.get_chunk_by_id()</code>: Public method for chunk lookup by ID</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li><code>docker_sandbox.py</code>, <code>runner.py</code>: Docker container plumbing replaced by <code>sandbox.py</code></li> </ul>"},{"location":"changelog/#0311-2026-02-20","title":"0.31.1 - 2026-02-20","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li><code>info</code> and <code>history</code> commands: Open database in read-only mode to prevent write failures on read-only filesystems</li> </ul>"},{"location":"changelog/#0310-2026-02-20","title":"0.31.0 - 2026-02-20","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>RAG skill (<code>haiku.rag.skills.rag</code>): haiku.skills integration with search, list_documents, get_document, ask, and research tools plus managed <code>RAGState</code></li> <li>RLM skill (<code>haiku.rag.skills.rlm</code>): haiku.skills integration with analyze tool for computational analysis via code execution</li> <li><code>HaikuRAG.research()</code>: Client method for multi-agent research</li> <li>haiku.skills entry points: <code>rag = \"haiku.rag.skills.rag:create_skill\"</code>, <code>rag-rlm = \"haiku.rag.skills.rlm:create_skill\"</code></li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Chat TUI: Rebuilt on RAG skill + haiku.skills <code>SkillToolset</code></li> <li>Web app backend: Rebuilt on RAG skill + <code>AGUIAdapter</code></li> <li>Toolsets simplified: Removed <code>ToolContext</code>, <code>SessionState</code>, <code>AgentDeps</code>, <code>Toolkit</code>; kept core <code>FunctionToolset</code> factories</li> <li>Research graph: Removed <code>session_context</code> and conversational output mode</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li><code>agents/chat/</code>: Entire chat agent module (replaced by RAG skill)</li> <li><code>--deep</code> flag: Removed from <code>ask</code> CLI (use <code>research</code> command instead)</li> <li><code>--context</code>/<code>--context-file</code>: Removed from <code>ask</code> CLI</li> <li><code>tools/</code> state machinery: <code>ToolContext</code>, <code>ToolContextCache</code>, <code>SessionState</code>, <code>AgentDeps</code>, <code>Toolkit</code>, etc.</li> </ul>"},{"location":"changelog/#0302-2026-02-19","title":"0.30.2 - 2026-02-19","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Added <code>cachetools</code> as an explicit dependency (was only available transitively, causing <code>ModuleNotFoundError</code> for some installations)</li> <li>download-models: Show actionable error message when Ollama is not running instead of cryptic \"All connection attempts failed\" (#277)</li> </ul>"},{"location":"changelog/#0301-2026-02-17","title":"0.30.1 - 2026-02-17","text":""},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>AG-UI state sync: <code>ask</code> tool now emits <code>StateDeltaEvent</code> (JSON Patch) instead of <code>StateSnapshotEvent</code>, consistent with the <code>search</code> tool</li> </ul>"},{"location":"changelog/#0300-2026-02-16","title":"0.30.0 - 2026-02-16","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Composable toolsets: New <code>haiku.rag.tools</code> module with reusable <code>FunctionToolset</code> factories that can be mixed into any pydantic-ai agent</li> <li><code>create_search_toolset()</code> \u2014 hybrid search with context expansion and citation tracking</li> <li><code>create_document_toolset()</code> \u2014 document listing, retrieval, and summarization</li> <li><code>create_qa_toolset()</code> \u2014 question answering via research graph with prior answer recall</li> <li><code>create_analysis_toolset()</code> \u2014 computational analysis via RLM agent (Docker sandbox)</li> <li><code>Toolkit</code> and <code>build_toolkit()</code>: High-level factory that bundles toolsets, prompt, and context creation for a given feature set. Reduces agent composition from ~15 lines to ~5. <code>build_chat_toolkit()</code> adds chat-specific defaults (background summarization callback)</li> <li><code>ToolContext</code>: Namespace-based state container shared across toolsets. Toolsets register Pydantic models under string namespaces, enabling state accumulation (search results, citations, QA history) across invocations</li> <li><code>ToolContextCache</code>: In-memory TTL-based cache for <code>ToolContext</code> instances, keyed by external session/thread ID. Replaces module-level caches for embeddings and summaries</li> <li><code>run_qa_core()</code>: Extracted core QA function for direct programmatic use without an agent</li> <li>Feature-based chat agent: <code>create_chat_agent()</code> accepts a <code>features</code> list to select which toolsets are enabled (<code>search</code>, <code>documents</code>, <code>qa</code>, <code>analysis</code>). System prompt is composed to match</li> <li>New documentation: <code>docs/tools.md</code> covers all toolsets, <code>ToolContext</code>, state management, filter helpers, and composing custom agents</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Toolset factories decoupled from runtime dependencies: <code>create_search_toolset()</code>, <code>create_qa_toolset()</code>, <code>create_document_toolset()</code>, <code>create_analysis_toolset()</code>, and <code>create_chat_agent()</code> no longer take <code>client</code> or <code>context</code> parameters. Instead, tool functions receive these via pydantic-ai's <code>RunContext.deps</code>. This enables toolset and agent creation at configuration time (cacheable, created once), with only lightweight deps created per-request. Deps must satisfy the <code>RAGDeps</code> protocol (<code>client: HaikuRAG</code>, <code>tool_context: ToolContext | None</code>)</li> <li>Toolset factory return types narrowed to <code>FunctionToolset[RAGDeps]</code>: All four toolset factories now declare their return type as <code>FunctionToolset[RAGDeps]</code> instead of bare <code>FunctionToolset</code></li> <li><code>create_chat_agent()</code> accepts optional <code>toolkit</code> parameter: Pass a pre-built <code>Toolkit</code> to share toolsets between agent and context creation, avoiding duplicate construction</li> <li><code>ChatDeps</code> now includes <code>client</code>: <code>ChatDeps(config=..., client=..., tool_context=...)</code> \u2014 the <code>client</code> field was added since it's no longer captured by the agent factory</li> <li><code>prepare_chat_context()</code> helper: Extracted from <code>create_chat_agent()</code> for idempotent namespace registration, since the agent factory no longer has access to the context</li> <li>Chat agent architecture: Rebuilt on composable toolsets instead of monolithic tool definitions. Chat agent is now a thin wrapper around <code>create_search_toolset</code>, <code>create_document_toolset</code>, <code>create_qa_toolset</code>, and <code>create_analysis_toolset</code></li> <li>State management simplified: Removed <code>session_id</code>, <code>incoming_session_id</code>, and <code>incoming_session_context</code> from the state layer. <code>ToolContextCache</code> preserves all state (embeddings, summaries, QA history) on cached <code>ToolContext</code> instances, eliminating the need for module-level caches</li> <li>AG-UI state sync: <code>ask</code> tool now emits <code>StateSnapshotEvent</code> instead of <code>StateDeltaEvent</code>, ensuring background summarization results are reliably delivered to clients</li> <li>TUI simplified: Chat TUI reads directly from <code>ToolContext</code> namespace states instead of maintaining a separate <code>ChatSessionState</code> and manually syncing via AG-UI state events</li> <li>AG-UI web app: Uses <code>ToolContextCache</code> to maintain per-thread state across requests</li> <li>Frontend session management: Persistent chat sessions with localStorage, wired to backend <code>ToolContextCache</code> via CopilotKit <code>threadId</code></li> <li>Session manager dropdown: create, switch, delete, and export sessions to markdown</li> <li>Messages, chat state, and citations restored on session switch</li> <li>Session title derived from first user message</li> <li>Inline citation blocks injected after assistant responses via <code>qa_history</code> correlation</li> </ul>"},{"location":"changelog/#removed_2","title":"Removed","text":"<ul> <li><code>SearchAgent</code>: Replaced by <code>create_search_toolset()</code></li> <li>Module-level session caches: <code>_session_cache</code>, <code>cache_session_context</code>, <code>get_cached_session_context</code>, <code>cache_question_embedding</code>, <code>get_cached_embedding</code> \u2014 all replaced by cached <code>ToolContext</code></li> <li><code>ChatSessionState</code> from TUI: TUI no longer maintains its own copy of session state</li> </ul>"},{"location":"changelog/#0291-2026-02-10","title":"0.29.1 - 2026-02-10","text":""},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Document listing memory usage: <code>list_documents</code> no longer loads full document content and docling blobs by default, preventing out-of-memory errors on large databases. Use <code>include_content=True</code> when content is needed.</li> <li>Chat session_id not persisting across AG-UI requests: <code>ChatSessionState.session_id</code> now defaults to <code>\"\"</code> instead of auto-generating a UUID. This ensures the session_id assignment is detected as a state change and included in the <code>StateDeltaEvent</code> delta, allowing clients to persist it across requests.</li> </ul>"},{"location":"changelog/#0290-2026-02-06","title":"0.29.0 - 2026-02-06","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>docling-serve Chunker OCR Options: The docling-serve chunker now respects OCR settings from <code>conversion_options</code></li> <li>Passes <code>do_ocr</code>, <code>force_ocr</code>, <code>ocr_engine</code>, and <code>ocr_lang</code> to the chunking API</li> <li>Allows disabling OCR via config when running docling-serve in read-only containers</li> <li>RLM Agent (Recursive Language Model): New agent for complex analytical tasks via sandboxed Python code execution</li> <li>Solves problems traditional RAG can't handle: aggregation, computation, multi-document analysis</li> <li>Docker-based sandbox with full Python environment (no import restrictions)</li> <li>Container reuse within a single <code>rlm()</code> call for reduced latency</li> <li>Available functions: <code>search()</code>, <code>list_documents()</code>, <code>get_document()</code>, <code>get_docling_document()</code>, <code>llm()</code></li> <li>Pre-loaded documents support via <code>documents</code> variable</li> <li>Context filter for scoping searches without LLM control</li> <li>New <code>client.rlm(question)</code> method on HaikuRAG client</li> <li>New <code>haiku-rag rlm</code> CLI command</li> <li>New <code>rlm_question</code> MCP tool</li> <li>New config options: <code>docker_image</code>, <code>docker_memory_limit</code></li> <li>CI: Docker sandbox integration tests run in GitHub Actions</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>CI: Cache HuggingFace tokenizer to prevent flaky test failures when HuggingFace has transient outages</li> </ul>"},{"location":"changelog/#0280-2026-01-31","title":"0.28.0 - 2026-01-31","text":""},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Iterative Research Planning: Research graph now uses an iterative feedback loop instead of batch question processing</li> <li>Planner proposes ONE question at a time, sees the answer, then decides whether to continue</li> <li>Removes <code>gather_context</code> tool \u2014 planner proposes questions directly</li> <li>Simpler flow: <code>plan_next</code> \u2192 <code>search_one</code> \u2192 loop back until complete \u2192 <code>synthesize</code></li> <li>Consolidated <code>build_conversational_graph()</code> into <code>build_research_graph(output_mode=\"conversational\")</code></li> </ul>"},{"location":"changelog/#removed_3","title":"Removed","text":"<ul> <li>Dead config options: Removed vestigial fields from iterative planning refactor</li> <li><code>confidence_threshold</code> from <code>ResearchConfig</code> and <code>ResearchState</code> (LLM decides completion via <code>is_complete</code>)</li> <li><code>max_sub_questions</code> from <code>QAConfig</code> (iterative flow uses one question at a time)</li> <li><code>sub_questions</code> field from <code>ResearchContext</code> (no longer populated)</li> </ul>"},{"location":"changelog/#0272-2026-01-29","title":"0.27.2 - 2026-01-29","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Deep Ask Evaluations: QA benchmarks can now use the research graph for multi-step reasoning</li> <li>New <code>--deep</code> flag on <code>evaluations run</code> enables deep ask mode</li> <li>Uses research graph with <code>max_iterations=2</code> and <code>confidence_threshold=0.0</code></li> <li>Evaluation name automatically suffixed with <code>_deep</code> when enabled</li> <li>Experiment metadata includes <code>deep_ask</code> field for tracking</li> <li>Chat Agent Document Awareness Tools: Two new tools for browsing and understanding the knowledge base</li> <li><code>list_documents</code> \u2014 Returns <code>DocumentListResponse</code> with paginated documents (50 per page), page number, total pages, and total count; respects session document filter</li> <li><code>summarize_document</code> \u2014 Generate LLM-powered summaries of specific documents</li> <li>Document Count API: New <code>count_documents(filter)</code> method on <code>HaikuRAG</code> client for efficient document counting</li> <li>Read-Only Initial Context: Initial context is now locked after the first message, providing consistent session context</li> <li>Chat TUI: <code>--initial-context</code> CLI option sets background context for the session</li> <li>Context can be edited via command palette before the first message is sent</li> <li>After first message, context becomes read-only (view only)</li> <li>Clearing chat resets context to CLI value and unlocks editing</li> <li>Web app: Memory panel now serves dual purpose - edit initial context before first message, view session context after</li> <li>Agent uses <code>initial_context</code> as fallback when <code>session_context</code> is empty</li> </ul>"},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>AG-UI State Delta Updates: Web application now sends <code>StateDeltaEvent</code> (JSON Patch RFC 6902) instead of full <code>StateSnapshotEvent</code> for state updates</li> <li>Reduces bandwidth when state grows large (e.g., 50 Q&amp;As with citations)</li> <li>First request still sends full snapshot; subsequent requests send only changes</li> <li>Backend logging shows incoming/outgoing state events for debugging</li> </ul>"},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Chat TUI Session State Sync: TUI now syncs full session state from AG-UI events</li> </ul>"},{"location":"changelog/#0271-2026-01-27","title":"0.27.1 - 2026-01-27","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Initial Context for Chat Sessions: New <code>initial_context</code> field on <code>ChatSessionState</code> allows external clients to seed sessions with background context</li> <li>Static context set once at session creation, used as fallback when no cached session context exists</li> <li>Incorporated into first summarization, after which evolved <code>session_context</code> takes precedence</li> <li>Eliminates need for clients to import and call internal cache functions (<code>cache_session_context</code>, <code>get_cached_session_context</code>)</li> <li><code>session_id</code> now auto-generates a UUID if not provided (previously defaulted to empty string)</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>AG-UI StateSnapshotEvent JSON Serialization: Chat agent tools now use <code>model_dump(mode=\"json\")</code> when creating <code>StateSnapshotEvent</code></li> <li>Fixes <code>TypeError: Object of type datetime is not JSON serializable</code> when external clients persist AG-UI state to database JSON columns</li> </ul>"},{"location":"changelog/#0270-2026-01-26","title":"0.27.0 - 2026-01-26","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>Evaluation Database Hosting: Pre-built evaluation databases available on HuggingFace</li> <li><code>evaluations download &lt;dataset&gt;</code> downloads pre-built databases from <code>ggozad/haiku-rag-eval-dbs</code></li> <li><code>evaluations upload &lt;dataset&gt;</code> uploads databases to HuggingFace (maintainer only)</li> <li>Supports <code>all</code> argument to download/upload all datasets at once</li> <li>Use <code>--force</code> flag to overwrite existing databases</li> <li>Avoids lengthy database rebuild times for users running benchmarks</li> <li>Stable Citation Registry: Citation indices now persist across tool calls within a session</li> <li>Same <code>chunk_id</code> always returns the same citation index (first-occurrence-wins)</li> <li>New <code>citation_registry: dict[str, int]</code> field on <code>ChatSessionState</code></li> <li>New <code>get_or_assign_index(chunk_id)</code> method for stable index assignment</li> <li>Registry serialized/restored via AG-UI state protocol</li> <li>Prior Answer Recall: The <code>ask</code> tool automatically checks conversation history before research</li> <li>Finds semantically similar prior answers using embedding similarity (0.7 cosine threshold)</li> <li>Relevant prior answers are passed to the research planner as context</li> <li>Planner can return empty sub_questions when context is sufficient, avoiding redundant searches</li> <li>Dynamic Session Context: Compressed conversation history for multi-turn chat</li> <li>New <code>SessionContext</code> model stores summarized conversation state instead of raw Q&amp;A history</li> <li>Background LLM-based summarization runs after each <code>ask</code> tool call (non-blocking)</li> <li>Previous summarization tasks are cancelled when new ones start</li> <li>Research graph receives compact context (~1,000-2,000 tokens) instead of raw qa_history (potentially thousands of tokens)</li> <li>New <code>session_context</code> field on <code>ChatSessionState</code> synced via AG-UI state protocol</li> <li>Chat TUI: New context modal (<code>Ctrl+O</code>) to view current session context</li> <li>Session Document Filter: Restrict all search/ask operations to selected documents</li> <li>New <code>document_filter</code> field on <code>ChatSessionState</code> stores list of document titles/URIs</li> <li>Session filter combines with per-tool <code>document_name</code> filter using AND logic</li> <li>Multi-document selection uses OR logic within the session filter</li> <li>Filter persists across tool calls and chat clears via AG-UI state protocol</li> <li>Chat TUI: Access via command palette (\"Filter documents\" command)</li> <li>Web Application: Filter button in header shows count of selected documents</li> </ul>"},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>Dependencies: Updated core dependencies</li> <li><code>pydantic-ai-slim</code>: 1.44.0 \u2192 1.46.0</li> <li><code>lancedb</code>: 0.26.1 \u2192 0.27.0</li> <li><code>docling</code>: 2.68.0 \u2192 2.69.1</li> <li><code>docling-core</code>: 2.59.0 \u2192 2.60.1</li> <li>VoyageAI Embeddings: Now uses pydantic-ai-slim's native VoyageAI support instead of custom implementation</li> <li>Removed <code>haiku.rag.embeddings.voyageai</code> module</li> <li>The <code>voyageai</code> extra now delegates to <code>pydantic-ai-slim[voyageai]</code></li> </ul>"},{"location":"changelog/#removed_4","title":"Removed","text":"<ul> <li>Q&amp;A History Functions: Removed standalone conversation history utilities</li> <li><code>rank_qa_history_by_similarity()</code> - similarity matching now integrated into <code>ask</code> tool</li> <li><code>format_conversation_context()</code> - replaced by <code>SessionContext</code> summarization</li> <li>Associated embedding cache and helper functions also removed</li> </ul>"},{"location":"changelog/#0269-2026-01-22","title":"0.26.9 - 2026-01-22","text":""},{"location":"changelog/#fixed_7","title":"Fixed","text":"<ul> <li>v0.25.0 Migration Failure: Fixed \"Table 'documents' already exists\" error during migration caused by held table references preventing <code>drop_table()</code> from succeeding. Added recovery logic to restore documents from staging table if a previous migration attempt failed mid-way.</li> </ul>"},{"location":"changelog/#0268-2026-01-22","title":"0.26.8 - 2026-01-22","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>Jina Reranker v3: Added support for Jina reranking with API mode (<code>provider: jina</code>) and local inference (<code>provider: jina-local</code>, requires <code>[jina]</code> extra)</li> <li>Model Downloads: <code>download-models</code> now pre-downloads HuggingFace models for <code>sentence-transformers</code>, <code>mxbai</code>, and <code>jina-local</code></li> <li>Reranker Factory: Removed unreliable <code>id(config)</code>-based caching from <code>get_reranker()</code>; factory now always instantiates fresh</li> </ul>"},{"location":"changelog/#changed_7","title":"Changed","text":"<ul> <li>Agent Search Result Display: Search results now show rank position instead of raw scores</li> <li><code>SearchResult.format_for_agent()</code> accepts optional <code>rank</code> and <code>total</code> parameters</li> <li>Output changes from <code>(score: 0.02)</code> to <code>[rank 1 of 5]</code> when rank is provided</li> <li>Prevents LLMs from misinterpreting low RRF hybrid search scores as \"2% relevant\"</li> <li>QA and Research agents updated to pass rank/total to formatted results</li> <li>Agent prompts updated to reference rank-based ordering instead of scores</li> </ul>"},{"location":"changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Test Cassette Organization: Consolidated all VCR cassettes to <code>tests/cassettes/</code></li> <li>Environment Loading: Fixed <code>.env</code> file loading to search from current working directory instead of source file directory (#250) - thanks @tianyicui</li> </ul>"},{"location":"changelog/#0267-2026-01-20","title":"0.26.7 - 2026-01-20","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>OCR Engine Selection: New <code>ocr_engine</code> option in <code>conversion_options</code> to explicitly select OCR backend (#246)</li> <li>Supported engines: <code>auto</code> (default), <code>easyocr</code>, <code>rapidocr</code>, <code>tesseract</code>, <code>tesserocr</code>, <code>ocrmac</code></li> <li>Works with both <code>docling-local</code> and <code>docling-serve</code> converters</li> <li>Fixes inconsistent OCR engine selection between docling-serve startup and conversion requests</li> </ul>"},{"location":"changelog/#removed_5","title":"Removed","text":"<ul> <li>A2A Example: Removed <code>examples/a2a-server/</code> A2A protocol server example</li> <li>Stale Example References: Cleaned up references to removed <code>ag-ui-research</code> example from documentation</li> </ul>"},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li>MCP Error Handling: MCP tools now let exceptions propagate naturally; FastMCP converts them to proper MCP error responses</li> <li>Chunk Contextualization: Consolidated duplicate <code>contextualize</code> logic into <code>Chunk.contextualize_content()</code> method</li> <li>Type Checker: Replaced pyright with ty, Astral's extremely fast Python type checker</li> <li>Added explicit <code>Agent[Deps, Output]</code> type annotations to all pydantic-ai agents for better type inference</li> <li>Removed ~24 unnecessary <code># type: ignore</code> comments that ty correctly infers</li> <li>Dependencies: Updated to latest versions</li> <li><code>pydantic-ai-slim</code>: 1.39.0 \u2192 1.44.0</li> <li><code>docling</code>: 2.67.0 \u2192 2.68.0</li> <li><code>pathspec</code>: 0.12.1 \u2192 1.0.3</li> <li><code>textual</code>: 7.0.0 \u2192 7.3.0</li> <li><code>datasets</code>: 4.4.2 \u2192 4.5.0</li> <li><code>ruff</code>: 0.14.11 \u2192 0.14.13</li> <li><code>opencv-python-headless</code>: 4.12.0.88 \u2192 4.13.0.90</li> </ul>"},{"location":"changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Chat TUI: Fixed crash when logfire is installed but user is not authenticated (#247)</li> </ul>"},{"location":"changelog/#0266-2026-01-19","title":"0.26.6 - 2026-01-19","text":""},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li>Explicit Database Migrations: Database migrations are no longer applied automatically on open</li> <li>Opening a database with pending migrations now raises <code>MigrationRequiredError</code> with a clear message</li> <li>New <code>haiku-rag migrate</code> command to explicitly apply pending migrations</li> <li>Version-only updates (no schema changes) are applied silently in writable mode</li> <li>New <code>skip_migration_check</code> parameter on <code>Store</code> for tools that need to bypass the check</li> <li><code>Store.migrate()</code> method returns list of applied migration descriptions</li> </ul>"},{"location":"changelog/#0265-2026-01-16","title":"0.26.5 - 2026-01-16","text":""},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>Background Context Support: Pass background context to agents via CLI or Python API</li> <li><code>haiku-rag ask --context \"...\" --context-file path</code> for Q&amp;A with background context</li> <li><code>haiku-rag research --context \"...\" --context-file path</code> for research with background context</li> <li><code>haiku-rag chat --context \"...\" --context-file path</code> for chat sessions with persistent context</li> <li><code>ResearchContext(background_context=\"...\")</code> for Python API usage</li> <li><code>ChatSessionState(background_context=\"...\")</code> for chat agent sessions</li> <li>Context is included in agent system prompts and research graph planning</li> <li>Frontend Background Context: Settings panel in the chat app to configure persistent background context</li> <li>Context is stored in localStorage and sent with each conversation</li> <li>Frontend Linting: Added Biome for linting and formatting the frontend codebase</li> </ul>"},{"location":"changelog/#0264-2026-01-15","title":"0.26.4 - 2026-01-15","text":""},{"location":"changelog/#added_11","title":"Added","text":"<ul> <li>AGUI_STATE_KEY Constant: Exported <code>AGUI_STATE_KEY</code> (<code>\"haiku.rag.chat\"</code>) from <code>haiku.rag.agents.chat</code> for namespaced AG-UI state emission</li> <li>Enables integrators to use a consistent key when combining haiku.rag with other agents</li> <li>Backend, TUI, and frontend now use this key for state emission and extraction</li> </ul>"},{"location":"changelog/#0263-2026-01-15","title":"0.26.3 - 2026-01-15","text":""},{"location":"changelog/#added_12","title":"Added","text":"<ul> <li>Enhanced Database Info: <code>haiku-rag info</code> now displays <code>pydantic-ai</code> version and <code>docling-document schema</code> version</li> <li>Keyed State Emission for Chat Agent: New <code>state_key</code> parameter in <code>ChatDeps</code> for namespaced AG-UI state snapshots</li> <li>When set, tools emit <code>{state_key: snapshot}</code> instead of bare state, enabling state merging when multiple agents share state</li> <li>Default <code>None</code> preserves backwards compatibility (bare state emission)</li> <li>Page Image Generation Control: New <code>generate_page_images</code> option in <code>ConversionOptions</code> to control PDF page image extraction</li> </ul>"},{"location":"changelog/#changed_10","title":"Changed","text":"<ul> <li>CLI Error Handling: Commands (<code>rebuild</code>, <code>vacuum</code>, <code>create-index</code>, <code>ask</code>, <code>research</code>) now propagate errors with proper exit codes instead of swallowing exceptions</li> </ul>"},{"location":"changelog/#fixed_10","title":"Fixed","text":"<ul> <li>Embed-only rebuild with changed vector dimensions: Fixed <code>haiku-rag rebuild --embed-only</code> failing when the configured embedding model has different dimensions than the database</li> <li>Store now reads stored vector dimension when opening existing databases, allowing chunks to be read regardless of current config</li> <li><code>_rebuild_embed_only</code> recreates the chunks table to handle dimension changes</li> <li><code>generate_page_images: bool = True</code> - Enable/disable rendered page images (used by <code>visualize_chunk()</code>)</li> <li>Works with both <code>docling-local</code> and <code>docling-serve</code> converters</li> <li>For <code>docling-serve</code>, maps to <code>image_export_mode</code> API parameter (<code>embedded</code>/<code>placeholder</code>)</li> <li>Note: <code>generate_picture_images</code> (embedded figures/diagrams) works with local converter but has limited support in docling-serve</li> </ul>"},{"location":"changelog/#0262-2026-01-13","title":"0.26.2 - 2026-01-13","text":""},{"location":"changelog/#changed_11","title":"Changed","text":"<ul> <li>Dependencies: Updated docling dependencies for latest docling-serve compatibility (#229)</li> <li><code>docling-core</code>: 2.57.0 \u2192 2.59.0 (supports schema 1.9.0)</li> <li><code>docling</code>: 2.65.0 \u2192 2.67.0</li> </ul>"},{"location":"changelog/#0261-2026-01-13","title":"0.26.1 - 2026-01-13","text":""},{"location":"changelog/#fixed_11","title":"Fixed","text":"<ul> <li>Docling Schema Version Mismatch: Fixed incompatibility between <code>docling</code> and <code>docling-core</code> causing <code>ValidationError: Doc version 1.9.0 incompatible with SDK schema version 1.8.0</code> when adding documents (#229)</li> <li>Root cause: <code>docling-core</code> was reverted to 2.57.0 (schema 1.8.0) for docling-serve compatibility, but <code>docling</code> remained at 2.67.0 (schema 1.9.0)</li> <li>Fix: Reverted <code>docling</code> from 2.67.0 to 2.65.0 to match <code>docling-core</code> schema version</li> </ul>"},{"location":"changelog/#0260-2026-01-13","title":"0.26.0 - 2026-01-13","text":""},{"location":"changelog/#added_13","title":"Added","text":"<ul> <li>Conversational RAG Application: Full-stack application (<code>app/</code>) with CopilotKit frontend and pydantic-ai AG-UI backend</li> <li>Next.js frontend with chat interface, citation display, and visual grounding</li> <li>Starlette backend using pydantic-ai's native <code>AGUIAdapter</code> for streaming</li> <li>Docker Compose setup for development (<code>docker-compose.dev.yml</code>) and production</li> <li>Logfire integration for debugging LLM calls</li> <li>SSE heartbeat to prevent connection timeouts</li> <li>Chat Agent (<code>haiku.rag.agents.chat</code>): New conversational RAG agent optimized for multi-turn chat</li> <li><code>create_chat_agent()</code> factory function for creating chat agents with AG-UI support</li> <li><code>SearchAgent</code> for internal query expansion with deduplication</li> <li><code>ChatDeps</code> and <code>ChatSessionState</code> for session management</li> <li><code>CitationInfo</code> and <code>QAResponse</code> models for structured responses</li> <li>Natural language document filtering via <code>build_document_filter()</code></li> <li>Configurable search limit per agent</li> <li>Chat TUI (<code>haiku-rag chat</code>): Terminal-based chat interface using Textual</li> <li>Single chat window with inline tool calls and expandable citations</li> <li>Visual grounding (<code>v</code> key) reuses inspector's <code>VisualGroundingModal</code></li> <li>Database info (<code>i</code> key) shows document/chunk counts and storage info</li> <li>Keybindings: <code>q</code> quit, <code>Ctrl+L</code> clear chat, <code>Escape</code> focus input</li> <li>Q/A History Management: Intelligent conversation history with semantic ranking</li> <li>FIFO queue with 50 max entries</li> <li>Embedding cache to avoid re-embedding Q/A pairs</li> <li><code>rank_qa_history_by_similarity()</code> returns top-K most relevant history entries</li> <li>Confidence filtering to exclude low-confidence answers from context</li> <li>Conversational Research Graph: Simplified single-iteration research graph for chat</li> <li><code>build_conversational_graph()</code> optimized for conversational Q&amp;A</li> <li>Context-aware planning (generates fewer sub-questions when history exists)</li> <li><code>ConversationalAnswer</code> output type with direct answer and citations</li> </ul>"},{"location":"changelog/#changed_12","title":"Changed","text":"<ul> <li>BREAKING: Module Reorganization: Consolidated all agent code under <code>haiku.rag.agents</code></li> <li>Moved <code>haiku.rag.qa</code> \u2192 <code>haiku.rag.agents.qa</code></li> <li>Moved <code>haiku.rag.graph.research</code> \u2192 <code>haiku.rag.agents.research</code></li> <li>Added <code>haiku.rag.agents.chat</code> module with conversational RAG agent</li> <li>Deleted <code>haiku.rag.graph</code> module (research graph now at <code>haiku.rag.agents.research.graph</code>)</li> </ul>"},{"location":"changelog/#removed_6","title":"Removed","text":"<ul> <li>BREAKING: Custom AG-UI Infrastructure: Removed custom AG-UI event handling in favor of pydantic-ai's native AG-UI support</li> <li>Deleted <code>haiku.rag.graph.agui</code> module (<code>AGUIEmitter</code>, <code>AGUIConsoleRenderer</code>, <code>stream_graph()</code>, <code>create_agui_server()</code>)</li> <li>Removed <code>--agui</code> flag from <code>serve</code> command</li> <li>Removed <code>--verbose</code> flags from <code>ask</code> and <code>research</code> commands</li> <li>Removed <code>--interactive</code> flag from <code>research</code> command</li> <li>Removed <code>AGUIConfig</code> from configuration</li> <li>Deleted <code>cli_chat.py</code> interactive chat module</li> <li>Research graph now uses <code>graph.run()</code> directly instead of <code>stream_graph()</code></li> <li>For AG-UI streaming, use pydantic-ai's native <code>AGUIAdapter</code> with <code>ToolReturn</code> and <code>StateSnapshotEvent</code> (see <code>app/backend/</code> for example)</li> <li>AG-UI Research Example: Removed <code>examples/ag-ui-research/</code> (replaced by <code>app/</code>)</li> </ul>"},{"location":"changelog/#0250-2026-01-12","title":"0.25.0 - 2026-01-12","text":""},{"location":"changelog/#fixed_12","title":"Fixed","text":"<ul> <li>Large Document Storage Overflow: Fixed \"byte array offset overflow\" panic when vacuuming/rebuilding databases with many large PDF documents (#225)</li> <li>Root cause: Arrow's 32-bit string column offsets limited to ~2GB per fragment</li> <li>Changed <code>docling_document_json</code> (string) to <code>docling_document</code> (bytes) with <code>large_binary</code> Arrow type (64-bit offsets)</li> <li>Added gzip compression for DoclingDocument JSON (~1.4x compression ratio)</li> <li>Migration automatically compresses existing documents in batches to avoid memory issues</li> <li>Breaking: Migration is destructive - all table version history is lost after upgrade</li> </ul>"},{"location":"changelog/#changed_13","title":"Changed","text":"<ul> <li>Dependencies: Updated lancedb 0.26.0 \u2192 0.26.1, docling 2.65.0 \u2192 2.67.0</li> </ul>"},{"location":"changelog/#removed_7","title":"Removed","text":"<ul> <li>Legacy Migrations: Removed obsolete database migration files (<code>v0_9_3.py</code>, <code>v0_10_1.py</code>, <code>v0_19_6.py</code>). These migrations were for versions prior to 0.20.0 and are no longer needed since the current release requires a database rebuild anyway.</li> </ul>"},{"location":"changelog/#0242-2026-01-08","title":"0.24.2 - 2026-01-08","text":""},{"location":"changelog/#fixed_13","title":"Fixed","text":"<ul> <li>Base64 Images in Expanded Context: Fixed base64 image data leaking into expanded search results when <code>expand_context()</code> processed <code>PictureItem</code> objects. The issue was <code>PictureItem.export_to_markdown()</code> defaulting to <code>EMBEDDED</code> mode. Now explicitly uses <code>PLACEHOLDER</code> mode to prevent base64 data while still including VLM descriptions and captions.</li> </ul>"},{"location":"changelog/#0241-2026-01-08","title":"0.24.1 - 2026-01-08","text":""},{"location":"changelog/#fixed_14","title":"Fixed","text":"<ul> <li>OpenAI Non-Reasoning Models: Fixed <code>reasoning_effort</code> parameter being sent to non-reasoning OpenAI models (gpt-4o, gpt-4o-mini), causing 400 errors. Now correctly detects reasoning models (o1, o3 series) using pydantic-ai's model profile.</li> <li>Bedrock Non-Reasoning Models: Fixed same issue for OpenAI models on Bedrock.</li> </ul>"},{"location":"changelog/#0240-2026-01-07","title":"0.24.0 - 2026-01-07","text":""},{"location":"changelog/#added_14","title":"Added","text":"<ul> <li>VLM Picture Description: Describe embedded images using Vision Language Models during document conversion</li> <li>Images are sent to a VLM for automatic description via OpenAI-compatible API</li> <li>Descriptions become searchable text, improving RAG retrieval for visual content</li> <li>Configure via <code>processing.conversion_options.picture_description</code> with <code>enabled</code>, <code>model</code>, <code>timeout</code>, <code>max_tokens</code></li> <li>Default prompt customizable via <code>prompts.picture_description</code></li> <li>Requires OpenAI-compatible <code>/v1/chat/completions</code> endpoint (Ollama, OpenAI, vLLM, LM Studio)</li> </ul>"},{"location":"changelog/#0232-2026-01-05","title":"0.23.2 - 2026-01-05","text":""},{"location":"changelog/#fixed_15","title":"Fixed","text":"<ul> <li>AG-UI Concurrent Step Tracking: Emitter now correctly tracks multiple concurrent steps (#216)</li> </ul>"},{"location":"changelog/#changed_14","title":"Changed","text":"<ul> <li>Dependencies: Updated core and development dependencies</li> </ul>"},{"location":"changelog/#0231-2025-12-29","title":"0.23.1 - 2025-12-29","text":""},{"location":"changelog/#added_15","title":"Added","text":"<ul> <li>Contextualized FTS Search: Full-text search now includes section headings</li> <li>New <code>content_fts</code> column stores contextualized content (headings + body text)</li> <li>FTS index now searches <code>content_fts</code> for better keyword matching on section context</li> <li>Original <code>content</code> column preserved for display and context expansion</li> <li>Migration automatically populates <code>content_fts</code> for existing databases</li> <li>GitHub Actions CI: Test workflow runs pytest, pyright, and ruff on push/PR to main</li> <li>VCR Cassette Recording: Integration tests use recorded HTTP responses for deterministic CI runs</li> <li>LLM tests (QA, embeddings, research graph) replay from cassettes without real API calls</li> <li>docling-serve tests run without Docker container in CI</li> <li>Uses pytest-recording with custom JSON body serializer</li> </ul>"},{"location":"changelog/#0230-2025-12-26","title":"0.23.0 - 2025-12-26","text":""},{"location":"changelog/#added_16","title":"Added","text":"<ul> <li>Prompt Customization: Configure agent prompts via <code>prompts</code> config section</li> <li><code>domain_preamble</code>: Prepended to all agent prompts for domain context</li> <li><code>qa</code>: Full replacement for QA agent prompt</li> <li><code>synthesis</code>: Full replacement for research synthesis prompt</li> </ul>"},{"location":"changelog/#changed_15","title":"Changed","text":"<ul> <li>Embeddings: Migrated to pydantic-ai's embeddings module</li> <li>Uses pydantic-ai v1.39.0+ embeddings with instrumentation and token counting support</li> <li>Explicit <code>embed_query()</code> and <code>embed_documents()</code> API for query/document distinction</li> <li>New providers available: Cohere (<code>cohere:</code>), SentenceTransformers (<code>sentence-transformers:</code>)</li> <li>VoyageAI refactored to extend pydantic-ai's <code>EmbeddingModel</code> base class</li> <li>Configuration: Added <code>base_url</code> to <code>ModelConfig</code> and <code>EmbeddingModelConfig</code></li> <li>Enables custom endpoints for OpenAI-compatible providers (vLLM, LM Studio, etc.)</li> <li>Model-level <code>base_url</code> takes precedence over provider config</li> </ul>"},{"location":"changelog/#deprecated","title":"Deprecated","text":"<ul> <li>vLLM and LM Studio providers: Use <code>openai</code> provider with <code>base_url</code> instead</li> <li><code>provider: vllm</code> \u2192 <code>provider: openai</code> with <code>base_url: http://localhost:8000/v1</code></li> <li><code>provider: lm_studio</code> \u2192 <code>provider: openai</code> with <code>base_url: http://localhost:1234/v1</code></li> </ul>"},{"location":"changelog/#removed_8","title":"Removed","text":"<ul> <li>Deleted obsolete embedder implementations: <code>ollama.py</code>, <code>openai.py</code>, <code>vllm.py</code>, <code>lm_studio.py</code>, <code>base.py</code></li> <li>Removed <code>VLLMConfig</code> and <code>LMStudioConfig</code> from configuration (use <code>base_url</code> in model config instead)</li> </ul>"},{"location":"changelog/#0220-2025-12-19","title":"0.22.0 - 2025-12-19","text":""},{"location":"changelog/#added_17","title":"Added","text":"<ul> <li>Read-Only Mode: Global <code>--read-only</code> CLI flag for safe database access without modifications</li> <li>Blocks all write operations at the Store layer</li> <li>Skips database upgrades and settings saves on open</li> <li>Excludes write tools (<code>add_document_*</code>, <code>delete_document</code>) from MCP server</li> <li>Disables file monitor with warning when <code>--read-only</code> is used with <code>serve --monitor</code></li> <li>Time Travel: Query the database as it existed at a previous point in time</li> <li>Global <code>--before</code> CLI flag accepts datetime strings (ISO 8601 or date-only)</li> <li>Automatically enables read-only mode when time-traveling</li> <li>New <code>history</code> command shows version history for database tables</li> <li>Useful for debugging and auditing</li> <li>Supported throughout: CLI, Client, App, Inspector</li> </ul>"},{"location":"changelog/#fixed_16","title":"Fixed","text":"<ul> <li>File Monitor Path Validation: Monitor now validates directories exist before watching (#204)</li> <li>Provides clear error message pointing to <code>haiku.rag.yaml</code> configuration</li> <li>Prevents cryptic <code>FileNotFoundError: No path was found</code> from watchfiles</li> <li>Docker Documentation: Improved Docker setup instructions</li> <li>Added volume mount examples for config file and documents directory</li> <li>Clarified that <code>monitor.directories</code> must use container paths, not host paths</li> </ul>"},{"location":"changelog/#changed_16","title":"Changed","text":"<ul> <li>Dependencies: Updated core dependencies</li> <li><code>pydantic-ai-slim</code>: 1.27.0 \u2192 1.36.0 (FileSearchTool, web chat UI, GPT-5.2 support, prompt caching)</li> <li><code>lancedb</code>: 0.25.3 \u2192 0.26.0</li> <li><code>docling</code>: 2.64.0 \u2192 2.65.0</li> <li><code>docling-core</code>: 2.54.0 \u2192 2.57.0</li> </ul>"},{"location":"changelog/#0210-2025-12-18","title":"0.21.0 - 2025-12-18","text":""},{"location":"changelog/#added_18","title":"Added","text":"<ul> <li>Interactive Research Mode: Human-in-the-loop research using graph-based decision nodes</li> <li><code>haiku-rag research --interactive</code> starts conversational CLI chat</li> <li>Natural language interpretation for user commands (search, modify questions, synthesize)</li> <li>Chat with assistant before starting research, and during decision points</li> <li>Review collected answers and pending questions at each decision point</li> <li>Add, remove, or modify sub-questions through natural conversation</li> <li>New <code>human_decide</code> graph node emits AG-UI tool calls (<code>TOOL_CALL_START/ARGS/END</code>) for frontend integration</li> <li>New <code>emit_tool_call_start()</code>, <code>emit_tool_call_args()</code>, <code>emit_tool_call_end()</code> AG-UI event helpers</li> <li>New <code>AGUIEmitter.emit()</code> method for direct event emission</li> <li>AG-UI Research Example: Human-in-the-loop research with client-side tool calling</li> <li>Frontend handles <code>human_decision</code> tool calls via AG-UI <code>TOOL_CALL_*</code> events</li> <li>Tool results sent directly to backend <code>/v1/research/stream</code> endpoint</li> <li>Backend queues decisions and continues the research graph</li> <li>HotpotQA Evaluation: Added HotpotQA dataset adapter for multi-hop QA benchmarks</li> <li>Extracts unique documents from validation set context paragraphs</li> <li>Uses MAP for retrieval evaluation (multiple supporting documents per question)</li> <li>Run with <code>evaluations hotpotqa</code></li> <li>Plain Text Format: Added <code>format=\"plain\"</code> for text conversion</li> <li>Use when content is plain text without markdown/HTML structure</li> <li>Falls back gracefully when docling cannot detect markdown format in content</li> <li>Supported in <code>create_document()</code>, <code>convert()</code>, and all converter classes</li> </ul>"},{"location":"changelog/#changed_17","title":"Changed","text":"<ul> <li>AG-UI Events: Replaced custom event classes with <code>ag_ui.core</code> types</li> <li>Removed <code>haiku.rag.graph.agui.events</code> module</li> <li>Event factory functions (<code>emit_*</code>) now wrap official <code>ag_ui.core</code> event classes</li> <li>Chunker Sets Order: Chunkers now set <code>chunk.order</code> directly</li> <li>Unified Research Graph: Simplified and unified research and deep QA into a single configurable graph</li> <li>Removed <code>analyze_insights</code> node - graph now flows directly from <code>collect_answers</code> to <code>decide</code></li> <li>Simplified <code>EvaluationResult</code> to: <code>is_sufficient</code>, <code>confidence_score</code>, <code>reasoning</code>, <code>new_questions</code></li> <li>Simplified <code>ResearchContext</code> - removed insight/gap tracking methods</li> <li><code>ask --deep</code> now uses research graph with <code>max_iterations=2</code>, <code>confidence_threshold=0.0</code></li> <li><code>ask --deep</code> output now shows executive summary, key findings, and sources</li> <li>Added <code>include_plan</code> parameter to <code>build_research_graph()</code> for plan-less execution</li> <li>Added <code>max_iterations</code> and <code>confidence_threshold</code> overrides to <code>ResearchState.from_config()</code></li> <li>Improved Synthesis Prompt: Updated synthesis agent prompt to produce direct answers</li> <li>Executive summary now directly answers the question instead of describing the report</li> <li>Added explicit examples of good vs bad output style</li> <li>Evaluations Vacuum Strategy: <code>populate_db</code> now uses periodic vacuum to prevent disk exhaustion with large datasets</li> <li>Disables auto_vacuum during population, vacuums every N documents with retention=0</li> <li>New <code>--vacuum-interval</code> CLI option (default: 100) to control vacuum frequency</li> <li>Prevents disk space issues when building databases with thousands of documents (e.g., HotpotQA)</li> <li>Benchmarks Documentation: Restructured benchmarks.md for clarity</li> <li>Added dedicated Methodology section explaining MRR, MAP, and QA Accuracy metrics</li> <li>Organized results by dataset with retrieval and QA subsections</li> </ul>"},{"location":"changelog/#removed_9","title":"Removed","text":"<ul> <li>Deep QA Graph: Removed <code>haiku.rag.graph.deep_qa</code> module entirely</li> <li>Use <code>build_research_graph()</code> with appropriate parameters instead</li> <li><code>ask --deep</code> CLI command now uses research graph internally</li> <li>Insight/Gap Tracking: Removed over-engineered insight and gap tracking from research graph</li> <li>Removed <code>InsightRecord</code>, <code>GapRecord</code>, <code>InsightAnalysis</code>, <code>InsightStatus</code>, <code>GapSeverity</code> models</li> <li>Removed <code>format_analysis_for_prompt()</code> helper</li> <li>Removed <code>INSIGHT_AGENT_PROMPT</code> from prompts</li> </ul>"},{"location":"changelog/#0202-2025-12-12","title":"0.20.2 - 2025-12-12","text":""},{"location":"changelog/#fixed_17","title":"Fixed","text":"<ul> <li>LLM Schema Compliance: Improved prompts to prevent LLMs from returning objects instead of plain strings for <code>list[str]</code> fields</li> <li>All graph prompts now explicitly state that list fields must contain plain strings only</li> <li>Added missing <code>query</code> and <code>confidence</code> fields to search agent output format documentation</li> <li>Fixes validation errors with less capable models that ignore JSON schema constraints</li> <li>AG-UI Frontend Types: Fixed TypeScript interfaces in ag-ui-research example to match backend Python models</li> <li><code>EvaluationResult</code>: <code>confidence</code> \u2192 <code>confidence_score</code>, <code>should_continue</code> \u2192 <code>is_sufficient</code>, <code>gaps_identified</code> \u2192 <code>gaps</code>, <code>follow_up_questions</code> \u2192 <code>new_questions</code>, added <code>key_insights</code></li> <li><code>ResearchReport</code>: <code>question</code> \u2192 <code>title</code>, <code>summary</code> \u2192 <code>executive_summary</code>, <code>findings</code> \u2192 <code>main_findings</code>, removed <code>insights_used</code>/<code>methodology</code>, added <code>limitations</code>/<code>recommendations</code>/<code>sources_summary</code></li> <li>Updated Final Report UI to display new fields (Recommendations, Limitations, Sources)</li> <li>Citation Formatting: Citations in CLI now render properly with Rich panels</li> <li>Content is rendered as markdown with proper code block formatting</li> <li>No longer truncates or flattens newlines in citation content</li> </ul>"},{"location":"changelog/#0201-2025-12-11","title":"0.20.1 - 2025-12-11","text":""},{"location":"changelog/#added_19","title":"Added","text":"<ul> <li>Search Filter for Graphs: Research and Deep QA graphs now support <code>search_filter</code> parameter to restrict searches to specific documents</li> <li>Set <code>state.search_filter</code> to a SQL WHERE clause (e.g., <code>\"id IN ('doc1', 'doc2')\"</code>) before running the graph</li> <li>Enables document-scoped research workflows</li> <li>CLI: <code>haiku-rag research \"question\" --filter \"uri LIKE '%paper%'\"</code></li> <li>CLI: <code>haiku-rag ask \"question\" --filter \"title = 'My Doc'\"</code></li> <li>Python: <code>client.ask(question, filter=\"...\")</code> and <code>agent.answer(question, filter=\"...\")</code></li> <li>AG-UI Research Example: Added bidirectional state demonstration with document filter</li> <li>New <code>/api/documents</code> endpoint to list available documents</li> <li>Frontend document selector component with search and multi-select</li> <li>Demonstrates client-to-server state flow via AG-UI protocol</li> <li>Inspector Info Modal: New <code>i</code> keyboard shortcut opens a modal displaying database information</li> </ul>"},{"location":"changelog/#changed_18","title":"Changed","text":"<ul> <li>Inspector Lazy Loading: Chunks panel now loads chunks in batches of 50 with infinite scroll</li> <li>Fixes unresponsive UI when viewing documents with large numbers of chunks</li> <li>New <code>ChunkRepository.get_by_document_id()</code> pagination with <code>limit</code> and <code>offset</code> parameters</li> <li>New <code>ChunkRepository.count_by_document_id()</code> method</li> </ul>"},{"location":"changelog/#0200-2025-12-10","title":"0.20.0 - 2025-12-10","text":""},{"location":"changelog/#added_20","title":"Added","text":"<ul> <li>DoclingDocument Storage: Full DoclingDocument JSON is now stored with each document, enabling rich context and visual grounding</li> <li>Documents store the complete DoclingDocument structure (JSON) and schema version</li> <li>Chunks store metadata with JSON pointer references (<code>doc_item_refs</code>), semantic labels, section headings, and page numbers</li> <li>New <code>ChunkMetadata</code> model for structured chunk provenance: <code>doc_item_refs</code>, <code>headings</code>, <code>labels</code>, <code>page_numbers</code></li> <li><code>Document.get_docling_document()</code> method to parse stored DoclingDocument</li> <li><code>ChunkMetadata.resolve_doc_items()</code> to resolve JSON pointer refs to actual DocItem objects</li> <li><code>ChunkMetadata.resolve_bounding_boxes()</code> for visual grounding with page coordinates</li> <li>LRU cache (100 documents) for parsed DoclingDocument objects to avoid repeated JSON parsing</li> <li>Enhanced Search Results: <code>search()</code> and <code>expand_context()</code> now return full provenance information</li> <li><code>SearchResult</code> includes <code>page_numbers</code>, <code>headings</code>, <code>labels</code>, and <code>doc_item_refs</code></li> <li>QA and research agents use provenance for better citations (page numbers, section headings)</li> <li>Type-Aware Context Expansion: <code>expand_context()</code> now uses document structure for intelligent expansion</li> <li>Structural content (tables, code blocks, lists) expands to complete structures regardless of chunking</li> <li>Text content uses radius-based expansion via <code>text_context_radius</code> setting</li> <li><code>max_context_items</code> and <code>max_context_chars</code> settings control expansion limits</li> <li><code>SearchResult.format_for_agent()</code> method formats expanded results with metadata for LLM consumption</li> <li>Visual Grounding: View page images with highlighted bounding boxes for chunks</li> <li>Inspector modal with keyboard navigation between pages</li> <li>CLI command: <code>haiku-rag visualize &lt;chunk_id&gt;</code></li> <li>Requires <code>textual-image</code> dependency and terminal with image support</li> <li>Processing Primitives: New methods for custom document processing pipelines</li> <li><code>convert()</code> - Convert files, URLs, or text to DoclingDocument</li> <li><code>chunk()</code> - Chunk a DoclingDocument into Chunk objects</li> <li><code>contextualize()</code> - Prepend section headings to chunk content for embedding</li> <li><code>embed_chunks()</code> - Generate embeddings for chunks</li> <li>New <code>import_document()</code> Method: Import pre-processed documents with custom chunks</li> <li>Accepts <code>DoclingDocument</code> directly for rich metadata (visual grounding, page numbers)</li> <li>Use when document conversion, chunking, or embedding were done externally</li> <li>Chunks without embeddings are automatically embedded</li> <li>Automatic Chunk Embedding: <code>import_document()</code> and <code>update_document()</code> automatically embed chunks that don't have embeddings</li> <li>Pass chunks with or without embeddings - missing embeddings are generated</li> <li>Chunks with pre-computed embeddings are stored as-is</li> <li>Format Parameter for Text Conversion: New <code>format</code> parameter for <code>convert()</code> and <code>create_document()</code> to specify content type</li> <li>Supports <code>\"md\"</code> (default) for markdown and <code>\"html\"</code> for HTML content</li> <li>HTML format preserves document structure (headings, lists, sections) in DoclingDocument</li> <li>Enables proper parsing of HTML content that was previously treated as plain text</li> <li>Inspector Context Modal: Press <code>c</code> in the inspector to view expanded context for the selected chunk</li> <li>Auto-Vacuum Configuration: New <code>storage.auto_vacuum</code> setting to control automatic vacuuming behavior</li> <li>When <code>true</code> (default), vacuum runs automatically after document create/update operations and rebuilds</li> <li>When <code>false</code>, vacuum only runs via explicit <code>haiku-rag vacuum</code> command</li> <li>Disabling can help avoid potential crashes in high-concurrency scenarios due to LanceDB race conditions</li> </ul>"},{"location":"changelog/#changed_19","title":"Changed","text":"<ul> <li>BREAKING: <code>create_document()</code> API: Removed <code>chunks</code> parameter</li> <li><code>create_document()</code> now always processes content (converts, chunks, embeds)</li> <li>Use <code>import_document()</code> for pre-processed documents with custom chunks</li> <li>BREAKING: <code>update_document()</code> API: Unified with <code>update_document_fields()</code></li> <li>Old: <code>update_document(document)</code> - pass modified Document object</li> <li>New: <code>update_document(document_id, content=, metadata=, chunks=, title=, docling_document=)</code></li> <li><code>content</code> and <code>docling_document</code> are mutually exclusive</li> <li>BREAKING: Chunker Interface: <code>DocumentChunker.chunk()</code> now returns <code>list[Chunk]</code> instead of <code>list[str]</code></li> <li>Chunks include structured metadata (doc_item_refs, labels, headings, page_numbers)</li> <li>Search Config: New settings in <code>search</code> section for search behavior and context expansion</li> <li><code>search.limit</code> - Default number of search results (default: 5). Used by CLI, MCP server, and API when no limit specified</li> <li><code>search.context_radius</code> - DocItems before/after to include for text content expansion (default: 0)</li> <li><code>search.max_context_items</code> - Maximum items in expanded context (default: 10)</li> <li><code>search.max_context_chars</code> - Maximum characters in expanded context (default: 10000)</li> <li>Rebuild Performance: Batched database writes during <code>rebuild</code> command reduce LanceDB versions by ~98%</li> <li>All rebuild modes (FULL, RECHUNK, EMBED_ONLY) now batch writes across documents</li> <li>Eliminates redundant per-document chunk deletions and vacuum calls</li> <li>Significantly reduces storage overhead and improves rebuild speed for large databases</li> <li>Embedding Architecture: Moved embedding generation from <code>ChunkRepository</code> to client layer</li> <li>Repository is now a pure persistence layer</li> <li>Client handles embedding via <code>_ensure_chunks_embedded()</code></li> <li>Chunk Text Storage: Chunks store raw text; headings prepended only at embedding time</li> <li>Stored chunk content stays clean without duplicate heading prefixes</li> <li>Local and serve chunkers now produce identical output</li> <li>Citation Models: Introduced <code>RawSearchAnswer</code> for LLM output, <code>SearchAnswer</code> with resolved citations</li> <li>Page Image Generation: Always enabled for local docling converter (required for visual grounding)</li> <li>Download Models Progress: <code>haiku-rag download-models</code> now shows real-time progress with Rich progress bars for Ollama model downloads</li> </ul>"},{"location":"changelog/#removed_10","title":"Removed","text":"<ul> <li>BREAKING: <code>markdown_preprocessor</code> Config Option: Use processing primitives (<code>convert()</code>, <code>chunk()</code>, <code>embed_chunks()</code>) for custom pipelines</li> <li><code>update_document_fields()</code>: Merged into <code>update_document()</code></li> </ul>"},{"location":"changelog/#migration","title":"Migration","text":"<p>This release requires a database rebuild to populate the new DoclingDocument fields:</p> <pre><code>haiku-rag rebuild\n</code></pre> <p>Existing documents without DoclingDocument data will work but won't have provenance information.</p>"},{"location":"changelog/#0196-2025-12-03","title":"0.19.6 - 2025-12-03","text":""},{"location":"changelog/#changed_20","title":"Changed","text":"<ul> <li>BREAKING: Explicit Database Creation: Databases must now be explicitly created before use</li> <li>New <code>haiku-rag init</code> command creates a new empty database</li> <li>Python API: <code>HaikuRAG(path, create=True)</code> to create database programmatically</li> <li>Operations on non-existent databases raise <code>FileNotFoundError</code></li> <li>BREAKING: Embeddings Configuration: Restructured to nested <code>EmbeddingModelConfig</code></li> <li>Config path changed from <code>embeddings.{provider, model, vector_dim}</code> to <code>embeddings.model.{provider, name, vector_dim}</code></li> <li>Automatic migration upgrades existing databases to new format</li> <li>Database Migrations: Always run when opening an existing database</li> </ul>"},{"location":"changelog/#0195-2025-12-01","title":"0.19.5 - 2025-12-01","text":""},{"location":"changelog/#changed_21","title":"Changed","text":"<ul> <li>Rebuild Performance: Optimized <code>rebuild --embed-only</code> to use batch updates via LanceDB's <code>merge_insert</code> instead of individual chunk updates, and skip chunks with unchanged embeddings</li> </ul>"},{"location":"changelog/#0194-2025-11-28","title":"0.19.4 - 2025-11-28","text":""},{"location":"changelog/#added_21","title":"Added","text":"<ul> <li>Rebuild Modes: New options for <code>rebuild</code> command to control what gets rebuilt</li> <li><code>--embed-only</code>: Only regenerate embeddings, keeping existing chunks (fastest option when changing embedding model)</li> <li><code>--rechunk</code>: Re-chunk from existing document content without accessing source files</li> <li>Default (no flag): Full rebuild with source file re-conversion</li> <li>Python API: <code>rebuild_database(mode=RebuildMode.EMBED_ONLY | RECHUNK | FULL)</code></li> </ul>"},{"location":"changelog/#0193-2025-11-27","title":"0.19.3 - 2025-11-27","text":""},{"location":"changelog/#changed_22","title":"Changed","text":"<ul> <li>Async Chunker: <code>DoclingServeChunker</code> now uses <code>httpx.AsyncClient</code> instead of sync <code>requests</code></li> </ul>"},{"location":"changelog/#fixed_18","title":"Fixed","text":"<ul> <li>OCR Options: Fixed <code>DoclingLocalConverter</code> using base <code>OcrOptions</code> class which docling's OCR factory doesn't recognize. Now uses <code>OcrAutoOptions</code> for automatic OCR engine selection.</li> <li>Dependencies: Added <code>opencv-python-headless</code> to the <code>docling</code> optional dependency for table structure detection.</li> </ul>"},{"location":"changelog/#0192-2025-11-27","title":"0.19.2 - 2025-11-27","text":""},{"location":"changelog/#changed_23","title":"Changed","text":"<ul> <li>Async Converters: Made document converters fully async</li> <li><code>BaseConverter.convert_file()</code> and <code>convert_text()</code> are now async methods</li> <li><code>DoclingLocalConverter</code> wraps blocking Docling operations with <code>asyncio.to_thread()</code></li> <li><code>DoclingServeConverter</code> now uses <code>httpx.AsyncClient</code> instead of sync <code>requests</code></li> <li>Async Model Prefetch: <code>prefetch_models()</code> is now async</li> <li>Uses <code>httpx.AsyncClient</code> for Ollama model pulls</li> <li>Wraps blocking Docling and HuggingFace downloads with <code>asyncio.to_thread()</code></li> </ul>"},{"location":"changelog/#0191-2025-11-26","title":"0.19.1 - 2025-11-26","text":""},{"location":"changelog/#added_22","title":"Added","text":"<ul> <li>LM Studio Provider: Added support for LM Studio as a provider for embeddings and QA/research models</li> <li>Configure with <code>provider: lm_studio</code> in embeddings, QA, or research model settings</li> <li>Supports thinking control for reasoning models (gpt-oss, etc.)</li> <li>Default base URL: <code>http://localhost:1234</code></li> </ul>"},{"location":"changelog/#fixed_19","title":"Fixed","text":"<ul> <li>Configuration: Fixed <code>init-config</code> command generating invalid configuration files (#165)</li> <li>Refactored <code>generate_default_config()</code> to use Pydantic model serialization instead of manual dict construction</li> <li>Updated <code>qa</code>, <code>research</code>, and <code>reranking</code> sections to use new <code>ModelConfig</code> structure</li> </ul>"},{"location":"changelog/#0190-2025-11-25","title":"0.19.0 - 2025-11-25","text":""},{"location":"changelog/#added_23","title":"Added","text":"<ul> <li>Model Customization: Added support for per-model configuration settings</li> <li>New <code>enable_thinking</code> parameter to control reasoning behavior (true/false/None)</li> <li>Support for <code>temperature</code> and <code>max_tokens</code> settings on QA and research models</li> <li>All settings apply to any provider that supports them</li> <li>Database Inspector: New <code>inspect</code> CLI command launches interactive TUI for browsing documents and chunks &amp; searching</li> <li>Evaluations: Added <code>evaluations</code> CLI script for running benchmarks (replaces <code>python -m evaluations.benchmark</code>)</li> <li>Evaluations: Added <code>--db</code> option to override evaluation database path</li> <li>Default database location moved to haiku.rag data directory:<ul> <li>macOS: <code>~/Library/Application Support/haiku.rag/evaluations/dbs/</code></li> <li>Linux: <code>~/.local/share/haiku.rag/evaluations/dbs/</code></li> <li>Windows: <code>C:/Users/&lt;USER&gt;/AppData/Roaming/haiku.rag/evaluations/dbs/</code></li> </ul> </li> <li>Previously stored in <code>evaluations/data/</code> within the repository</li> <li>Evaluations: Added comprehensive experiment metadata tracking for better reproducibility</li> <li>Records dataset name, test case count, and all model configurations</li> <li>Tracks embedder settings: provider, model, and vector dimensions</li> <li>Tracks QA model: provider and model name</li> <li>Tracks judge model: provider and model name for LLM evaluation</li> <li>Tracks processing parameters: <code>chunk_size</code> and <code>context_chunk_radius</code></li> <li>Tracks retrieval configuration: <code>retrieval_limit</code> for number of chunks retrieved</li> <li>Tracks reranking configuration: <code>rerank_provider</code> and <code>rerank_model</code></li> <li>Enables comparison of evaluation runs with different configurations in Logfire</li> <li>Evaluations: Refactored retrieval evaluation to use pydantic-ai experiment framework</li> <li>New <code>evaluators</code> module with <code>MRREvaluator</code> (Mean Reciprocal Rank) and <code>MAPEvaluator</code> (Mean Average Precision)</li> <li>Retrieval benchmarks now use <code>Dataset.evaluate()</code> with full Logfire experiment tracking</li> <li>Dataset specifications now declare their retrieval evaluator (MRR for RepliQA, MAP for Wix)</li> <li>Replaced Recall@K and Success@K with industry-standard MRR and MAP metrics</li> <li>Unified evaluation framework for both retrieval and QA benchmarks</li> <li>AG-UI Events: Enhanced ActivitySnapshot events with richer structured data</li> <li>Added <code>stepName</code> field to identify which graph node emitted each activity</li> <li>Added structured fields to activity content while preserving backward-compatible <code>message</code> field:<ul> <li>Planning: <code>sub_questions</code> - list of sub-question strings</li> <li>Searching: <code>query</code> - the search query, <code>confidence</code> - answer confidence (on success), <code>error</code> - error message (on failure)</li> <li>Analyzing (research): <code>insights</code> - list of insight objects, <code>gaps</code> - list of gap objects, <code>resolved_gaps</code> - list of resolved gap strings</li> <li>Evaluating (research): <code>confidence</code> - confidence score, <code>is_sufficient</code> - sufficiency flag</li> <li>Evaluating (deep QA): <code>is_sufficient</code> - sufficiency flag, <code>iterations</code> - iteration count</li> </ul> </li> </ul>"},{"location":"changelog/#changed_24","title":"Changed","text":"<ul> <li>Evaluations: Renamed <code>--qa-limit</code> CLI parameter to <code>--limit</code>, now applies to both retrieval and QA benchmarks</li> <li>Evaluations: Retrieval evaluator selection moved from runtime logic to dataset configuration</li> </ul>"},{"location":"changelog/#0180-2025-11-21","title":"0.18.0 - 2025-11-21","text":""},{"location":"changelog/#added_24","title":"Added","text":"<ul> <li>Manual Vector Indexing: New <code>create-index</code> CLI command for explicit vector index creation</li> <li>Creates IVF_PQ indexes</li> <li>Requires minimum 256 chunks (LanceDB training data requirement)</li> <li>New <code>search.vector_index_metric</code> config option: <code>cosine</code> (default), <code>l2</code>, or <code>dot</code></li> <li>New <code>search.vector_refine_factor</code> config option (default: 30) for accuracy/speed tradeoff</li> <li>Indexes not created automatically during ingestion to avoid performance degradation</li> <li>Manual rebuilding required after adding significant new data</li> <li>Enhanced Info Command: <code>haiku-rag info</code> now shows storage sizes and vector index statistics</li> <li>Displays storage size for documents and chunks tables in human-readable format</li> <li>Shows vector index status (exists/not created)</li> <li>Shows indexed and unindexed chunk counts for monitoring index staleness</li> </ul>"},{"location":"changelog/#changed_25","title":"Changed","text":"<ul> <li>BREAKING: Default Embedding Model: Changed default embedding model from <code>qwen3-embedding</code> to <code>qwen3-embedding:4b</code> with vector dimension 2560 (previously 4096)</li> <li>New installations will use the smaller, more efficient 4B parameter model by default</li> <li>Action required: Existing databases created with the old default will be incompatible. Users must either:<ul> <li>Explicitly set <code>embeddings.model: \"qwen3-embedding\"</code> and <code>embeddings.vector_dim: 4096</code> in their config to maintain compatibility with existing databases</li> <li>Or run <code>haiku-rag rebuild</code> to re-embed all documents with the new default</li> </ul> </li> <li>This change provides better performance for most use cases while reducing resource requirements</li> <li>Evaluations: Improved evaluation dataset naming and simplified evaluator configuration</li> <li><code>EvalDataset</code> now accepts dataset name for better organization in Logfire</li> <li>Added <code>--name</code> CLI parameter to override evaluation run names</li> <li>Removed <code>IsInstance</code> evaluator, using only <code>LLMJudge</code> for QA evaluation</li> <li>Search Accuracy: Applied <code>refine_factor</code> to vector and hybrid searches for improved accuracy</li> <li>Retrieves <code>refine_factor * limit</code> candidates and re-ranks in memory</li> <li>Higher values increase accuracy but slow down queries</li> </ul>"},{"location":"changelog/#fixed_20","title":"Fixed","text":"<ul> <li>AG-UI Activity Events: Activity events now correctly use structured dict content instead of strings</li> <li>Graph Configuration: Graph builder functions now properly accept and use non-global config (#149)</li> <li><code>build_research_graph()</code> and <code>build_deep_qa_graph()</code> now pass config to all agents and model creation</li> <li><code>get_model()</code> utility function accepts <code>config</code> parameter (defaults to global Config)</li> <li>Allows creating multiple graphs with different configurations in the same application</li> </ul>"},{"location":"changelog/#0172-2025-11-19","title":"0.17.2 - 2025-11-19","text":""},{"location":"changelog/#added_25","title":"Added","text":"<ul> <li>Document Update API: New <code>update_document_fields()</code> method for partial document updates</li> <li>Update individual fields (content, metadata, title, chunks) without fetching full document</li> <li>Support for custom chunks or auto-generation from content</li> </ul>"},{"location":"changelog/#changed_26","title":"Changed","text":"<ul> <li>Chunk Creation: <code>ChunkRepository.create()</code> now accepts both single chunks and lists for batch insertion</li> <li>Batch insertion reduces LanceDB version creation when adding multiple chunks with custom chunks</li> <li>Batch embedding generation for improved performance with multiple chunks</li> <li>Updated core dependencies</li> </ul>"},{"location":"changelog/#0171-2025-11-18","title":"0.17.1 - 2025-11-18","text":""},{"location":"changelog/#added_26","title":"Added","text":"<ul> <li>Conversion Options: Fine-grained control over document conversion for both local and remote converters</li> <li>New <code>conversion_options</code> config section in <code>ProcessingConfig</code></li> <li>OCR settings: <code>do_ocr</code>, <code>force_ocr</code>, <code>ocr_lang</code> for controlling OCR behavior</li> <li>Table extraction: <code>do_table_structure</code>, <code>table_mode</code> (fast/accurate), <code>table_cell_matching</code></li> <li>Image settings: <code>images_scale</code> to control image resolution</li> <li>Options work identically with both <code>docling-local</code> and <code>docling-serve</code> converters</li> </ul>"},{"location":"changelog/#changed_27","title":"Changed","text":"<ul> <li>Increase reranking candidate retrieval multiplier from 3x to 10x for improved result quality</li> <li>Docker Images: Main <code>haiku.rag</code> image no longer automatically built and published</li> <li>Conversion Options: Removed the legacy <code>pdf_backend</code> setting; docling now chooses the optimal backend automatically</li> </ul>"},{"location":"changelog/#0170-2025-11-17","title":"0.17.0 - 2025-11-17","text":""},{"location":"changelog/#added_27","title":"Added","text":"<ul> <li>Remote Processing: Support for docling-serve as remote document processing and chunking service</li> <li>New <code>converter</code> config option: <code>docling-local</code> (default) or <code>docling-serve</code></li> <li>New <code>chunker</code> config option: <code>docling-local</code> (default) or <code>docling-serve</code></li> <li>New <code>providers.docling_serve</code> config section with <code>base_url</code>, <code>api_key</code>, and <code>timeout</code></li> <li>Comprehensive error handling for connection, timeout, and authentication issues</li> <li>Chunking Strategies: Support for both hybrid and hierarchical chunking</li> <li>New <code>chunker_type</code> config option: <code>hybrid</code> (default) or <code>hierarchical</code></li> <li>Hybrid chunking: Structure-aware splitting that respects document boundaries</li> <li>Hierarchical chunking: Preserves document hierarchy for nested documents</li> <li>Table Serialization Control: Configurable table representation in chunks</li> <li>New <code>chunking_use_markdown_tables</code> config option (default: <code>false</code>)</li> <li><code>false</code>: Tables serialized as narrative text (\"Value A, Column 2 = Value B\")</li> <li><code>true</code>: Tables preserved as markdown format with structure</li> <li>Chunking Configuration: Additional chunking control options</li> <li>New <code>chunking_merge_peers</code> config option (default: <code>true</code>) to merge undersized successive chunks</li> <li>Docker Images: Two Docker images for different deployment scenarios</li> <li><code>haiku.rag</code>: Full image with all dependencies for self-contained deployments</li> <li><code>haiku.rag-slim</code>: Minimal image designed for use with external docling-serve</li> <li>Multi-platform support (linux/amd64, linux/arm64)</li> <li>Docker Compose examples with docling-serve integration</li> <li>Automated CI/CD workflows for both images</li> <li>Build script (<code>scripts/build-docker-images.sh</code>) for local multi-platform builds</li> </ul>"},{"location":"changelog/#changed_28","title":"Changed","text":"<ul> <li>BREAKING: Chunking Tokenizer: Switched from tiktoken to HuggingFace tokenizers for consistency with docling-serve</li> <li>Default tokenizer changed from tiktoken \"gpt-4o\" to \"Qwen/Qwen3-Embedding-0.6B\"</li> <li>New <code>chunking_tokenizer</code> config option in <code>ProcessingConfig</code> for customization</li> <li><code>download-models</code> CLI command now also downloads the configured HuggingFace tokenizer</li> <li>Docker Examples: Updated examples to demonstrate remote processing</li> <li><code>examples/docker</code> now uses slim image with docling-serve</li> <li><code>examples/ag-ui-research</code> backend uses slim image with docling-serve</li> <li>Configuration examples include remote processing setup</li> </ul>"},{"location":"changelog/#0161-2025-11-14","title":"0.16.1 - 2025-11-14","text":""},{"location":"changelog/#changed_29","title":"Changed","text":"<ul> <li>Evaluations: Refactored QA benchmark to run entire dataset as single evaluation for better Logfire experiment tracking</li> <li>Evaluations: Added <code>.env</code> file loading support via <code>python-dotenv</code> dependency</li> </ul>"},{"location":"changelog/#0160-2025-11-13","title":"0.16.0 - 2025-11-13","text":""},{"location":"changelog/#added_28","title":"Added","text":"<ul> <li>AG-UI Protocol Support: Full AG-UI (Agent-UI) protocol implementation for graph execution with event streaming</li> <li>New <code>AGUIEmitter</code> class for emitting AG-UI events from graphs</li> <li>Support for all AG-UI event types: lifecycle events (<code>RUN_STARTED</code>, <code>RUN_FINISHED</code>, <code>RUN_ERROR</code>), step events (<code>STEP_STARTED</code>, <code>STEP_FINISHED</code>), state updates (<code>STATE_SNAPSHOT</code>, <code>STATE_DELTA</code>), activity narration (<code>ACTIVITY_SNAPSHOT</code>), and text messages (<code>TEXT_MESSAGE_CHUNK</code>)</li> <li><code>AGUIConsoleRenderer</code> for rendering AG-UI event streams to terminal with Rich formatting</li> <li><code>stream_graph()</code> utility function for executing graphs with AG-UI event emission</li> <li>State diff computation for efficient state synchronization</li> <li>Delta State Updates: AG-UI emitter now supports incremental state updates via JSON Patch operations (<code>STATE_DELTA</code> events) to reduce bandwidth, configurable via <code>use_deltas</code> parameter (enabled by default)</li> <li>AG-UI Server: Starlette-based HTTP server for serving graphs via AG-UI protocol</li> <li>Server-Sent Events (SSE) streaming endpoint at <code>/v1/agent/stream</code></li> <li>Health check endpoint at <code>/health</code></li> <li>Full CORS support configurable via <code>agui</code> config section</li> <li><code>create_agui_server()</code> function for programmatic server creation</li> <li>Deep QA AG-UI Support: Deep QA graph now fully supports AG-UI event streaming</li> <li>Integration with <code>AGUIEmitter</code> for progress tracking</li> <li>Step-by-step execution visibility via AG-UI events</li> <li>CLI AG-UI Flag: New <code>--agui</code> flag for <code>serve</code> command to start AG-UI server</li> <li>Graph Module: New unified <code>haiku.rag.graph</code> module containing all graph-related functionality</li> <li>Common Graph Nodes: New factory functions (<code>create_plan_node</code>, <code>create_search_node</code>) in <code>haiku.rag.graph.common.nodes</code> for reusable graph components</li> <li>AG-UI Research Example: New full-stack example (<code>examples/ag-ui-research</code>) demonstrating agent+graph architecture with CopilotKit frontend</li> <li>Pydantic AI agent with research tool that invokes the research graph</li> <li>Custom AG-UI streaming endpoint with anyio memory streams</li> <li>React/Next.js frontend with split-pane UI showing live research state</li> <li>Real-time progress tracking of questions, answers, insights, and gaps</li> <li>Docker Compose setup for easy local development</li> </ul>"},{"location":"changelog/#changed_30","title":"Changed","text":"<ul> <li>Vacuum Retention: Default <code>vacuum_retention_seconds</code> increased from 60 seconds to 86400 seconds (1 day) for better version retention in typical workflows</li> <li>BREAKING: Major refactoring of graph-related code into unified <code>haiku.rag.graph</code> module structure:</li> <li><code>haiku.rag.research</code> \u2192 <code>haiku.rag.graph.research</code></li> <li><code>haiku.rag.qa.deep</code> \u2192 <code>haiku.rag.graph.deep_qa</code></li> <li><code>haiku.rag.agui</code> \u2192 <code>haiku.rag.graph.agui</code></li> <li><code>haiku.rag.graph_common</code> \u2192 <code>haiku.rag.graph.common</code></li> <li>BREAKING: Research and Deep QA graphs now use AG-UI event protocol instead of direct console logging</li> <li>Removed <code>console</code> and <code>stream</code> parameters from graph dependencies</li> <li>All progress updates now emit through <code>AGUIEmitter</code></li> <li>BREAKING: <code>ResearchState</code> converted from dataclass to Pydantic <code>BaseModel</code> for JSON serialization and AG-UI compatibility</li> <li>Research and Deep QA graphs now emit detailed execution events for better observability</li> <li>CLI research command now uses AG-UI event rendering for <code>--verbose</code> output</li> <li>Improved graph execution visibility with step-by-step progress tracking</li> <li>Updated all documentation to reflect new import paths and AG-UI usage</li> <li>Updated examples (ag-ui-research, a2a-server) to use new import paths</li> </ul>"},{"location":"changelog/#fixed_21","title":"Fixed","text":"<ul> <li>Document Creation: Optimized <code>create_document</code> to skip unnecessary DoclingDocument conversion when chunks are pre-provided</li> <li>FileReader: Error messages now include both original exception details and file path for easier debugging</li> <li>Database Auto-creation: Read operations (search, list, get, ask, research) no longer auto-create empty databases. Write operations (add, add-src, delete, rebuild) still create the database as needed. This prevents the confusing scenario where a search query creates an empty database. Fixes issue #137.</li> </ul>"},{"location":"changelog/#removed_11","title":"Removed","text":"<ul> <li>BREAKING: Removed <code>disable_autocreate</code> config option - the behavior is now automatic based on operation type</li> <li>BREAKING: Removed legacy <code>ResearchStream</code> and <code>ResearchStreamEvent</code> classes (replaced by AG-UI event protocol)</li> </ul>"},{"location":"changelog/#0150-2025-11-07","title":"0.15.0 - 2025-11-07","text":""},{"location":"changelog/#added_29","title":"Added","text":"<ul> <li>File Monitor: Orphan deletion feature - automatically removes documents from database when source files are deleted (enabled via <code>monitor.delete_orphans</code> config option, default: false)</li> </ul>"},{"location":"changelog/#changed_31","title":"Changed","text":"<ul> <li>Configuration: All CLI commands now properly support <code>--config</code> parameter for specifying custom configuration files</li> <li>Configuration loading consolidated across CLI, app, and client with consistent resolution order</li> <li><code>HaikuRAGApp</code> and MCP server now accept <code>config</code> parameter for programmatic configuration</li> <li>Updated CLI documentation to clarify global vs per-command options</li> <li>BREAKING: Standardized configuration filename to <code>haiku.rag.yaml</code> in user directories (was incorrectly using <code>config.yaml</code>). Users with existing <code>config.yaml</code> in their user directory will need to rename it to <code>haiku.rag.yaml</code></li> </ul>"},{"location":"changelog/#fixed_22","title":"Fixed","text":"<ul> <li>File Monitor: Fixed incorrect \"Updated document\" logging for unchanged files - monitor now properly skips files when MD5 hash hasn't changed</li> </ul>"},{"location":"changelog/#removed_12","title":"Removed","text":"<ul> <li>BREAKING: A2A (Agent-to-Agent) protocol support has been moved to a separate self-contained package in <code>examples/a2a-server/</code>. The A2A server is no longer part of the main haiku.rag package. Users who need A2A functionality can install and run it from the examples directory with <code>cd examples/a2a-server &amp;&amp; uv sync</code>.</li> <li>BREAKING: Removed deprecated <code>.env</code>-based configuration system. The <code>haiku-rag init-config --from-env</code> command and <code>load_config_from_env()</code> function have been removed. All configuration must now be done via YAML files. Environment variables for API keys (e.g., <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>) and service URLs (e.g., <code>OLLAMA_BASE_URL</code>) are still supported and can be set via <code>.env</code> files.</li> </ul>"},{"location":"changelog/#0141-2025-11-06","title":"0.14.1 - 2025-11-06","text":""},{"location":"changelog/#added_30","title":"Added","text":"<ul> <li>Migrated research and deep QA agents to use Pydantic Graph beta API for better graph execution</li> <li>Automatic semaphore-based concurrency control for parallel sub-question processing</li> <li><code>max_concurrency</code> parameter for controlling parallel execution in research and deep QA (default: 1)</li> </ul>"},{"location":"changelog/#changed_32","title":"Changed","text":"<ul> <li>BREAKING: Research and Deep QA graphs now use <code>pydantic_graph.beta</code> instead of the class-based graph implementation</li> <li>Refactored graph common patterns into <code>graph_common</code> module</li> <li>Sub-questions now process using <code>.map()</code> for true parallel execution</li> <li>Improved graph structure with cleaner node definitions and flow control</li> <li>Pinned critical dependencies: <code>docling-core</code>, <code>lancedb</code>, <code>docling</code></li> </ul>"},{"location":"changelog/#0140-2024-11-05","title":"0.14.0 - 2024-11-05","text":""},{"location":"changelog/#added_31","title":"Added","text":"<ul> <li>New <code>haiku.rag-slim</code> package with minimal dependencies for users who want to install only what they need</li> <li>Evaluations package (<code>haiku.rag-evals</code>) for internal benchmarking and testing</li> <li>Improved search filtering performance by using pandas DataFrames for joins instead of SQL WHERE IN clauses</li> </ul>"},{"location":"changelog/#changed_33","title":"Changed","text":"<ul> <li>BREAKING: Restructured project into UV workspace with three packages:</li> <li><code>haiku.rag-slim</code> - Core package with minimal dependencies</li> <li><code>haiku.rag</code> - Full package with all extras (recommended for most users)</li> <li><code>haiku.rag-evals</code> - Internal benchmarking and evaluation tools</li> <li>Migrated from <code>pydantic-ai</code> to <code>pydantic-ai-slim</code> with extras system</li> <li>Docling is now an optional dependency (install with <code>haiku.rag-slim[docling]</code>)</li> <li>Package metadata checks now use <code>haiku.rag-slim</code> (always present) instead of <code>haiku.rag</code></li> <li>Docker image optimized: removed evaluations package, reducing installed packages from 307 to 259</li> <li>Improved vector search performance through optimized score normalization</li> </ul>"},{"location":"changelog/#fixed_23","title":"Fixed","text":"<ul> <li>ImportError now properly raised when optional docling dependency is missing</li> </ul>"},{"location":"changelog/#0133-2024-11-04","title":"0.13.3 - 2024-11-04","text":""},{"location":"changelog/#added_32","title":"Added","text":"<ul> <li>Support for Zero Entropy reranker</li> <li>Filter parameter to <code>search()</code> for filtering documents before search</li> <li>Filter parameter to CLI <code>search</code> command</li> <li>Filter parameter to CLI <code>list</code> command for filtering document listings</li> <li>Config option to pass custom configuration files to evaluation commands</li> <li>Document filtering now respects configured include/exclude patterns when using <code>add-src</code> with directories</li> <li>Max retries to insight_agent when producing structured output</li> </ul>"},{"location":"changelog/#fixed_24","title":"Fixed","text":"<ul> <li>CLI now loads <code>.env</code> files at startup</li> <li>Info command no longer attempts to use deprecated <code>.env</code> settings</li> <li>Documentation typos</li> </ul>"},{"location":"changelog/#0132-2024-11-04","title":"0.13.2 - 2024-11-04","text":""},{"location":"changelog/#added_33","title":"Added","text":"<ul> <li>Gitignore-style pattern filtering for file monitoring using pathspec</li> <li>Include/exclude pattern documentation for FileMonitor</li> </ul>"},{"location":"changelog/#changed_34","title":"Changed","text":"<ul> <li>Moved monitor configuration to its own section in config</li> <li>Improved configuration documentation</li> <li>Updated dependencies</li> </ul>"},{"location":"changelog/#0131-2024-11-03","title":"0.13.1 - 2024-11-03","text":""},{"location":"changelog/#added_34","title":"Added","text":"<ul> <li>Initial version tracking</li> </ul>"},{"location":"cli/","title":"Command Line Interface","text":"<p>The <code>haiku-rag</code> CLI provides complete document management functionality.</p> <p>Note</p> <p>Global options (must be specified before the command):</p> <ul> <li><code>--config</code> - Specify custom configuration file</li> <li><code>--read-only</code> - Open database in read-only mode (blocks writes, skips upgrades)</li> <li><code>--before</code> - Query database as it existed before a datetime (implies <code>--read-only</code>)</li> <li><code>--version</code> / <code>-v</code> - Show version and exit</li> </ul> <p>Per-command options:</p> <ul> <li><code>--db</code> - Specify custom database path</li> <li><code>-h</code> - Show help for specific command</li> </ul> <p>Example: <pre><code>haiku-rag --config /path/to/config.yaml list\nhaiku-rag --config /path/to/config.yaml list --db /path/to/custom.db\nhaiku-rag --read-only search \"query\"\nhaiku-rag --before \"2025-01-15\" search \"query\"\nhaiku-rag add -h\n</code></pre></p>"},{"location":"cli/#document-management","title":"Document Management","text":""},{"location":"cli/#list-documents","title":"List Documents","text":"<pre><code>haiku-rag list\n</code></pre> <p>Filter documents by properties: <pre><code># Filter by URI pattern (--filter or -f)\nhaiku-rag list --filter \"uri LIKE '%arxiv%'\"\n\n# Filter by exact title\nhaiku-rag list --filter \"title = 'My Document'\"\n\n# Combine multiple conditions\nhaiku-rag list --filter \"uri LIKE '%.pdf' AND title LIKE '%paper%'\"\n</code></pre></p>"},{"location":"cli/#add-documents","title":"Add Documents","text":"<p>From text: <pre><code>haiku-rag add \"Your document content here\"\n\n# Set a title\nhaiku-rag add \"Your document content here\" --title \"My Document\"\n\n# Attach metadata (repeat --meta for multiple entries)\nhaiku-rag add \"Your document content here\" --meta author=alice --meta topic=notes\n</code></pre></p> <p>From file or URL: <pre><code>haiku-rag add-src /path/to/document.pdf\nhaiku-rag add-src https://example.com/article.html\n\n# Optionally set a human\u2011readable title stored in the DB schema\nhaiku-rag add-src /mnt/data/doc1.pdf --title \"Q3 Financial Report\"\n\n# Optionally attach metadata (repeat --meta). Values use JSON parsing if possible:\n# numbers, booleans, null, arrays/objects; otherwise kept as strings.\nhaiku-rag add-src /mnt/data/doc1.pdf --meta source=manual --meta page_count=12 --meta published=true\n</code></pre></p> <p>From directory (recursively adds all supported files): <pre><code>haiku-rag add-src /path/to/documents/\n</code></pre></p> <p>Note</p> <p>When adding a directory, the same content filters configured for file monitoring are applied. This means <code>ignore_patterns</code> and <code>include_patterns</code> from your configuration will be used to filter which files are added.</p> <p>Note</p> <p>As you add documents to <code>haiku.rag</code> the database keeps growing. By default, LanceDB supports versioning of your data. Create/update operations are atomic\u2011feeling: if anything fails during chunking or embedding, the database rolls back to the pre\u2011operation snapshot using LanceDB table versioning. You can optimize and compact the database by running the vacuum command.</p>"},{"location":"cli/#get-document","title":"Get Document","text":"<pre><code>haiku-rag get 3f4a...   # document ID\n</code></pre>"},{"location":"cli/#delete-document","title":"Delete Document","text":"<pre><code>haiku-rag delete 3f4a...   # document ID\nhaiku-rag rm 3f4a...       # alias\n</code></pre>"},{"location":"cli/#visualize-chunk","title":"Visualize Chunk","text":"<p>Display visual grounding for a chunk - shows page images with highlighted bounding boxes:</p> <pre><code>haiku-rag visualize &lt;chunk_id&gt;\n</code></pre> <p>This renders the source document pages with the chunk's location highlighted. Useful for verifying chunk boundaries and understanding document structure.</p> <p>Note</p> <p>Requires a terminal with image support (iTerm2, Kitty, WezTerm, etc.) and documents processed with docling that have page images stored.</p>"},{"location":"cli/#search","title":"Search","text":"<p>Basic search: <pre><code>haiku-rag search \"machine learning\"\n</code></pre></p> <p>With options: <pre><code>haiku-rag search \"python programming\" --limit 10  # or -l 10\n</code></pre></p> <p>With filters (filter by document properties, use <code>--filter</code> or <code>-f</code>): <pre><code># Filter by URI pattern\nhaiku-rag search \"neural networks\" --filter \"uri LIKE '%arxiv%'\"\n\n# Filter by exact title\nhaiku-rag search \"transformers\" --filter \"title = 'Deep Learning Guide'\"\n\n# Combine multiple conditions\nhaiku-rag search \"AI\" --filter \"uri LIKE '%.pdf' AND title LIKE '%paper%'\"\n</code></pre></p>"},{"location":"cli/#question-answering","title":"Question Answering","text":"<p>Ask questions about your documents: <pre><code>haiku-rag ask \"Who is the author of haiku.rag?\"\n</code></pre></p> <p>Ask questions with citations showing source documents: <pre><code>haiku-rag ask \"Who is the author of haiku.rag?\" --cite\n</code></pre></p> <p>Filter to specific documents: <pre><code>haiku-rag ask \"What are the main findings?\" --filter \"uri LIKE '%paper%'\"\n</code></pre></p> <p>The QA agent searches your documents for relevant information and provides a comprehensive answer. When available, citations use the document title; otherwise they fall back to the URI.</p> <p>Flags:</p> <ul> <li><code>--cite</code>: Include citations showing which documents were used</li> <li><code>--filter</code> / <code>-f</code>: Restrict searches to documents matching the filter (see Filtering Search Results)</li> </ul>"},{"location":"cli/#chat","title":"Chat","text":"<p>Launch an interactive chat session for multi-turn conversations:</p> <pre><code>haiku-rag chat\nhaiku-rag chat --db /path/to/database.lancedb\n</code></pre> <p>Note</p> <p>Requires the <code>tui</code> extra: <code>pip install haiku.rag-slim[tui]</code> (included in full <code>haiku.rag</code> package)</p> <p>The chat interface provides:</p> <ul> <li>Streaming responses with real-time tool execution</li> <li>Expandable citations with source metadata</li> <li>Session memory for context-aware follow-up questions</li> <li>Visual grounding to inspect chunk source locations</li> </ul> <p>See Applications for keyboard shortcuts and features.</p>"},{"location":"cli/#inspect","title":"Inspect","text":"<p>Launch the interactive inspector TUI for browsing documents and chunks:</p> <pre><code>haiku-rag inspect\nhaiku-rag inspect --db /path/to/database.lancedb\n</code></pre> <p>Note</p> <p>Requires the <code>tui</code> extra: <code>pip install haiku.rag-slim[tui]</code> (included in full <code>haiku.rag</code> package)</p> <p>The inspector provides:</p> <ul> <li>Browse all documents in the database</li> <li>View document metadata and content</li> <li>Explore individual chunks</li> <li>Search and filter results</li> </ul> <p>See Applications for details.</p>"},{"location":"cli/#research","title":"Research","text":"<p>Run the multi-step research graph:</p> <pre><code>haiku-rag research \"How does haiku.rag organize and query documents?\"\n</code></pre> <p>Filter to specific documents:</p> <pre><code>haiku-rag research \"What are the key findings?\" --filter \"uri LIKE '%paper%'\"\n</code></pre> <p>Flags:</p> <ul> <li><code>--filter</code> / <code>-f</code>: SQL WHERE clause to filter documents (see Filtering Search Results)</li> </ul> <p>Research parameters like <code>max_iterations</code> and <code>max_concurrency</code> are configured in your configuration file under the <code>research</code> section.</p>"},{"location":"cli/#rlm-recursive-language-model","title":"RLM (Recursive Language Model)","text":"<p>Answer complex analytical questions via code execution:</p> <pre><code>haiku-rag rlm \"How many documents mention security?\"\n</code></pre> <p>Filter to specific documents:</p> <pre><code>haiku-rag rlm \"What is the total revenue?\" --filter \"title LIKE '%Financial%'\"\n</code></pre> <p>Pre-load specific documents for comparison:</p> <pre><code>haiku-rag rlm \"Compare the conclusions\" --document \"Report A\" --document \"Report B\"\n</code></pre> <p>Flags:</p> <ul> <li><code>--filter</code> / <code>-f</code>: SQL WHERE clause to restrict document access</li> <li><code>--document</code> / <code>-d</code>: Pre-load a document by title or ID (can repeat)</li> </ul> <p>See RLM Agent for details on capabilities and configuration.</p>"},{"location":"cli/#server","title":"Server","text":"<p>Start services (requires at least one flag): <pre><code># MCP server only (HTTP transport)\nhaiku-rag serve --mcp\n\n# MCP server (stdio transport)\nhaiku-rag serve --mcp --stdio\n\n# File monitoring only\nhaiku-rag serve --monitor\n\n# Both services\nhaiku-rag serve --monitor --mcp\n\n# Custom MCP port\nhaiku-rag serve --mcp --mcp-port 9000\n\n# Read-only mode (excludes write MCP tools, disables monitor)\nhaiku-rag --read-only serve --mcp\n</code></pre></p> <p>See Server Mode for details on available services.</p>"},{"location":"cli/#settings","title":"Settings","text":"<p>View current configuration settings: <pre><code>haiku-rag settings\n</code></pre></p>"},{"location":"cli/#generate-configuration-file","title":"Generate Configuration File","text":"<p>Generate a YAML configuration file with defaults: <pre><code>haiku-rag init-config [output_path]\n</code></pre></p> <p>If no path is specified, creates <code>haiku.rag.yaml</code> in the current directory.</p>"},{"location":"cli/#database-management","title":"Database Management","text":""},{"location":"cli/#initialize-database","title":"Initialize Database","text":"<p>Create a new database:</p> <pre><code>haiku-rag init [--db /path/to/your.lancedb]\n</code></pre> <p>This creates the database with the configured settings. All other commands require an existing database - they will fail with an informative error if the database doesn't exist.</p>"},{"location":"cli/#migrate-database","title":"Migrate Database","text":"<p>Apply pending database migrations:</p> <pre><code>haiku-rag migrate [--db /path/to/your.lancedb]\n</code></pre> <p>When you upgrade haiku.rag to a new version that includes schema changes, the database requires migration. Opening a database with pending migrations will display an error:</p> <pre><code>Error: Database requires migration from 0.19.0 to 0.26.5. 3 migration(s) pending. Run 'haiku-rag migrate' to upgrade.\n</code></pre> <p>Run <code>haiku-rag migrate</code> to apply the pending migrations. The command shows which migrations were applied:</p> <pre><code>Applied 3 migration(s):\n  - 0.20.0: Add 'docling_document_json' and 'docling_version' columns\n  - 0.23.1: Add content_fts column for contextualized FTS search\n  - 0.25.0: Compress docling_document with gzip\nMigration completed successfully.\n</code></pre> <p>Tip</p> <p>Back up your database before running migrations. While migrations are designed to be safe, having a backup provides peace of mind for production databases.</p>"},{"location":"cli/#info","title":"Info","text":"<p>Display database metadata:</p> <pre><code>haiku-rag info [--db /path/to/your.lancedb]\n</code></pre> <p>Shows: - path to the database - stored haiku.rag version (from settings) - embeddings provider/model and vector dimension - number of documents and chunks (with storage sizes) - vector index status (exists/not created, indexed/unindexed chunks) - table versions per table (documents, chunks)</p> <p>At the end, a separate \"Versions\" section lists runtime package versions: - haiku.rag - lancedb - docling</p>"},{"location":"cli/#create-vector-index","title":"Create Vector Index","text":"<p>Create a vector index on the chunks table for fast approximate nearest neighbor search:</p> <pre><code>haiku-rag create-index [--db /path/to/your.lancedb]\n</code></pre> <p>Requirements: - Minimum 256 chunks required for index creation (LanceDB training data requirement) - Creates an IVF_PQ index using the configured <code>search.vector_index_metric</code> (cosine/l2/dot)</p> <p>When to use: - After ingesting documents (indexes are not created automatically) - After adding significant new data to rebuild the index - Use <code>haiku-rag info</code> to check index status and see how many chunks are indexed/unindexed</p> <p>Search behavior: - Without index: Brute-force kNN search (exact nearest neighbors, slower for large datasets) - With index: Fast ANN (approximate nearest neighbors) using IVF_PQ - With stale index: LanceDB combines indexed results (fast ANN) + brute-force kNN on unindexed rows - Performance degrades as more unindexed data accumulates</p>"},{"location":"cli/#vacuum-optimize-and-cleanup","title":"Vacuum (Optimize and Cleanup)","text":"<p>Reduce disk usage by optimizing and pruning old table versions across all tables:</p> <pre><code>haiku-rag vacuum\n</code></pre> <p>Automatic Cleanup: Vacuum runs automatically in the background after document operations. By default, it removes versions older than 1 day (configurable via <code>storage.vacuum_retention_seconds</code>), preserving recent versions for concurrent connections. Manual vacuum can be useful for cleanup after bulk operations or to free disk space immediately.</p>"},{"location":"cli/#rebuild-database","title":"Rebuild Database","text":"<p>Rebuild the database by re-indexing documents. Useful when switching embeddings provider/model or changing chunking settings:</p> <pre><code># Full rebuild (default) - re-converts from source files, re-chunks, re-embeds\nhaiku-rag rebuild\n\n# Re-chunk from stored content (no source file access)\nhaiku-rag rebuild --rechunk\n\n# Only regenerate embeddings (fastest, keeps existing chunks)\nhaiku-rag rebuild --embed-only\n\n# Only generate titles for untitled documents\nhaiku-rag rebuild --title-only\n</code></pre> <p>Rebuild modes:</p> Mode Flag Use case Full (default) Changed converter, source files updated Rechunk <code>--rechunk</code> Changed chunking strategy or chunk size Embed only <code>--embed-only</code> Changed embedding model or vector dimensions Title only <code>--title-only</code> Generate titles for documents without one"},{"location":"cli/#download-models","title":"Download Models","text":"<p>Download required runtime models:</p> <pre><code>haiku-rag download-models\n</code></pre> <p>This command downloads:</p> <ul> <li>Docling OCR/conversion models</li> <li>HuggingFace tokenizer (for chunking)</li> <li>Ollama models referenced in your configuration (embeddings, QA, research, rerank)</li> </ul> <p>Progress is displayed in real-time with download status and progress bars for Ollama model pulls.</p>"},{"location":"cli/#time-travel","title":"Time Travel","text":"<p>LanceDB maintains version history for tables, enabling you to query the database as it existed at a previous point in time. This is useful for:</p> <ul> <li>Debugging: Investigate data before a problematic change</li> <li>Auditing: Verify what knowledge was available when a support ticket was filed</li> </ul>"},{"location":"cli/#query-historical-state","title":"Query Historical State","text":"<p>Use <code>--before</code> to query the database as it existed before a specific datetime:</p> <pre><code># Query documents as of January 15, 2025\nhaiku-rag --before \"2025-01-15\" list\n\n# Search historical state\nhaiku-rag --before \"2025-01-15T14:30:00\" search \"machine learning\"\n\n# Ask questions against historical data\nhaiku-rag --before \"2025-01-15\" ask \"What documents existed?\"\n</code></pre> <p>Supported datetime formats:</p> <ul> <li>ISO 8601: <code>2025-01-15T14:30:00</code>, <code>2025-01-15T14:30:00Z</code>, <code>2025-01-15T14:30:00+00:00</code></li> <li>Date only: <code>2025-01-15</code> (interpreted as start of day)</li> </ul> <p>Note</p> <p>Time travel mode automatically enables read-only mode. You cannot modify the database while viewing historical state.</p>"},{"location":"cli/#version-history","title":"Version History","text":"<p>View version history for database tables:</p> <pre><code># Show history for all tables\nhaiku-rag history\n\n# Show history for a specific table\nhaiku-rag history --table documents\n\n# Limit number of versions shown\nhaiku-rag history --limit 10\n</code></pre> <p>Output shows version numbers and timestamps, sorted newest first:</p> <pre><code>Version History\n\ndocuments\n  v5: 2025-01-15 14:30:00\n  v4: 2025-01-14 10:00:00\n  v3: 2025-01-13 09:15:00\n\nchunks\n  v8: 2025-01-15 14:30:00\n  v7: 2025-01-14 10:00:00\n  ...\n</code></pre> <p>Use the timestamps from <code>history</code> to construct <code>--before</code> queries.</p>"},{"location":"custom-pipelines/","title":"Custom Processing Pipelines","text":"<p>haiku.rag provides processing primitives that let you build custom document pipelines. Use these when you need control over conversion, chunking, or embedding\u2014for example, to preprocess content, use external services, or implement custom chunking logic.</p>"},{"location":"custom-pipelines/#processing-primitives","title":"Processing Primitives","text":"<p>The client exposes four primitives that can be composed into custom workflows:</p> Primitive Input Output Purpose <code>convert()</code> file, URL, or text <code>DoclingDocument</code> Convert source to structured document <code>chunk()</code> <code>DoclingDocument</code> <code>list[Chunk]</code> Split document into chunks <code>embed_chunks()</code> <code>list[Chunk]</code> <code>list[Chunk]</code> Generate embeddings for chunks (includes contextualization) <code>contextualize()</code> <code>list[Chunk]</code> <code>list[str]</code> Get embedding-ready text (for custom embedders only)"},{"location":"custom-pipelines/#basic-pipeline","title":"Basic Pipeline","text":"<p>The standard pipeline mirrors what <code>create_document()</code> does internally:</p> <pre><code>from haiku.rag.client import HaikuRAG\nfrom haiku.rag.embeddings import embed_chunks\n\nasync with HaikuRAG(\"database.lancedb\", create=True) as client:\n    # 1. Convert source to DoclingDocument\n    docling_doc = await client.convert(\"path/to/document.pdf\")\n\n    # 2. Chunk the document\n    chunks = await client.chunk(docling_doc)\n\n    # 3. Generate embeddings\n    embedded_chunks = await embed_chunks(chunks)\n\n    # 4. Store the document with chunks\n    doc = await client.import_document(\n        docling_document=docling_doc,\n        chunks=embedded_chunks,\n        uri=\"file:///path/to/document.pdf\",\n        title=\"My Document\",\n    )\n</code></pre>"},{"location":"custom-pipelines/#convert","title":"Convert","text":"<p><code>convert()</code> accepts files, URLs, or plain text and returns a <code>DoclingDocument</code>:</p> <pre><code># From local file\ndocling_doc = await client.convert(\"report.pdf\")\ndocling_doc = await client.convert(Path(\"/absolute/path/to/file.docx\"))\n\n# From URL (downloads and converts)\ndocling_doc = await client.convert(\"https://example.com/paper.pdf\")\n\n# From plain text (parsed as markdown by default)\ndocling_doc = await client.convert(\"# Title\\n\\nYour text content here\")\n\n# From HTML text (use format parameter to preserve structure)\nhtml_content = \"&lt;h1&gt;Title&lt;/h1&gt;&lt;p&gt;Paragraph&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Item&lt;/li&gt;&lt;/ul&gt;\"\ndocling_doc = await client.convert(html_content, format=\"html\")\n\n# From file:// URI\ndocling_doc = await client.convert(\"file:///path/to/document.md\")\n</code></pre> <p>The <code>format</code> parameter controls how text content is parsed:</p> <ul> <li><code>\"md\"</code> (default) - Parse as Markdown</li> <li><code>\"html\"</code> - Parse as HTML, preserving semantic structure (headings, lists, tables)</li> </ul> <p>Note</p> <p>The <code>format</code> parameter only applies to text content. Files and URLs determine their format from the file extension or content-type header.</p> <p>Supported formats depend on your converter configuration (docling-local or docling-serve). Common formats include PDF, DOCX, HTML, Markdown, and images.</p>"},{"location":"custom-pipelines/#chunk","title":"Chunk","text":"<p><code>chunk()</code> splits a <code>DoclingDocument</code> into <code>Chunk</code> objects with metadata:</p> <pre><code>chunks = await client.chunk(docling_doc)\n\nfor chunk in chunks:\n    print(f\"Order: {chunk.order}\")\n    print(f\"Content: {chunk.content[:100]}...\")\n\n    # Access structured metadata\n    meta = chunk.get_chunk_metadata()\n    print(f\"Headings: {meta.headings}\")\n    print(f\"Page numbers: {meta.page_numbers}\")\n    print(f\"Labels: {meta.labels}\")\n</code></pre> <p>Chunks are returned with:</p> <ul> <li><code>content</code> - The chunk text</li> <li><code>order</code> - Position in document (0-indexed)</li> <li><code>metadata</code> - Dict with <code>doc_item_refs</code>, <code>headings</code>, <code>labels</code>, <code>page_numbers</code></li> <li><code>embedding</code> - <code>None</code> (not yet embedded)</li> <li><code>document_id</code> - <code>None</code> (not yet stored)</li> </ul>"},{"location":"custom-pipelines/#embed","title":"Embed","text":"<p><code>embed_chunks()</code> generates embeddings for chunks. It automatically contextualizes chunks (prepends section headings) before embedding for better semantic search, without modifying the stored content:</p> <pre><code>from haiku.rag.embeddings import embed_chunks\n\n# Generate embeddings (returns new Chunk objects)\nembedded_chunks = await embed_chunks(chunks)\n\n# Original chunks unchanged\nassert chunks[0].embedding is None\n\n# New chunks have embeddings\nassert embedded_chunks[0].embedding is not None\n</code></pre> <p><code>embed_chunks()</code> returns new <code>Chunk</code> objects with embeddings set. The original chunks are not modified.</p>"},{"location":"custom-pipelines/#contextualize-for-custom-embedders","title":"Contextualize (for custom embedders)","text":"<p><code>contextualize()</code> is a lower-level utility that prepares chunk content for embedding by prepending section headings. You only need this when implementing custom embedding logic\u2014<code>embed_chunks()</code> already calls it internally.</p> <pre><code>from haiku.rag.embeddings import contextualize\n\n# Get embedding-ready text (only needed for custom embedders)\ntexts = contextualize(chunks)\n# texts[0] might be: \"Chapter 1\\nIntroduction\\nThe actual chunk content...\"\n</code></pre> <p>See the Custom Embeddings example below for when to use <code>contextualize()</code>.</p>"},{"location":"custom-pipelines/#custom-processing-examples","title":"Custom Processing Examples","text":""},{"location":"custom-pipelines/#preprocessing-content","title":"Preprocessing Content","text":"<p>Transform content before chunking:</p> <pre><code>def clean_markdown(text: str) -&gt; str:\n    \"\"\"Remove HTML comments and normalize whitespace.\"\"\"\n    import re\n    text = re.sub(r'&lt;!--.*?--&gt;', '', text, flags=re.DOTALL)\n    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n    return text.strip()\n\nasync with HaikuRAG(\"database.lancedb\", create=True) as client:\n    # Convert to get raw content\n    docling_doc = await client.convert(\"document.md\")\n\n    # Extract and preprocess markdown\n    markdown = docling_doc.export_to_markdown()\n    cleaned = clean_markdown(markdown)\n\n    # Re-convert the cleaned content\n    processed_doc = await client.convert(cleaned)\n\n    # Continue with standard pipeline\n    chunks = await client.chunk(processed_doc)\n    embedded_chunks = await embed_chunks(chunks)\n\n    await client.import_document(\n        chunks=embedded_chunks,\n        content=cleaned,\n    )\n</code></pre>"},{"location":"custom-pipelines/#filtering-chunks","title":"Filtering Chunks","text":"<p>Remove unwanted chunks before embedding:</p> <pre><code>async with HaikuRAG(\"database.lancedb\", create=True) as client:\n    docling_doc = await client.convert(\"document.pdf\")\n    chunks = await client.chunk(docling_doc)\n\n    # Filter out short chunks or boilerplate\n    filtered = [\n        c for c in chunks\n        if len(c.content) &gt; 50\n        and \"copyright\" not in c.content.lower()\n    ]\n\n    # Re-number the order field after filtering\n    for i, chunk in enumerate(filtered):\n        chunk.order = i\n\n    embedded_chunks = await embed_chunks(filtered)\n\n    await client.import_document(\n        docling_document=docling_doc,\n        chunks=embedded_chunks,\n    )\n</code></pre>"},{"location":"custom-pipelines/#custom-embeddings","title":"Custom Embeddings","text":"<p>Use your own embedding service:</p> <pre><code>async def my_embedder(texts: list[str]) -&gt; list[list[float]]:\n    \"\"\"Your custom embedding function.\"\"\"\n    # Call your embedding API here\n    ...\n\nasync with HaikuRAG(\"database.lancedb\", create=True) as client:\n    docling_doc = await client.convert(\"document.pdf\")\n    chunks = await client.chunk(docling_doc)\n\n    # Use contextualize for consistent embedding input\n    texts = contextualize(chunks)\n\n    # Generate embeddings with your service\n    embeddings = await my_embedder(texts)\n\n    # Create chunks with embeddings\n    from haiku.rag.store.models.chunk import Chunk\n\n    embedded_chunks = [\n        Chunk(\n            content=chunk.content,\n            metadata=chunk.metadata,\n            order=chunk.order,\n            embedding=embedding,\n        )\n        for chunk, embedding in zip(chunks, embeddings)\n    ]\n\n    await client.import_document(\n        docling_document=docling_doc,\n        chunks=embedded_chunks,\n    )\n</code></pre>"},{"location":"custom-pipelines/#when-to-use-custom-pipelines","title":"When to Use Custom Pipelines","text":"<p>Use the primitives when you need to:</p> <ul> <li>Preprocess or clean content before chunking</li> <li>Filter or modify chunks before embedding</li> <li>Use external embedding services</li> <li>Implement custom chunking strategies</li> <li>Debug or inspect intermediate processing steps</li> </ul> <p>For standard use cases, prefer the convenience methods:</p> <ul> <li><code>create_document()</code> - Create from text content</li> <li><code>create_document_from_source()</code> - Create from file or URL</li> <li><code>import_document()</code> - Store pre-processed documents with custom chunks</li> </ul>"},{"location":"development/","title":"Development","text":"<p>This guide covers setting up a development environment and running tests.</p>"},{"location":"development/#setup","title":"Setup","text":"<p>Clone the repository and install dependencies:</p> <pre><code>git clone https://github.com/ggozad/haiku.rag.git\ncd haiku.rag\nuv sync\n</code></pre>"},{"location":"development/#running-tests","title":"Running Tests","text":"<pre><code>uv run pytest\n</code></pre>"},{"location":"development/#test-markers","title":"Test Markers","text":"<p>Tests use pytest markers to categorize them:</p> <ul> <li><code>@pytest.mark.integration</code> - Tests requiring local services (Docling models, etc.) that aren't available in CI</li> <li><code>@pytest.mark.asyncio</code> - Async tests (applied automatically via pytest-asyncio)</li> <li><code>@pytest.mark.vcr()</code> - Tests with HTTP call recording</li> </ul> <p>CI runs <code>pytest -m \"not integration\"</code> to skip integration tests.</p>"},{"location":"development/#http-recording-with-vcr","title":"HTTP Recording with VCR","text":"<p>Tests use pytest-recording (VCR.py) to record and replay HTTP calls. This allows tests to run without external services like Ollama or API providers.</p>"},{"location":"development/#how-it-works","title":"How It Works","text":"<ol> <li>Tests marked with <code>@pytest.mark.vcr()</code> record HTTP interactions to YAML cassettes</li> <li>On subsequent runs, HTTP calls are replayed from cassettes instead of hitting real services</li> <li>Cassettes are committed to the repository so CI can run tests without external dependencies</li> </ol>"},{"location":"development/#recording-new-cassettes","title":"Recording New Cassettes","text":"<p>When adding a new test that makes HTTP calls:</p> <ol> <li>Add the <code>@pytest.mark.vcr()</code> decorator to your test</li> <li>Run the test with the required services available (e.g., Ollama running)</li> <li>The cassette is automatically created on first run</li> </ol>"},{"location":"development/#re-recording-cassettes","title":"Re-recording Cassettes","text":"<p>To update an existing cassette, delete it and re-run the test, or use <code>--record-mode=rewrite</code>.</p>"},{"location":"development/#running-without-cassettes-live-mode","title":"Running Without Cassettes (Live Mode)","text":"<p>To run tests against real services instead of recorded cassettes:</p> <pre><code>uv run pytest --disable-recording\n</code></pre>"},{"location":"development/#writing-tests","title":"Writing Tests","text":""},{"location":"development/#common-fixtures","title":"Common Fixtures","text":"<p>Available fixtures from <code>tests/conftest.py</code>:</p> <ul> <li><code>temp_db_path</code> - Isolated temporary database</li> <li><code>temp_yaml_config</code> - Temporary config file</li> <li><code>allow_model_requests</code> - Enables pydantic-ai model calls</li> </ul>"},{"location":"development/#example-adding-a-new-test-with-vcr","title":"Example: Adding a New Test with VCR","text":"<pre><code>import pytest\nfrom haiku.rag.client import HaikuRAG\n\n\n@pytest.mark.asyncio\n@pytest.mark.vcr()\nasync def test_my_feature(temp_db_path):\n    async with HaikuRAG(temp_db_path, create=True) as client:\n        doc = await client.create_document(\"Test content\", uri=\"test://doc\")\n        assert doc.id is not None\n</code></pre>"},{"location":"development/#integration-tests","title":"Integration Tests","text":"<p>For tests requiring local services that can't be mocked via VCR:</p> <pre><code>@pytest.mark.integration\n@pytest.mark.asyncio\nasync def test_pdf_visualization(temp_db_path):\n    # Test code that needs local PDF processing\n    pass\n</code></pre> <p>Integration tests are skipped in CI but run locally when you have the required services.</p>"},{"location":"development/#linting-and-formatting","title":"Linting and Formatting","text":"<pre><code>uv run ruff check\nuv run ruff format\nuv run pyright\n</code></pre>"},{"location":"development/#mock-api-keys","title":"Mock API Keys","text":"<p>Tests automatically set mock API keys for providers that require them during client initialization. When running with VCR playback, these mock keys are sufficient since no real API calls are made.</p> <p>When recording new cassettes, set real API keys via environment variables:</p> <pre><code>ANTHROPIC_API_KEY=sk-ant-... uv run pytest tests/test_qa.py::test_qa_anthropic --record-mode=rewrite\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#choose-your-package","title":"Choose Your Package","text":"<p>haiku.rag is available in two packages:</p>"},{"location":"installation/#full-package-recommended","title":"Full Package (Recommended)","text":"<pre><code>uv pip install haiku.rag\n</code></pre> <p>The full package includes all features and extras: - Document processing (Docling) - PDF, DOCX, PPTX, images, and 40+ file formats - All embedding providers - VoyageAI - All rerankers - MixedBread AI, Cohere, Zero Entropy</p> <p>This is the easiest way to get started with all features enabled.</p>"},{"location":"installation/#slim-package-minimal-dependencies","title":"Slim Package (Minimal Dependencies)","text":"<pre><code># Minimal installation (no document processing)\nuv pip install haiku.rag-slim\n\n# With document processing\nuv pip install haiku.rag-slim[docling]\n\n# With specific providers\nuv pip install haiku.rag-slim[docling,voyageai,mxbai]\n</code></pre> <p>The slim package has minimal dependencies and lets you install only what you need:</p> <ul> <li><code>docling</code> - PDF, DOCX, PPTX, images, and other document formats</li> <li><code>voyageai</code> - VoyageAI embeddings</li> <li><code>mxbai</code> - MixedBread AI reranking</li> <li><code>cohere</code> - Cohere reranking</li> <li><code>zeroentropy</code> - Zero Entropy reranking</li> <li><code>tui</code> - Terminal UI for <code>chat</code> and <code>inspect</code> commands</li> </ul> <p>Built-in providers (no extras needed): - Ollama (default embedding provider) - OpenAI (GPT models for QA and embeddings) - Anthropic (Claude models for QA)</p> <p>See Configuration for configuring providers including advanced options like vLLM.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12+</li> <li>Ollama (for default embeddings and QA)</li> </ul>"},{"location":"installation/#pre-download-models-optional","title":"Pre-download Models (Optional)","text":"<p>You can prefetch all required runtime models before first use:</p> <pre><code>haiku-rag download-models\n</code></pre> <p>This will download: - Docling models for document processing - HuggingFace tokenizer models for chunking - Any Ollama models referenced by your current configuration</p>"},{"location":"installation/#remote-processing-optional","title":"Remote Processing (Optional)","text":"<p>When using <code>haiku.rag-slim</code>, you can skip installing the <code>docling</code> extra and instead use docling-serve for remote document processing. This is useful for:</p> <ul> <li>Keeping dependencies minimal</li> <li>Offloading heavy document processing to a dedicated service</li> <li>Production deployments with separate processing infrastructure</li> </ul> <p>See Remote processing for setup instructions and Document Processing for configuration options.</p>"},{"location":"installation/#docker","title":"Docker","text":"<p>Two Docker images are available:</p>"},{"location":"installation/#slim-image-minimal","title":"Slim Image (Minimal)","text":"<p>Pre-built slim image with minimal dependencies - use with external docling-serve for document processing:</p> <pre><code>docker pull ghcr.io/ggozad/haiku.rag-slim:latest\n</code></pre> <p>See <code>examples/docker/docker-compose.yml</code> for a complete setup with docling-serve.</p>"},{"location":"installation/#full-image-self-contained","title":"Full Image (Self-contained)","text":"<p>Build locally to include all features and document processing without docling-serve:</p> <pre><code>docker build -f docker/Dockerfile -t haiku-rag .\ndocker run -p 8001:8001 \\\n  -v /path/to/haiku.rag.yaml:/app/haiku.rag.yaml \\\n  -v /path/to/data:/data \\\n  haiku-rag\n</code></pre> <p>See <code>docker/README.md</code> for complete build and configuration instructions, including how to enable file monitoring.</p>"},{"location":"mcp/","title":"Model Context Protocol (MCP)","text":"<p>The MCP server exposes <code>haiku.rag</code> as MCP tools for compatible MCP clients like Claude Desktop.</p>"},{"location":"mcp/#available-tools","title":"Available Tools","text":""},{"location":"mcp/#document-management","title":"Document Management","text":"<ul> <li><code>add_document_from_file</code> - Add documents from local file paths</li> <li><code>file_path</code> (required): Path to the file</li> <li><code>metadata</code> (optional): Key-value metadata</li> <li> <p><code>title</code> (optional): Human-readable title</p> </li> <li> <p><code>add_document_from_url</code> - Add documents from URLs</p> </li> <li><code>url</code> (required): URL to fetch</li> <li><code>metadata</code> (optional): Key-value metadata</li> <li> <p><code>title</code> (optional): Human-readable title</p> </li> <li> <p><code>add_document_from_text</code> - Add documents from raw text content</p> </li> <li><code>content</code> (required): Text content</li> <li><code>uri</code> (optional): URI identifier</li> <li><code>metadata</code> (optional): Key-value metadata</li> <li> <p><code>title</code> (optional): Human-readable title</p> </li> <li> <p><code>get_document</code> - Retrieve a document by ID</p> </li> <li> <p><code>document_id</code> (required): The document ID</p> </li> <li> <p><code>list_documents</code> - List documents with pagination and filtering</p> </li> <li><code>limit</code> (optional): Maximum number to return</li> <li><code>offset</code> (optional): Number to skip</li> <li> <p><code>filter</code> (optional): SQL WHERE clause for filtering</p> </li> <li> <p><code>delete_document</code> - Delete a document by ID</p> </li> <li><code>document_id</code> (required): The document ID</li> </ul>"},{"location":"mcp/#search","title":"Search","text":"<ul> <li><code>search_documents</code> - Search using hybrid search (vector + full-text)</li> <li><code>query</code> (required): Search query</li> <li><code>limit</code> (optional): Maximum results (uses config default if not specified)</li> </ul>"},{"location":"mcp/#question-answering","title":"Question Answering","text":"<ul> <li><code>ask_question</code> - Ask questions about your documents</li> <li><code>question</code> (required): The question to ask</li> <li><code>cite</code> (optional): Include source citations (default: false)</li> <li> <p><code>deep</code> (optional): Use multi-agent deep QA for complex questions (default: false)</p> </li> <li> <p><code>research_question</code> - Run multi-agent research on complex topics</p> </li> <li><code>question</code> (required): The research question</li> <li> <p>Returns a structured research report with findings, conclusions, and sources</p> </li> <li> <p><code>rlm_question</code> - Answer complex analytical questions via code execution</p> </li> <li><code>question</code> (required): The question to answer</li> <li><code>filter</code> (optional): SQL WHERE clause to restrict document access</li> <li><code>document</code> (optional): Document title/ID to pre-load (can repeat)</li> <li>Best for aggregation, computation, and multi-document analysis</li> </ul>"},{"location":"mcp/#starting-mcp-server","title":"Starting MCP Server","text":"<p>The MCP server supports Streamable HTTP and stdio transports:</p> <pre><code># Default streamable HTTP transport on port 8001\nhaiku-rag serve --mcp\n\n# Custom port\nhaiku-rag serve --mcp --mcp-port 9000\n\n# stdio transport (for Claude Desktop)\nhaiku-rag serve --mcp --stdio\n\n# Read-only mode (excludes write tools)\nhaiku-rag --read-only serve --mcp --stdio\n</code></pre> <p>Read-only mode: When <code>--read-only</code> is specified, write tools (<code>add_document_from_file</code>, <code>add_document_from_url</code>, <code>add_document_from_text</code>, <code>delete_document</code>) are not registered. Only search and query tools remain available.</p>"},{"location":"mcp/#claude-desktop-integration","title":"Claude Desktop Integration","text":"<p>Add to your Claude Desktop configuration (<code>claude_desktop_config.json</code>):</p> <pre><code>{\n  \"mcpServers\": {\n    \"haiku-rag\": {\n      \"command\": \"haiku-rag\",\n      \"args\": [\"serve\", \"--mcp\", \"--stdio\"]\n    }\n  }\n}\n</code></pre> <p>With a custom database path:</p> <pre><code>{\n  \"mcpServers\": {\n    \"haiku-rag\": {\n      \"command\": \"haiku-rag\",\n      \"args\": [\"serve\", \"--mcp\", \"--stdio\", \"--db\", \"/path/to/database.lancedb\"]\n    }\n  }\n}\n</code></pre> <p>After restarting Claude Desktop, you can ask Claude to search your documents, add new content, or answer questions using your knowledge base.</p>"},{"location":"mcp/#running-with-other-services","title":"Running with Other Services","text":"<p>Combine MCP with file monitoring:</p> <pre><code># MCP + file monitoring\nhaiku-rag serve --mcp --monitor\n</code></pre> <p>See Server Mode for details on file monitoring.</p>"},{"location":"python/","title":"Python API","text":"<p>Use <code>haiku.rag</code> directly in your Python applications.</p>"},{"location":"python/#basic-usage","title":"Basic Usage","text":"<pre><code>from pathlib import Path\nfrom haiku.rag.client import HaikuRAG\n\n# Create a new database\nasync with HaikuRAG(\"path/to/database.lancedb\", create=True) as client:\n    # Your code here\n    pass\n\n# Open an existing database (will fail if database doesn't exist)\nasync with HaikuRAG(\"path/to/database.lancedb\") as client:\n    # Your code here\n    pass\n\n# Open in read-only mode (blocks writes)\nasync with HaikuRAG(\"path/to/database.lancedb\", read_only=True) as client:\n    results = await client.search(\"query\")  # Read operations work\n    # await client.create_document(...)  # Would raise ReadOnlyError\n</code></pre> <p>Note</p> <p>Databases must be explicitly created with <code>create=True</code> or via <code>haiku-rag init</code> before use. Operations on non-existent databases will raise <code>FileNotFoundError</code>.</p> <p>Note</p> <p>Read-only mode is useful for safely accessing databases without risk of modification. It blocks all write operations and prevents settings from being saved.</p> <p>Database Migrations</p> <p>When upgrading haiku.rag to a version with schema changes, opening an existing database will raise <code>MigrationRequiredError</code>. Run <code>haiku-rag migrate</code> to apply pending migrations before using the database. See CLI Database Management for details.</p>"},{"location":"python/#document-management","title":"Document Management","text":""},{"location":"python/#creating-documents","title":"Creating Documents","text":"<p>From text: <pre><code>doc = await client.create_document(\n    content=\"Your document content here\",\n    uri=\"doc://example\",\n    title=\"My Example Document\",  # optional human\u2011readable title\n    metadata={\"source\": \"manual\", \"topic\": \"example\"}\n)\n</code></pre></p> <p>From HTML content (preserves document structure): <pre><code>html_content = \"&lt;h1&gt;Title&lt;/h1&gt;&lt;p&gt;Paragraph&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Item 1&lt;/li&gt;&lt;/ul&gt;\"\ndoc = await client.create_document(\n    content=html_content,\n    uri=\"doc://html-example\",\n    format=\"html\"  # parse as HTML instead of markdown\n)\n</code></pre></p> <p>The <code>format</code> parameter controls how text content is parsed:</p> <ul> <li><code>\"md\"</code> (default) - Parse as Markdown</li> <li><code>\"html\"</code> - Parse as HTML, preserving semantic structure (headings, lists, tables)</li> <li><code>\"plain\"</code> - Plain text, no parsing (creates a simple text document)</li> </ul> <p>Note</p> <p>The document's <code>content</code> field stores the markdown export of the parsed document for consistent display. The original input is preserved in the <code>docling_document_json</code> field.</p> <p>From file: <pre><code>doc = await client.create_document_from_source(\n    \"path/to/document.pdf\", title=\"Project Brief\"\n)\n</code></pre></p> <p>From URL: <pre><code>doc = await client.create_document_from_source(\n    \"https://example.com/article.html\", title=\"Example Article\"\n)\n</code></pre></p>"},{"location":"python/#importing-pre-processed-documents","title":"Importing Pre-Processed Documents","text":"<p>If you process documents externally or need custom processing, use <code>import_document()</code>:</p> <pre><code>from haiku.rag.store.models.chunk import Chunk\n\n# Convert your source to a DoclingDocument\ndocling_doc = await client.convert(\"path/to/document.pdf\")\n\n# Create chunks (embeddings optional - will be generated if missing)\nchunks = [\n    Chunk(\n        content=\"This is the first chunk\",\n        metadata={\"section\": \"intro\"},\n        order=0,\n    ),\n    Chunk(\n        content=\"This is the second chunk\",\n        metadata={\"section\": \"body\"},\n        embedding=[0.1] * 1024,  # Optional: pre-computed embedding\n        order=1,\n    ),\n]\n\n# Import document with custom chunks\ndoc = await client.import_document(\n    docling_document=docling_doc,\n    chunks=chunks,\n    uri=\"doc://custom\",\n    title=\"Custom Document\",\n    metadata={\"source\": \"external-pipeline\"},\n)\n</code></pre> <p>The <code>docling_document</code> provides rich metadata for visual grounding, page numbers, and section headings. Content is automatically extracted from the DoclingDocument.</p> <p>See Custom Processing Pipelines for building pipelines with <code>convert()</code>, <code>chunk()</code>, and <code>embed_chunks()</code>.</p>"},{"location":"python/#retrieving-documents","title":"Retrieving Documents","text":"<p>By ID: <pre><code>doc = await client.get_document_by_id(\"document-id-string\")\n</code></pre></p> <p>By URI: <pre><code>doc = await client.get_document_by_uri(\"file:///path/to/document.pdf\")\n</code></pre></p> <p>List all documents: <pre><code>docs = await client.list_documents(limit=10, offset=0)\n\n# Include full content and docling document (not loaded by default)\ndocs = await client.list_documents(include_content=True)\n</code></pre></p> <p>Filter documents by properties: <pre><code># Filter by URI pattern\ndocs = await client.list_documents(filter=\"uri LIKE '%arxiv%'\")\n\n# Filter by exact title\ndocs = await client.list_documents(filter=\"title = 'My Document'\")\n\n# Combine multiple conditions\ndocs = await client.list_documents(\n    limit=10,\n    filter=\"uri LIKE '%.pdf' AND title LIKE '%paper%'\"\n)\n</code></pre></p> <p>Count documents: <pre><code># Count all documents\ntotal = await client.count_documents()\n\n# Count with filter\npdf_count = await client.count_documents(filter=\"uri LIKE '%.pdf'\")\n</code></pre></p>"},{"location":"python/#updating-documents","title":"Updating Documents","text":"<pre><code># Update content (triggers re-chunking)\nawait client.update_document(document_id=doc.id, content=\"New content\")\n\n# Update metadata only (no re-chunking)\nawait client.update_document(\n    document_id=doc.id,\n    metadata={\"version\": \"2.0\", \"updated_by\": \"admin\"}\n)\n\n# Update title only (no re-chunking)\nawait client.update_document(document_id=doc.id, title=\"New Title\")\n\n# Update multiple fields at once\nawait client.update_document(\n    document_id=doc.id,\n    content=\"New content\",\n    title=\"Updated Title\",\n    metadata={\"status\": \"final\"}\n)\n\n# Use custom chunks (embeddings optional - will be generated if missing)\ncustom_chunks = [\n    Chunk(content=\"Custom chunk 1\"),\n    Chunk(content=\"Custom chunk 2\", embedding=[...]),  # Pre-computed embedding\n]\nawait client.update_document(document_id=doc.id, chunks=custom_chunks)\n</code></pre> <p>Notes:</p> <ul> <li>Updates to only <code>metadata</code> or <code>title</code> skip re-chunking</li> <li>Updates to <code>content</code> trigger re-chunking and re-embedding</li> <li>Custom <code>chunks</code> with embeddings are stored as-is; missing embeddings are generated automatically</li> </ul>"},{"location":"python/#deleting-documents","title":"Deleting Documents","text":"<pre><code>await client.delete_document(doc.id)\n</code></pre>"},{"location":"python/#rebuilding-the-database","title":"Rebuilding the Database","text":"<pre><code>from haiku.rag.client import RebuildMode\n\n# Full rebuild (default) - re-converts from source files, re-chunks, re-embeds\nasync for doc_id in client.rebuild_database():\n    print(f\"Processed document {doc_id}\")\n\n# Re-chunk from stored content (no source file access)\nasync for doc_id in client.rebuild_database(mode=RebuildMode.RECHUNK):\n    print(f\"Processed document {doc_id}\")\n\n# Only regenerate embeddings (fastest, keeps existing chunks)\nasync for doc_id in client.rebuild_database(mode=RebuildMode.EMBED_ONLY):\n    print(f\"Processed document {doc_id}\")\n</code></pre> <p>Rebuild modes:</p> <ul> <li><code>RebuildMode.FULL</code> - Re-convert from source files, re-chunk, re-embed (default)</li> <li><code>RebuildMode.RECHUNK</code> - Re-chunk from existing document content, re-embed</li> <li><code>RebuildMode.EMBED_ONLY</code> - Keep existing chunks, only regenerate embeddings</li> <li><code>RebuildMode.TITLE_ONLY</code> - Generate titles for untitled documents (no re-chunking or re-embedding)</li> </ul>"},{"location":"python/#generating-titles","title":"Generating Titles","text":"<p>Generate a title for an existing document on demand:</p> <pre><code>title = await client.generate_title(doc)\nif title:\n    await client.update_document(document_id=doc.id, title=title)\n</code></pre> <p>Uses the same two-tier approach as automatic ingestion: structural extraction from DoclingDocument metadata first, with LLM fallback via <code>processing.title_model</code>. Unlike ingestion, this method does not catch exceptions \u2014 if the LLM call fails, the error propagates.</p> <p>To batch-generate titles for all untitled documents, use <code>RebuildMode.TITLE_ONLY</code>:</p> <pre><code>async for doc_id in client.rebuild_database(mode=RebuildMode.TITLE_ONLY):\n    print(f\"Generated title for {doc_id}\")\n</code></pre> <p>See Automatic Title Generation for configuration details.</p>"},{"location":"python/#maintenance","title":"Maintenance","text":"<p>Run maintenance to optimize storage and prune old table versions:</p> <pre><code>await client.vacuum()\n</code></pre> <p>This compacts tables and removes historical versions to keep disk usage in check. It\u2019s safe to run anytime, for example after bulk imports or periodically in long\u2011running apps.</p>"},{"location":"python/#atomic-writes-and-rollback","title":"Atomic Writes and Rollback","text":"<p>Document create and update operations take a snapshot of table versions before any write and automatically roll back to that snapshot if something fails (for example, during chunking or embedding). This restores both the <code>documents</code> and <code>chunks</code> tables to their pre\u2011operation state using LanceDB\u2019s table versioning.</p> <ul> <li>Applies to: <code>create_document(...)</code>, <code>create_document_from_source(...)</code>, <code>update_document(...)</code>, and internal rebuild/update flows.</li> <li>Scope: Both document rows and all associated chunks are rolled back together.</li> <li>Vacuum: Running <code>vacuum()</code> later prunes old versions for disk efficiency; rollbacks occur immediately during the failing operation and are not impacted.</li> </ul>"},{"location":"python/#searching-documents","title":"Searching Documents","text":"<p>The search method performs native hybrid search (vector + full-text) using LanceDB with optional reranking for improved relevance:</p> <p>Basic hybrid search (default): <pre><code>results = await client.search(\"machine learning algorithms\", limit=5)\nfor result in results:\n    print(f\"Score: {result.score:.3f}\")\n    print(f\"Content: {result.content}\")\n    print(f\"Document ID: {result.document_id}\")\n</code></pre></p> <p>Search with different search types: <pre><code># Vector search only\nresults = await client.search(\n    query=\"machine learning\",\n    limit=5,\n    search_type=\"vector\"\n)\n\n# Full-text search only\nresults = await client.search(\n    query=\"machine learning\",\n    limit=5,\n    search_type=\"fts\"\n)\n\n# Hybrid search (default - combines vector + fts with native LanceDB RRF)\nresults = await client.search(\n    query=\"machine learning\",\n    limit=5,\n    search_type=\"hybrid\"\n)\n\n# Process results\nfor result in results:\n    print(f\"Relevance: {result.score:.3f}\")\n    print(f\"Content: {result.content}\")\n    print(f\"From document: {result.document_id}\")\n    print(f\"Document URI: {result.document_uri}\")\n    print(f\"Document Title: {result.document_title}\")  # when available\n</code></pre></p>"},{"location":"python/#filtering-search-results","title":"Filtering Search Results","text":"<p>Filter search results to only include chunks from documents matching specific criteria:</p> <pre><code># Filter by document URI pattern\nresults = await client.search(\n    query=\"machine learning\",\n    limit=5,\n    filter=\"uri LIKE '%arxiv%'\"\n)\n\n# Filter by exact document title\nresults = await client.search(\n    query=\"neural networks\",\n    limit=5,\n    filter=\"title = 'Deep Learning Guide'\"\n)\n\n# Combine multiple filter conditions\nresults = await client.search(\n    query=\"AI research\",\n    limit=5,\n    filter=\"uri LIKE '%.pdf' AND title LIKE '%paper%'\"\n)\n\n# Filter with any search type\nresults = await client.search(\n    query=\"transformers\",\n    limit=5,\n    search_type=\"vector\",\n    filter=\"uri LIKE '%huggingface%'\"\n)\n</code></pre> <p>Note: Filters apply to document properties only. Available columns for filtering: - <code>id</code> - Document ID - <code>uri</code> - Document URI/URL - <code>title</code> - Document title (if set) - <code>created_at</code>, <code>updated_at</code> - Timestamps - <code>metadata</code> - Document metadata (as string, use LIKE for pattern matching)</p>"},{"location":"python/#expanding-search-context","title":"Expanding Search Context","text":"<p>Expand search results with adjacent chunks for more complete context:</p> <pre><code># Get initial search results\nsearch_results = await client.search(\"machine learning\", limit=3)\n\n# Expand search results with adjacent content from the source document\nexpanded_results = await client.expand_context(search_results)\n\n# The expanded results contain chunks with combined content\nfor result in expanded_results:\n    print(f\"Expanded content: {result.content}\")\n</code></pre> <p>Context expansion uses your configuration settings:</p> <ul> <li>search.context_radius: For text content (paragraphs), includes N DocItems before and after</li> <li>search.max_context_items: Limits how many document items can be included</li> <li>search.max_context_chars: Hard limit on total characters</li> </ul> <p>Type-aware expansion: Structural content (tables, code blocks, lists) automatically expands to include the complete structure, regardless of how it was split during chunking.</p> <p>Smart Merging: When expanded chunks overlap or are adjacent within the same document, they are automatically merged into single chunks with continuous content. This eliminates duplication and provides coherent text blocks. The merged chunk uses the highest relevance score from the original chunks.</p>"},{"location":"python/#question-answering","title":"Question Answering","text":"<p>Ask questions about your documents:</p> <pre><code>answer, citations = await client.ask(\"Who is the author of haiku.rag?\")\nprint(answer)\nfor cite in citations:\n    print(f\"  [{cite.chunk_id}] {cite.document_title or cite.document_uri}\")\n</code></pre> <p>Customize the QA agent's behavior with a custom system prompt:</p> <pre><code>custom_prompt = \"\"\"You are a technical support expert for WIX.\nAnswer questions based on the knowledge base documents provided.\nBe concise and helpful.\"\"\"\n\nanswer, citations = await client.ask(\n    \"How do I create a blog?\",\n    system_prompt=custom_prompt\n)\n</code></pre> <p>Filter to specific documents:</p> <pre><code>answer, citations = await client.ask(\n    \"What are the main findings?\",\n    filter=\"uri LIKE '%paper%'\"\n)\n</code></pre> <p>The QA agent searches your documents for relevant information and uses the configured LLM to generate an answer. The method returns a tuple of <code>(answer_text, list[Citation])</code>. Citations include page numbers, section headings, and document references.</p> <p>The QA provider and model are configured in <code>haiku.rag.yaml</code> or can be passed directly to the client (see Configuration).</p> <p>See also: Agents for details on the QA agent and the multi\u2011agent research workflow.</p>"},{"location":"python/#rlm-recursive-language-model","title":"RLM (Recursive Language Model)","text":"<p>Answer complex analytical questions via code execution:</p> <pre><code># Aggregation across documents\nresult = await client.rlm(\"Which quarter had the highest revenue?\")\nprint(result.answer)    # The answer\nprint(result.program)   # The final consolidated program\n\n# Computation within a document set\nresult = await client.rlm(\n    \"What is the average deal size mentioned in these contracts?\",\n    filter=\"uri LIKE '%contracts%'\"\n)\n\n# Multi-document comparison\nresult = await client.rlm(\n    \"What changed between these two versions of the policy?\",\n    documents=[\"Policy v1.0\", \"Policy v2.0\"]\n)\n</code></pre> <p>The RLM agent writes and executes Python code in a sandboxed environment to solve problems that traditional RAG struggles with: aggregation, computation, and multi-document analysis.</p> <p>See RLM Agent for details on capabilities and configuration.</p>"},{"location":"python/#building-custom-agents","title":"Building Custom Agents","text":"<p>haiku.rag provides a RAG skill built on haiku.skills that bundles all capabilities into a composable agent:</p> <pre><code>from pydantic_ai import Agent\nfrom haiku.rag.skills.rag import create_skill\nfrom haiku.skills.agent import SkillToolset\nfrom haiku.skills.prompts import build_system_prompt\n\nskill = create_skill(db_path=db_path, config=config)\ntoolset = SkillToolset(skills=[skill])\n\nagent = Agent(\n    \"openai:gpt-4o\",\n    instructions=build_system_prompt(toolset.skill_catalog),\n    toolsets=[toolset],\n)\n\nresult = await agent.run(\"What are the main findings?\")\n</code></pre> <p>See Toolsets for the full API reference.</p>"},{"location":"remote-processing/","title":"Remote Processing","text":"<p><code>haiku.rag</code> can use docling-serve for remote document processing and chunking, offloading resource-intensive operations to a dedicated service.</p>"},{"location":"remote-processing/#overview","title":"Overview","text":"<p>docling-serve is a REST API service that provides:</p> <ul> <li>Document conversion (PDF, DOCX, PPTX, images, etc.)</li> <li>Intelligent chunking with structure preservation</li> <li>OCR capabilities for scanned documents</li> <li>Table and figure extraction</li> </ul>"},{"location":"remote-processing/#when-to-use-docling-serve","title":"When to Use docling-serve","text":"<p>Use local processing (default) when:</p> <ul> <li>Working with small to medium document volumes</li> <li>Running on development machines</li> <li>Want zero external dependencies</li> <li>Processing simple document formats</li> </ul> <p>Use docling-serve when:</p> <ul> <li>Processing large volumes of documents</li> <li>Working with complex PDFs requiring OCR</li> <li>Running in production environments</li> <li>Want to separate compute-intensive tasks</li> <li>Need to scale document processing independently</li> </ul>"},{"location":"remote-processing/#setup","title":"Setup","text":""},{"location":"remote-processing/#docker-compose-recommended","title":"Docker Compose (Recommended)","text":"<p>The easiest way to use haiku.rag with docling-serve is using the slim Docker image with docker-compose. See <code>examples/docker/docker-compose.yml</code> for a complete setup that includes both services.</p>"},{"location":"remote-processing/#running-docling-serve-manually","title":"Running docling-serve Manually","text":"<p>See the official docling-serve repository for installation options. The quickest way is using Docker:</p> <pre><code>docker run -p 5001:5001 quay.io/docling-project/docling-serve\n</code></pre> <p>To enable the web UI for debugging:</p> <pre><code>docker run -p 5001:5001 -e DOCLING_SERVE_ENABLE_UI=true quay.io/docling-project/docling-serve\n</code></pre>"},{"location":"remote-processing/#configuration","title":"Configuration","text":"<p>Configure haiku.rag to use docling-serve. See the Document Processing guide for all available options.</p> <pre><code># haiku.rag.yaml\nprocessing:\n  converter: docling-serve  # Use remote conversion\n  chunker: docling-serve    # Use remote chunking\n\nproviders:\n  docling_serve:\n    base_url: http://localhost:5001\n    api_key: \"\"  # Optional API key for authentication\n</code></pre>"},{"location":"remote-processing/#features","title":"Features","text":""},{"location":"remote-processing/#remote-document-conversion","title":"Remote Document Conversion","text":"<p>When <code>converter: docling-serve</code> is configured, documents are sent to the docling-serve API for conversion:</p> <pre><code>from haiku.rag.client import HaikuRAG\n\nasync with HaikuRAG() as client:\n    # PDF is processed by docling-serve\n    doc = await client.create_document_from_source(\"complex.pdf\")\n</code></pre>"},{"location":"remote-processing/#remote-chunking","title":"Remote Chunking","text":"<p>When <code>chunker: docling-serve</code> is configured, chunking is performed remotely:</p> <pre><code>processing:\n  chunker: docling-serve\n  chunker_type: hybrid              # or hierarchical\n  chunk_size: 256\n  chunking_tokenizer: \"Qwen/Qwen3-Embedding-0.6B\"\n  chunking_merge_peers: true\n  chunking_use_markdown_tables: false\n</code></pre>"},{"location":"remote-processing/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"remote-processing/#custom-tokenizers","title":"Custom Tokenizers","text":"<p>You can use any HuggingFace tokenizer model:</p> <pre><code>processing:\n  chunking_tokenizer: \"bert-base-uncased\"  # Or any HF model\n</code></pre>"},{"location":"remote-processing/#chunking-strategies","title":"Chunking Strategies","text":"<p>Hybrid Chunking (default):</p> <ul> <li>Best for most documents</li> <li>Preserves semantic boundaries</li> <li>Structure-aware splitting</li> </ul> <p>Hierarchical Chunking:</p> <ul> <li>Maintains document hierarchy</li> <li>Better for deeply nested documents</li> <li>Preserves parent-child relationships</li> </ul> <pre><code>processing:\n  chunker_type: hierarchical\n</code></pre>"},{"location":"remote-processing/#table-handling","title":"Table Handling","text":"<p>Control how tables are represented:</p> <pre><code>processing:\n  chunking_use_markdown_tables: true  # Preserve table structure\n</code></pre> <ul> <li><code>false</code> (default): Tables as narrative text</li> <li><code>true</code>: Tables as markdown format</li> </ul>"},{"location":"remote-processing/#vlm-picture-description-with-docling-serve","title":"VLM Picture Description with docling-serve","text":"<p>When using VLM picture description with docling-serve, the VLM API calls are made by the docling-serve container, not by haiku.rag. This requires additional configuration.</p>"},{"location":"remote-processing/#enable-remote-services","title":"Enable Remote Services","text":"<p>docling-serve blocks external API calls by default. To enable VLM picture description, start docling-serve with:</p> <pre><code>docker run -p 5001:5001 -e DOCLING_SERVE_ENABLE_REMOTE_SERVICES=true quay.io/docling-project/docling-serve\n</code></pre>"},{"location":"remote-processing/#docker-networking","title":"Docker Networking","text":"<p>When docling-serve runs in Docker and your VLM (e.g., Ollama) runs on the host, <code>localhost</code> inside the container refers to the container itself, not your host machine.</p> <p>Use <code>host.docker.internal</code> to reach host services from within Docker:</p> <pre><code># haiku.rag.yaml\nprocessing:\n  converter: docling-serve\n  chunker: docling-serve\n  conversion_options:\n    picture_description:\n      enabled: true\n      model:\n        provider: ollama\n        name: ministral-3\n        base_url: http://host.docker.internal:11434  # NOT localhost!\n</code></pre>"},{"location":"remote-processing/#complete-example","title":"Complete Example","text":"<ol> <li>Start Ollama with a vision model on your host:</li> </ol> <pre><code>ollama pull ministral-3\nollama serve\n</code></pre> <ol> <li>Start docling-serve with remote services enabled:</li> </ol> <pre><code>docker run -p 5001:5001 -e DOCLING_SERVE_ENABLE_REMOTE_SERVICES=true quay.io/docling-project/docling-serve\n</code></pre> <ol> <li>Configure haiku.rag:</li> </ol> <pre><code># haiku.rag.yaml\nprocessing:\n  converter: docling-serve\n  chunker: docling-serve\n  conversion_options:\n    picture_description:\n      enabled: true\n      model:\n        provider: ollama\n        name: ministral-3\n        base_url: http://host.docker.internal:11434\n\nproviders:\n  docling_serve:\n    base_url: http://localhost:5001\n</code></pre> <ol> <li>Add a document:</li> </ol> <pre><code>haiku-rag add-src document.pdf\n</code></pre>"},{"location":"remote-processing/#resources","title":"Resources","text":"<ul> <li>docling-serve GitHub</li> <li>docling-serve Documentation</li> </ul>"},{"location":"server/","title":"Server Mode","text":"<p>The server provides automatic file monitoring and MCP functionality.</p>"},{"location":"server/#starting-the-server","title":"Starting the Server","text":"<p>The <code>serve</code> command requires at least one service flag. You can enable file monitoring, MCP server, or both:</p>"},{"location":"server/#mcp-server-only","title":"MCP Server Only","text":"<pre><code>haiku-rag serve --mcp\n</code></pre> <p>Transport options: - Default - Streamable HTTP transport on port 8001 - <code>--stdio</code> - Standard input/output transport - <code>--mcp-port</code> - Custom port (default: 8001)</p>"},{"location":"server/#file-monitoring-only","title":"File Monitoring Only","text":"<pre><code>haiku-rag serve --monitor\n</code></pre>"},{"location":"server/#both-services","title":"Both Services","text":"<pre><code>haiku-rag serve --monitor --mcp\n</code></pre> <p>This will start file monitoring and MCP server on port 8001.</p>"},{"location":"server/#file-monitoring","title":"File Monitoring","text":"<p>Configure directories to monitor in your <code>haiku.rag.yaml</code> (see Document Processing for all options):</p> <pre><code>monitor:\n  directories:\n    - /path/to/documents\n    - /another/path\n</code></pre> <p>Then start the server:</p> <pre><code>haiku-rag serve --monitor\n</code></pre>"},{"location":"server/#monitoring-features","title":"Monitoring Features","text":"<ul> <li>Startup: Scans all monitored directories and adds new files</li> <li>File Added/Modified: Automatically parses and updates documents</li> <li>File Deleted: Removes corresponding documents from database</li> </ul>"},{"location":"server/#filtering-files","title":"Filtering Files","text":"<p>You can filter which files to monitor using gitignore-style patterns:</p> <pre><code>monitor:\n  directories:\n    - /path/to/documents\n\n  # Ignore patterns (exclude files)\n  ignore_patterns:\n    - \"*draft*\"         # Ignore draft files\n    - \"temp/\"           # Ignore temp directory\n    - \"**/archive/**\"   # Ignore archive directories\n\n  # Include patterns (whitelist files)\n  include_patterns:\n    - \"*.md\"           # Only markdown files\n    - \"**/docs/**\"     # Files in docs directories\n</code></pre> <p>Pattern behavior: - Extension filtering is applied first (only supported file types) - Include patterns create a whitelist (if specified) - Ignore patterns exclude files - Both can be combined for fine-grained control</p>"},{"location":"server/#supported-formats","title":"Supported Formats","text":"<p>The file monitor processes documents using Docling, which supports:</p> <p>Documents: - PDF (<code>.pdf</code>) - with OCR support for scanned documents - Microsoft Word (<code>.docx</code>) - Microsoft Excel (<code>.xlsx</code>) - Microsoft PowerPoint (<code>.pptx</code>) - HTML (<code>.html</code>, <code>.htm</code>) - Markdown (<code>.md</code>) - AsciiDoc (<code>.adoc</code>, <code>.asciidoc</code>)</p> <p>Data formats: - CSV (<code>.csv</code>) - JSON (<code>.json</code>) - XML (<code>.xml</code>)</p> <p>Images (via OCR): - PNG (<code>.png</code>) - JPEG (<code>.jpg</code>, <code>.jpeg</code>) - TIFF (<code>.tiff</code>, <code>.tif</code>) - BMP (<code>.bmp</code>)</p> <p>Code files: - Python (<code>.py</code>) - JavaScript (<code>.js</code>) - TypeScript (<code>.ts</code>) - And other text-based code files</p> <p>Plain text: - Text files (<code>.txt</code>) - RST (<code>.rst</code>)</p> <p>URLs are also supported - the content is fetched and converted to markdown.</p>"},{"location":"tools/","title":"Toolsets","text":"<p>haiku.rag exposes its RAG capabilities as haiku.skills skills. See the Skills section for the primary way to use haiku.rag tools.</p> <p>For lower-level access, <code>haiku.rag.tools</code> provides individual <code>FunctionToolset</code> factories used internally by agents.</p>"},{"location":"tools/#low-level-toolsets","title":"Low-Level Toolsets","text":"<p>For advanced use cases, individual toolset factories are available in <code>haiku.rag.tools</code>. These are used internally by the QA agent and can be composed into custom agents.</p>"},{"location":"tools/#ragdeps-protocol","title":"RAGDeps Protocol","text":"<p>All toolsets use the <code>RAGDeps</code> protocol for dependency injection:</p> <pre><code>from haiku.rag.tools import RAGDeps\n\nclass MyDeps:\n    def __init__(self, client: HaikuRAG):\n        self.client = client\n</code></pre>"},{"location":"tools/#search-toolset","title":"Search Toolset","text":"<p><code>create_search_toolset()</code> provides hybrid search with context expansion.</p> <pre><code>from haiku.rag.tools import create_search_toolset\n\nsearch = create_search_toolset(config)\n</code></pre> Parameter Default Description <code>config</code> required <code>AppConfig</code> <code>expand_context</code> <code>True</code> Expand results with surrounding chunks <code>base_filter</code> <code>None</code> SQL WHERE clause applied to all searches <code>tool_name</code> <code>\"search\"</code> Name of the tool exposed to the agent <code>on_results</code> <code>None</code> Callback <code>(list[SearchResult]) -&gt; None</code> invoked with results"},{"location":"tools/#document-toolset","title":"Document Toolset","text":"<p><code>create_document_toolset()</code> provides document browsing and retrieval.</p> <pre><code>from haiku.rag.tools import create_document_toolset\n\ndocs = create_document_toolset(config)\n</code></pre> Parameter Default Description <code>config</code> required <code>AppConfig</code> <code>base_filter</code> <code>None</code> SQL WHERE clause for list operations <p>Tools:</p> <ul> <li><code>list_documents(page?)</code> \u2014 Paginated document listing (50 per page).</li> <li><code>get_document(query)</code> \u2014 Retrieve a document by title or URI.</li> <li><code>summarize_document(query)</code> \u2014 Generate an LLM summary of a document's content.</li> </ul>"},{"location":"tools/#analysis-toolset","title":"Analysis Toolset","text":"<p><code>create_analysis_toolset()</code> provides computational analysis via the RLM agent.</p> <pre><code>from haiku.rag.tools import create_analysis_toolset\n\nanalysis = create_analysis_toolset(config)\n</code></pre> Parameter Default Description <code>config</code> required <code>AppConfig</code> <code>base_filter</code> <code>None</code> SQL WHERE clause applied to searches <code>tool_name</code> <code>\"analyze\"</code> Name of the tool exposed to the agent"},{"location":"tools/#filter-helpers","title":"Filter Helpers","text":"<p><code>haiku.rag.tools.filters</code> provides utilities for building SQL filters:</p> <ul> <li><code>build_document_filter(document_name)</code> \u2014 Builds a LIKE filter matching against both <code>uri</code> and <code>title</code>, case-insensitive. Also matches without spaces (e.g., \"TB MED 593\" matches \"tbmed593\").</li> <li><code>build_multi_document_filter(document_names)</code> \u2014 Combines multiple document name filters with OR logic.</li> <li><code>combine_filters(filter1, filter2)</code> \u2014 Combines two filters with AND logic. Returns <code>None</code> if both are <code>None</code>.</li> </ul>"},{"location":"tuning/","title":"Tuning haiku.rag for Your Corpus","text":"<p>This guide explains how to tune haiku.rag settings based on your document corpus characteristics. The right settings depend on your document types, query patterns, and accuracy requirements.</p>"},{"location":"tuning/#key-concepts","title":"Key Concepts","text":""},{"location":"tuning/#retrieval-vs-generation","title":"Retrieval vs Generation","text":"<p>RAG has two phases:</p> <ol> <li>Retrieval: Finding relevant chunks from your corpus</li> <li>Generation: Using those chunks to answer questions</li> </ol> <p>Poor retrieval means the LLM never sees the relevant content, regardless of how good the model is. Tuning retrieval is usually more impactful than tuning generation.</p>"},{"location":"tuning/#recall-vs-precision","title":"Recall vs Precision","text":"<ul> <li>Recall: What fraction of relevant documents did we find?</li> <li>Precision: What fraction of retrieved documents are relevant?</li> </ul> <p>For RAG, recall matters more than precision. Missing a relevant chunk means wrong answers. Including an extra irrelevant chunk just wastes context tokens.</p>"},{"location":"tuning/#search-settings","title":"Search Settings","text":""},{"location":"tuning/#searchlimit","title":"<code>search.limit</code>","text":"<p>Default number of chunks to retrieve.</p> <pre><code>search:\n  limit: 5  # Default\n</code></pre> <p>When to increase:</p> <ul> <li>Complex questions requiring information from multiple sources</li> <li>Broad topics spread across many documents</li> </ul> <p>When to decrease:</p> <ul> <li>Simple factual questions</li> <li>Highly focused corpus where top results are usually correct</li> <li>Cost-sensitive deployments (fewer chunks = fewer tokens)</li> </ul> <p>Typical values: 3-10</p>"},{"location":"tuning/#searchcontext_radius","title":"<code>search.context_radius</code>","text":"<p>Number of adjacent DocItems to include when expanding search results. Only applies to text content (paragraphs). Tables, code blocks, and lists use structural expansion automatically.</p> <pre><code>search:\n  context_radius: 0  # Default: no expansion\n</code></pre> <p>When to increase:</p> <ul> <li>Answers require surrounding context (definitions, explanations)</li> <li>Chunks are small and queries need more context</li> <li>Documents have strong local coherence (adjacent paragraphs relate)</li> </ul> <p>When to keep at 0:</p> <ul> <li>Large chunks that already contain sufficient context</li> <li>Documents where adjacent content is often unrelated</li> <li>When chunk boundaries align well with semantic units</li> </ul> <p>Typical values: 0-3</p>"},{"location":"tuning/#searchmax_context_items-and-searchmax_context_chars","title":"<code>search.max_context_items</code> and <code>search.max_context_chars</code>","text":"<p>Safety limits on context expansion to prevent runaway expansion.</p> <pre><code>search:\n  max_context_items: 10     # Max DocItems per expanded result\n  max_context_chars: 10000  # Max characters per expanded result\n</code></pre> <p>Increase if expansion is being truncated and you need more context. Decrease if expanded results are too long for your LLM context window.</p>"},{"location":"tuning/#processing-settings","title":"Processing Settings","text":""},{"location":"tuning/#processingchunk_size","title":"<code>processing.chunk_size</code>","text":"<p>Maximum tokens per chunk (using the configured tokenizer).</p> <pre><code>processing:\n  chunk_size: 256  # Default\n</code></pre> <p>Trade-offs:</p> Smaller chunks (128-256) Larger chunks (512-1024) More precise retrieval Better context per chunk May miss spanning content Better recall More chunks to search Faster search Better for specific queries Better for broad queries <p>Guidance by corpus type:</p> <ul> <li>Technical documentation: 256-512 (specific lookups)</li> <li>Long-form articles: 512-1024 (need context)</li> <li>FAQs/short answers: 128-256 (discrete answers)</li> <li>Code documentation: 256-512 (function-level)</li> </ul>"},{"location":"tuning/#processingchunker_type","title":"<code>processing.chunker_type</code>","text":"<p>Chunking strategy.</p> <pre><code>processing:\n  chunker_type: hybrid  # Default\n</code></pre> <ul> <li><code>hybrid</code>: Structure-aware with token limits. Best for most documents.</li> <li><code>hierarchical</code>: Preserves document hierarchy strictly. Use for highly structured documents where hierarchy matters.</li> </ul>"},{"location":"tuning/#processingchunking_merge_peers","title":"<code>processing.chunking_merge_peers</code>","text":"<p>Whether to merge adjacent small chunks that share the same section.</p> <pre><code>processing:\n  chunking_merge_peers: true  # Default\n</code></pre> <p>Keep <code>true</code> unless you specifically want very granular chunks. Merging improves embedding quality by ensuring chunks have sufficient context.</p>"},{"location":"tuning/#embedding-settings","title":"Embedding Settings","text":""},{"location":"tuning/#model-selection","title":"Model Selection","text":"<p>Embedding model choice significantly impacts retrieval quality.</p> <pre><code>embeddings:\n  model:\n    provider: ollama\n    name: qwen3-embedding:4b\n    vector_dim: 2560\n</code></pre> <p>Considerations:</p> <ul> <li>Larger models generally produce better embeddings but are slower</li> <li>Match <code>vector_dim</code> to your model's actual output dimension</li> <li>Local models (Ollama) vs API models (OpenAI, VoyageAI) trade-off cost vs quality</li> </ul>"},{"location":"tuning/#contextualizing-embeddings","title":"Contextualizing Embeddings","text":"<p>Chunks are embedded with section headings prepended (via <code>contextualize()</code>). This improves retrieval by including structural context in the embedding.</p> <p>If your documents lack clear headings, embeddings will be based on chunk content alone.</p>"},{"location":"tuning/#reranking","title":"Reranking","text":"<p>Reranking retrieves more candidates than needed, then uses a cross-encoder to re-score them.</p> <pre><code>reranking:\n  model:\n    provider: mxbai  # or cohere, zeroentropy, vllm\n    name: mixedbread-ai/mxbai-rerank-base-v2\n</code></pre> <p>When to use reranking:</p> <ul> <li>Embedding model has limited accuracy</li> <li>Queries are complex or ambiguous</li> <li>You can afford the latency (adds ~100-500ms)</li> </ul> <p>When to skip reranking:</p> <ul> <li>Simple, specific queries</li> <li>High-quality embedding model</li> <li>Latency-sensitive applications</li> </ul> <p>When reranking is enabled, haiku.rag automatically retrieves 10x the requested limit, then reranks to the final count. You don't need to adjust <code>search.limit</code> for reranking.</p>"},{"location":"tuning/#tuning-workflow","title":"Tuning Workflow","text":""},{"location":"tuning/#1-use-the-inspector","title":"1. Use the Inspector","text":"<p>The inspector is your best tool for understanding how your corpus is chunked and how search behaves:</p> <pre><code>haiku-rag inspect\n</code></pre> <p>What to look for:</p> <ul> <li>Browse documents and their chunks to see how content is split</li> <li>Use the search modal (<code>/</code>) to test queries and see which chunks are retrieved</li> <li>Press <code>c</code> on a chunk to view expanded context - see what additional content would be included with <code>context_radius &gt; 0</code></li> <li>Check chunk sizes - are they too small (fragmented) or too large (unfocused)?</li> </ul>"},{"location":"tuning/#2-test-search-manually","title":"2. Test Search Manually","text":"<p>Before changing settings, run searches from the CLI to understand current behavior:</p> <pre><code># Search and see results\nhaiku-rag search \"your test query\" --limit 10\n\n# Try the QA to see end-to-end behavior\nhaiku-rag ask \"your question\"\n</code></pre>"},{"location":"tuning/#3-identify-the-bottleneck","title":"3. Identify the Bottleneck","text":"<ul> <li>Relevant chunks not retrieved: Try larger <code>search.limit</code>, smaller <code>chunk_size</code>, or a different embedding model</li> <li>Too many irrelevant chunks: Try reranking or larger <code>chunk_size</code></li> <li>Chunks found but answers wrong: Try <code>context_radius</code> expansion or a better QA model</li> </ul>"},{"location":"tuning/#4-test-one-change-at-a-time","title":"4. Test One Change at a Time","text":"<pre><code># After changing chunk_size, rebuild is required\nhaiku-rag rebuild\n\n# After changing search settings, no rebuild needed - just test again\nhaiku-rag search \"your test query\"\n</code></pre>"},{"location":"tuning/#5-build-dataset-specific-evaluations","title":"5. Build Dataset-Specific Evaluations","text":"<p>For systematic tuning, create evaluations specific to your corpus. See the <code>evaluations/</code> directory in the repository for examples of how to:</p> <ul> <li>Define test cases with questions and expected answers</li> <li>Run retrieval benchmarks (MRR, MAP)</li> <li>Run QA accuracy benchmarks with LLM judges</li> </ul> <p>Custom evaluations let you measure the impact of configuration changes objectively rather than relying on intuition.</p>"},{"location":"tuning/#6-consider-your-corpus","title":"6. Consider Your Corpus","text":"Corpus Type Suggested Starting Point Technical docs <code>chunk_size: 256</code>, <code>limit: 10</code>, <code>context_radius: 1</code> Legal/contracts <code>chunk_size: 512</code>, <code>limit: 5</code>, <code>context_radius: 2</code> News articles <code>chunk_size: 512</code>, <code>limit: 5</code>, <code>context_radius: 0</code> Scientific papers <code>chunk_size: 256</code>, <code>limit: 5</code>, reranking enabled FAQs <code>chunk_size: 128</code>, <code>limit: 5</code>, <code>context_radius: 0</code> Code repos <code>chunk_size: 256</code>, <code>limit: 10</code>, <code>context_radius: 1</code>"},{"location":"tuning/#common-issues","title":"Common Issues","text":""},{"location":"tuning/#relevant-content-not-being-retrieved","title":"\"Relevant content not being retrieved\"","text":"<ol> <li>Check chunk boundaries - is the content split awkwardly?</li> <li>Try smaller chunks for more granular matching</li> <li>Increase <code>search.limit</code></li> <li>Consider a different embedding model</li> </ol>"},{"location":"tuning/#retrieved-chunks-lack-context","title":"\"Retrieved chunks lack context\"","text":"<ol> <li>Increase <code>context_radius</code> for text content</li> <li>Increase <code>chunk_size</code> for more context per chunk</li> <li>Structural content (tables, code) expands automatically</li> </ol>"},{"location":"tuning/#search-is-slow","title":"\"Search is slow\"","text":"<ol> <li>Create a vector index: <code>haiku-rag create-index</code></li> <li>Reduce <code>search.limit</code></li> <li>Consider a smaller embedding model</li> </ol>"},{"location":"tuning/#qa-answers-are-wrong-despite-good-retrieval","title":"\"QA answers are wrong despite good retrieval\"","text":"<ol> <li>Check if chunks are being truncated by LLM context limits</li> <li>Try a more capable QA model</li> <li>Reduce number of chunks or expansion to fit context window</li> </ol>"},{"location":"tuning/#example-configurations","title":"Example Configurations","text":""},{"location":"tuning/#high-precision-technical-documentation","title":"High-Precision Technical Documentation","text":"<pre><code>processing:\n  chunk_size: 256\n  chunker_type: hybrid\n\nsearch:\n  limit: 10\n  context_radius: 1\n  max_context_items: 15\n\nreranking:\n  model:\n    provider: mxbai\n    name: mixedbread-ai/mxbai-rerank-base-v2\n</code></pre>"},{"location":"tuning/#long-form-content-articles-reports","title":"Long-Form Content (Articles, Reports)","text":"<pre><code>processing:\n  chunk_size: 512\n  chunker_type: hybrid\n\nsearch:\n  limit: 5\n  context_radius: 2\n  max_context_items: 10\n</code></pre>"},{"location":"tuning/#faqknowledge-base","title":"FAQ/Knowledge Base","text":"<pre><code>processing:\n  chunk_size: 128\n  chunker_type: hybrid\n\nsearch:\n  limit: 5\n  context_radius: 0\n</code></pre>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This tutorial provides quickstart instructions for getting familiar with <code>haiku.rag</code>. This tutorial is intended for people who are familiar with command line and Python, but not different AI ecosystem tools.</p> <p>The tutorial covers:</p> <ul> <li>RAG and embeddings basics</li> <li>Installing <code>haiku.rag</code> Python package</li> <li>Configuring <code>haiku.rag</code> with YAML</li> <li>Adding and retrieving items</li> <li>Inspecting the database</li> </ul> <p>The tutorial uses OpenAI API service - no local installation needed and will work on computers with any amount of RAM and GPU. The OpenAI API is pay-as-you-go, so you need to top it up with at least ~$5 when creating the API key.</p>"},{"location":"tutorial/#introduction","title":"Introduction","text":"<p>Retrieval-Augmented Generation (RAG) lets you give AI models access to your own documents and data. Instead of relying solely on the model's training data, RAG finds relevant information from your documents and includes it in the AI's responses.</p> <p><code>haiku.rag</code> handles the mechanics: it converts your documents into searchable embeddings, stores them locally, and retrieves relevant chunks when you ask questions. You provide the documents and questions, and it coordinates between the embedding service (like OpenAI) and the AI model to give you accurate, grounded answers.</p>"},{"location":"tutorial/#setup","title":"Setup","text":"<p>First, get an OpenAI API key.</p> <p>Install <code>haiku.rag</code> Python package using uv or your favourite Python package manager:</p> <pre><code># Python 3.12+ needed\nuv pip install haiku.rag\n</code></pre> <p>Configure haiku.rag to use OpenAI. Create a <code>haiku.rag.yaml</code> file:</p> <pre><code>embeddings:\n  model:\n    provider: openai\n    name: text-embedding-3-small  # or text-embedding-3-large\n    vector_dim: 1536\n\nqa:\n  model:\n    provider: openai\n    name: gpt-4o-mini  # or gpt-4o, gpt-4, etc.\n</code></pre> <p>Set your OpenAI API key as an environment variable (API keys should not be stored in the YAML file):</p> <pre><code>export OPENAI_API_KEY=\"&lt;your OpenAI API key&gt;\"\n</code></pre> <p>For the list of available OpenAI models and their vector dimensions, see the OpenAI documentation.</p> <p>See Configuration for all available options.</p>"},{"location":"tutorial/#initialize-the-database","title":"Initialize the database","text":"<p>Before adding documents, initialize the database:</p> <pre><code>haiku-rag init\n</code></pre> <p>This creates an empty database with the configured settings.</p>"},{"location":"tutorial/#adding-the-first-documents","title":"Adding the first documents","text":"<p>Now you can add some pieces of text in the database:</p> <pre><code>haiku-rag add \"Python is the best programming language in the world, because it is flexible, with robust ecosystem, open source licensing and thousands of contributors\"\nhaiku-rag add \"JavaScript is a popular programming language, but has a lot of warts\"\nhaiku-rag add \"PHP is a bad programming language, because of spotted security history, horrible syntax and declining popularity\"\n</code></pre> <p>What will happen:</p> <ul> <li>The piece of text is sent to OpenAI <code>/embeddings</code> API service</li> <li>OpenAI translates the free form text to RAG embedding vectors needed for the retrieval</li> <li>The vector values will be stored in a local database</li> </ul> <p>Now you can view your LanceDB database, and the embeddings it is configured for:</p> <pre><code>haiku-rag info\n</code></pre> <p>You should see output similar to:</p> <pre><code>haiku.rag database info\n  path: /Users/moo/Library/Application Support/haiku.rag/haiku.rag.lancedb\n  haiku.rag version (db): x.y.z\n  embeddings: openai/text-embedding-3-small (dim: 1536)\n  documents: 3 (storage: 48.0 KB)\n  chunks: 3 (storage: 52.0 KB)\n  vector index: not created\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nVersions\n  haiku.rag: x.y.z\n  lancedb: ...\n  docling: ...\n</code></pre>"},{"location":"tutorial/#asking-questions-and-retrieving-information","title":"Asking questions and retrieving information","text":"<p>Now we can use OpenAI LLMs to retrieve information from our embeddings database.</p> <p>In this example, we connect to a remote OpenAI API.</p> <p>Behind the scenes pydantic-ai query is created using <code>OpenAIChatModel.request()</code>.</p> <p>The easiest way to do this is <code>ask</code> CLI command:</p> <pre><code>haiku-rag ask \"What is the best programming language in the world\"\n</code></pre> <pre><code>Question: What is the best programming language in the world\n\nAnswer:\nAccording to the document, Python is considered the best programming language in the world due to its flexibility, robust ecosystem, open-source licensing, and thousands of contributors.\n</code></pre>"},{"location":"tutorial/#programmatic-interaction-in-python","title":"Programmatic interaction in Python","text":"<p>You can interact with haiku.rag from Python. Since the API is async, we'll use IPython which supports async/await directly.</p> <pre><code>uv pip install ipython\nipython\n</code></pre> <p>Then run:</p> <pre><code>from haiku.rag.client import HaikuRAG\n\n# Uses database from default location (must be initialized first)\nasync with HaikuRAG() as client:\n    answer, citations = await client.ask(\"What is the best programming language in the world?\")\n    print(answer)\n</code></pre> <p>You should see:</p> <pre><code>According to the document, Python is considered the best programming language in the world due to its flexibility, robust ecosystem, open-source licensing, and support from thousands of contributors.\n</code></pre>"},{"location":"tutorial/#complex-documents","title":"Complex documents","text":"<p>Haiku RAG can also handle types beyond plain text, including PDF, DOCX, HTML, and 40+ other file formats.</p> <p>Here we add research papers about Python from arxiv using URL retriever.</p> <pre><code># Better Python Programming for all: With the focus on Maintainability\nhaiku-rag add-src --meta collection=\"Interesting Python papers\" \"https://arxiv.org/pdf/2408.09134\"\n\n# Interoperability From OpenTelemetry to Kieker: Demonstrated as Export from the Astronomy Shop\nhaiku-rag add-src --meta collection=\"Interesting Python papers\" \"https://arxiv.org/pdf/2510.11179\"\n</code></pre> <p>Then we can query this:</p> <pre><code>haiku-rag ask \"Who wrote a paper about OpenTelemetry interoperability, and what was his take\"\n</code></pre> <p>We should get something along the lines:</p> <pre><code>Answer:\nDavid Georg Reichelt from Lancaster University wrote a paper titled \"Interoperability From OpenTelemetry to Kieker: Demonstrated as Export from the Astronomy Shop.\" In his work, he indicates that there is a structural difference between Kieker\u2019s synchronous traces and OpenTelemetry\u2019s asynchronous traces, leading to  limited compatibility between the two systems. This highlights the challenges of interoperability in observability frameworks.\n</code></pre> <p>We can also add offline files, like PDFs. Here we add a local file to ensure OpenAI does not cheat - a file we know that should not be very well known in Internet:</p> <pre><code># This static file is supplied in haiku.rag repo\nhaiku-rag add-src \"examples/samples/PyCon Finland 2025 Schedule.html\"\n</code></pre> <p>And then:</p> <pre><code>haiku-rag ask \"Who were presenting talks in Pycon Finland 2025? Can you give at least five different people.\"\n</code></pre> <pre><code>The following people are presenting talks at PyCon Finland 2025:\n\n 1 Jeremy Mayeres - Talk: The Limits of Imagination: An Open Source Journey\n 2 Aroma Rodrigues - Talk: Python and Rust, a Perfect Pairing\n 3 Andreas Jung - Talk: Guillotina Volto: A New Backend for Volto\n 4 Daniel Vahla - Talk: Experiences with AI in Software Projects\n 5 Andreas Jung (also presenting another talk) - Talk: Debugging Python\n</code></pre>"},{"location":"tutorial/#next-steps","title":"Next Steps","text":"<ul> <li>Chat - Interactive conversations with <code>haiku-rag chat</code></li> <li>CLI Reference - All available commands and options</li> <li>Python API - Use haiku.rag in your Python applications</li> <li>Agents - Deep QA and multi-agent research workflows</li> <li>Configuration - Complete YAML configuration reference</li> <li>Server Mode - File monitoring and MCP server</li> </ul>"},{"location":"agents/","title":"Agents","text":"<p>Three agentic flows are provided by haiku.rag:</p> <ul> <li>Simple QA Agent \u2014 a focused question answering agent</li> <li>Research Graph \u2014 a multi-step research workflow with question decomposition</li> <li>RLM Agent \u2014 complex analytical tasks via sandboxed Python code execution (see RLM Agent)</li> </ul> <p>For multi-turn conversational RAG, haiku.rag provides skills built on haiku.skills. The skills bundle search, Q&amp;A, analysis, and research tools with session state management.</p> <p>See QA and Research Configuration for configuring model, iterations, concurrency, and other settings.</p>"},{"location":"agents/#simple-qa-agent","title":"Simple QA Agent","text":"<p>The simple QA agent answers a single question using the knowledge base. It retrieves relevant chunks, optionally expands context around them, and asks the model to answer strictly based on that context.</p> <p>Key points:</p> <ul> <li>Uses a single <code>search_documents</code> tool to fetch relevant chunks</li> <li>Can be run with or without inline citations in the prompt</li> <li>Returns a plain string answer</li> </ul> <p>CLI usage:</p> <pre><code>haiku-rag ask \"What is climate change?\"\n\n# With citations\nhaiku-rag ask \"What is climate change?\" --cite\n</code></pre> <p>Python usage:</p> <pre><code>from haiku.rag.client import HaikuRAG\nfrom haiku.rag.config.models import ModelConfig\nfrom haiku.rag.agents.qa.agent import QuestionAnswerAgent\n\nasync with HaikuRAG(path_to_db) as client:\n    agent = QuestionAnswerAgent(\n        client=client,\n        model_config=ModelConfig(provider=\"openai\", name=\"gpt-4o-mini\"),\n    )\n\n    answer, citations = await agent.answer(\"What is climate change?\")\n    print(answer)\n</code></pre>"},{"location":"agents/#research-graph","title":"Research Graph","text":"<p>The research workflow is implemented as a typed pydantic-graph. It uses an iterative feedback loop where the planner proposes one question at a time, sees the answer, then decides whether to continue or synthesize.</p> <pre><code>---\ntitle: Research graph\n---\nstateDiagram-v2\n  state plan_next_decision &lt;&lt;choice&gt;&gt;\n  [*] --&gt; plan_next\n  plan_next --&gt; plan_next_decision\n  plan_next_decision --&gt; search_one: Has next question\n  plan_next_decision --&gt; synthesize: Complete or max iterations\n  search_one --&gt; plan_next: Answer added to context\n  synthesize --&gt; [*]\n\n  note right of plan_next\n    Uses prior_answers from previous iterations.\n    Uses a different prompt when prior answers exist.\n  end note</code></pre> <p>The graph receives a <code>ResearchContext</code> containing:</p> <ul> <li><code>original_question</code> \u2014 the user's question</li> <li><code>qa_responses</code> \u2014 prior answers from previous iterations (injected as <code>&lt;prior_answers&gt;</code> XML)</li> </ul> <p>When prior answers are provided, the planner uses a context-aware prompt that evaluates whether existing evidence is sufficient. If it is, the planner marks <code>is_complete=True</code> and the graph skips directly to synthesis without any searches.</p> <p>Key nodes:</p> <ul> <li>plan_next: Evaluates gathered evidence and either proposes the next question to investigate or marks research as complete. Uses a context-aware prompt when prior answers exist, allowing it to skip research entirely.</li> <li>search_one: Answers a single question using the knowledge base (up to 3 search calls per question). Each answer is added to <code>ResearchContext.qa_responses</code> for the next planning iteration.</li> <li>synthesize: Generates the final output from all gathered evidence.</li> </ul> <p>Iterative flow:</p> <ul> <li>Each iteration: planner evaluates context \u2192 proposes one question \u2192 search answers it \u2192 loop back</li> <li>Planner can decompose complex questions (e.g., \"benefits and drawbacks\" \u2192 start with \"benefits\")</li> <li>Prior answers let the planner skip redundant searches</li> <li>Loop terminates when planner marks <code>is_complete=True</code> or <code>max_iterations</code> is reached</li> </ul>"},{"location":"agents/#cli-usage","title":"CLI Usage","text":"<pre><code># Basic usage\nhaiku-rag research \"How does haiku.rag organize and query documents?\"\n\n# With document filter\nhaiku-rag research \"What are the key findings?\" --filter \"uri LIKE '%report%'\"\n</code></pre>"},{"location":"agents/#python-usage","title":"Python Usage","text":"<p>Basic example:</p> <pre><code>from haiku.rag.client import HaikuRAG\nfrom haiku.rag.config import Config\nfrom haiku.rag.agents.research.dependencies import ResearchContext\nfrom haiku.rag.agents.research.graph import build_research_graph\nfrom haiku.rag.agents.research.state import ResearchDeps, ResearchState\n\nasync with HaikuRAG(path_to_db) as client:\n    graph = build_research_graph(config=Config)\n    context = ResearchContext(original_question=\"What are the main features?\")\n    state = ResearchState.from_config(context=context, config=Config)\n    deps = ResearchDeps(client=client)\n\n    report = await graph.run(state=state, deps=deps)\n\n    print(report.title)\n    print(report.executive_summary)\n</code></pre> <p>With custom config:</p> <pre><code>from haiku.rag.client import HaikuRAG\nfrom haiku.rag.config.models import AppConfig, ModelConfig, ResearchConfig\nfrom haiku.rag.agents.research.dependencies import ResearchContext\nfrom haiku.rag.agents.research.graph import build_research_graph\nfrom haiku.rag.agents.research.state import ResearchDeps, ResearchState\n\ncustom_config = AppConfig(\n    research=ResearchConfig(\n        model=ModelConfig(provider=\"openai\", name=\"gpt-4o-mini\"),\n        max_iterations=5,\n        max_concurrency=3,\n    )\n)\n\nasync with HaikuRAG(path_to_db) as client:\n    graph = build_research_graph(config=custom_config)\n    context = ResearchContext(original_question=\"What are the main features?\")\n    state = ResearchState.from_config(context=context, config=custom_config)\n    deps = ResearchDeps(client=client)\n\n    report = await graph.run(state=state, deps=deps)\n</code></pre>"},{"location":"agents/#filtering-documents","title":"Filtering Documents","text":"<p>Restrict searches to specific documents via the <code>search_filter</code> parameter:</p> <pre><code># Set filter before running the graph\nstate = ResearchState.from_config(context=context, config=Config)\nstate.search_filter = \"id IN ('doc-123', 'doc-456')\"\n\nreport = await graph.run(state=state, deps=deps)\n</code></pre> <p>The filter applies to all search operations in the graph. See Filtering Search Results for available filter columns and syntax.</p>"},{"location":"agents/rlm/","title":"RLM Agent (Recursive Language Model)","text":"<p>The RLM agent enables complex analytical tasks by writing and executing Python code in a sandboxed environment. It solves problems that traditional RAG struggles with:</p> <ul> <li>Aggregation: \"How many documents mention security vulnerabilities?\"</li> <li>Computation: \"What's the average revenue across all quarterly reports?\"</li> <li>Multi-document analysis: \"Compare the key findings between Report A and Report B\"</li> <li>Structured data extraction: \"Extract all dollar amounts and compute totals\"</li> </ul>"},{"location":"agents/rlm/#how-it-works","title":"How It Works","text":"<ol> <li>The agent receives a question</li> <li>It writes Python code to explore the knowledge base</li> <li>Code executes in a sandboxed Python interpreter with access to knowledge base functions</li> <li>The agent iterates: run code, examine results, refine approach</li> <li>Final answer is synthesized from the gathered data</li> </ol>"},{"location":"agents/rlm/#cli-usage","title":"CLI Usage","text":"<pre><code># Basic usage\nhaiku-rag rlm \"How many documents are in the database?\"\n\n# With document filter (restricts what the agent can access)\nhaiku-rag rlm \"Summarize the key points\" --filter \"uri LIKE '%report%'\"\n\n# Pre-load specific documents\nhaiku-rag rlm \"Compare these two reports\" --document \"Q1 Report\" --document \"Q2 Report\"\n</code></pre>"},{"location":"agents/rlm/#python-usage","title":"Python Usage","text":"<pre><code>from haiku.rag.client import HaikuRAG\n\nasync with HaikuRAG(path_to_db) as client:\n    # Basic question\n    result = await client.rlm(\"How many documents mention 'security'?\")\n    print(result.answer)    # The answer\n    print(result.program)   # The final consolidated program\n\n    # With filter (agent can only see filtered documents)\n    result = await client.rlm(\n        \"What is the total revenue?\",\n        filter=\"title LIKE '%Financial%'\"\n    )\n\n    # Pre-load specific documents\n    result = await client.rlm(\n        \"Compare the conclusions\",\n        documents=[\"Report A\", \"Report B\"]\n    )\n</code></pre>"},{"location":"agents/rlm/#sandbox-capabilities","title":"Sandbox Capabilities","text":"<p>The agent's code runs in a sandboxed Python interpreter (pydantic-monty) with access to these knowledge base functions:</p> Function Description <code>search(query, limit)</code> Hybrid search (vector + full-text) returning matching chunks with scores <code>list_documents(limit, offset)</code> List documents in the knowledge base <code>get_document(id_or_title)</code> Get full text content of a document <code>get_chunk(chunk_id)</code> Get a chunk with metadata (headings, page numbers, labels) for citations <code>get_docling_document(document_id)</code> Get the full DoclingDocument structure as a dict (texts, tables, pictures, pages) <code>llm(prompt)</code> Call an LLM for classification, summarization, or extraction <code>regex_findall(pattern, text)</code>, <code>regex_sub(pattern, repl, text)</code>, <code>regex_search(pattern, text)</code>, <code>regex_split(pattern, text)</code> Regular expression matching via Python's <code>re</code> module <p>When documents are pre-loaded via the <code>documents</code> parameter, they are injected as a <code>documents</code> variable accessible in the sandbox code.</p>"},{"location":"agents/rlm/#python-features","title":"Python Features","text":"<p>The interpreter supports a subset of Python: variables, arithmetic, strings, f-strings, lists, dicts, tuples, sets, loops, conditionals, comprehensions, functions, async/await, try/except, and the <code>json</code> module.</p> <p>Not supported: imports (other than <code>json</code>), class definitions, generators/yield, match statements, decorators, <code>with</code> statements. For pattern matching, the agent can use the <code>regex_*</code> functions, string methods, or the <code>llm()</code> function.</p>"},{"location":"agents/rlm/#security","title":"Security","text":"<p>Code executes in an isolated interpreter with:</p> <ul> <li>No filesystem access: Code cannot read or write files</li> <li>No network access: Code cannot make HTTP requests or open sockets</li> <li>No imports: Only the <code>json</code> module is available</li> <li>Execution timeout: Configurable limit (default 60s)</li> <li>Output truncation: Large outputs are truncated to prevent memory issues</li> </ul>"},{"location":"agents/rlm/#context-filter","title":"Context Filter","text":"<p>The <code>filter</code> parameter restricts what documents the agent can access. Unlike tool parameters, the filter is applied automatically and cannot be bypassed by the LLM:</p> <pre><code># Agent can only see documents with \"confidential\" in the URI\nresult = await client.rlm(\n    \"Summarize all findings\",\n    filter=\"uri LIKE '%confidential%'\"\n)\n</code></pre> <p>This is useful for scoping to specific document sets, enforcing access control, or limiting context for focused analysis.</p>"},{"location":"agents/rlm/#configuration","title":"Configuration","text":"<p>RLM settings can be configured in <code>haiku.rag.yaml</code>:</p> <pre><code>rlm:\n  model:\n    provider: anthropic\n    name: claude-sonnet-4-20250514\n  code_timeout: 60.0      # Max seconds for code execution\n  max_output_chars: 50000 # Truncate output after this many chars\n</code></pre>"},{"location":"configuration/","title":"Configuration","text":"<p>Configuration is done through YAML configuration files.</p> <p>Note</p> <p>If you create a db with certain settings and later change them, <code>haiku.rag</code> will detect incompatibilities (for example, if you change embedding provider) and will exit. You can rebuild the database to apply the new settings, see Rebuild Database.</p>"},{"location":"configuration/#getting-started","title":"Getting Started","text":"<p>Generate a configuration file with defaults:</p> <pre><code>haiku-rag init-config\n</code></pre> <p>This creates a <code>haiku.rag.yaml</code> file in your current directory with all available settings.</p>"},{"location":"configuration/#configuration-file-locations","title":"Configuration File Locations","text":"<p><code>haiku.rag</code> searches for configuration files in this order:</p> <ol> <li>Path specified via <code>--config</code> flag: <code>haiku-rag --config /path/to/config.yaml &lt;command&gt;</code></li> <li><code>./haiku.rag.yaml</code> (current directory)</li> <li>Platform-specific user directory:<ul> <li>Linux: <code>~/.local/share/haiku.rag/haiku.rag.yaml</code></li> <li>macOS: <code>~/Library/Application Support/haiku.rag/haiku.rag.yaml</code></li> <li>Windows: <code>C:/Users/&lt;USER&gt;/AppData/Roaming/haiku.rag/haiku.rag.yaml</code></li> </ul> </li> </ol>"},{"location":"configuration/#minimal-configuration","title":"Minimal Configuration","text":"<p>A minimal configuration file with defaults:</p> <pre><code># haiku.rag.yaml\nenvironment: production\n\nembeddings:\n  model:\n    provider: ollama\n    name: qwen3-embedding:4b\n    vector_dim: 2560\n\nqa:\n  model:\n    provider: ollama\n    name: gpt-oss\n    enable_thinking: false\n</code></pre>"},{"location":"configuration/#complete-configuration-example","title":"Complete Configuration Example","text":"<pre><code># haiku.rag.yaml\nenvironment: production\n\nstorage:\n  data_dir: \"\"  # Empty = use default platform location\n  vacuum_retention_seconds: 86400\n\nmonitor:\n  directories:\n    - /path/to/documents\n    - /another/path\n  ignore_patterns: []  # Gitignore-style patterns to exclude\n  include_patterns: []  # Gitignore-style patterns to include\n\nlancedb:\n  uri: \"\"  # Empty for local, or db://, s3://, az://, gs://\n  api_key: \"\"\n  region: \"\"\n\nembeddings:\n  model:\n    provider: ollama\n    name: qwen3-embedding:4b\n    vector_dim: 2560\n\nreranking:\n  model:\n    provider: \"\"  # Empty to disable, or mxbai, cohere, zeroentropy, vllm\n    name: \"\"\n\nqa:\n  model:\n    provider: ollama\n    name: gpt-oss\n    enable_thinking: false\n  max_iterations: 2\n  max_concurrency: 1\n\nresearch:\n  model:\n    provider: \"\"  # Empty to use qa settings\n    name: \"\"\n    enable_thinking: false\n  max_iterations: 3\n  max_concurrency: 1\n\nsearch:\n  limit: 5                     # Default number of results to return\n  context_radius: 0            # DocItems before/after to include for text content\n  max_context_items: 10        # Maximum items in expanded context\n  max_context_chars: 10000     # Maximum characters in expanded context\n  vector_index_metric: cosine  # cosine, l2, or dot\n  vector_refine_factor: 30\n\nprompts:\n  domain_preamble: \"\"  # Prepended to all agent prompts\n  qa: null             # Custom QA agent prompt (null = use default)\n  synthesis: null      # Custom research synthesis prompt (null = use default)\n\nprocessing:\n  converter: docling-local  # docling-local or docling-serve\n  chunker: docling-local    # docling-local or docling-serve\n  chunker_type: hybrid      # hybrid or hierarchical\n  chunk_size: 256\n  chunking_tokenizer: \"Qwen/Qwen3-Embedding-0.6B\"\n  chunking_merge_peers: true\n  chunking_use_markdown_tables: false\n  auto_title: false              # Auto-generate titles on ingestion\n  title_model:\n    provider: ollama\n    name: gpt-oss\n    enable_thinking: false\n  conversion_options:\n    do_ocr: true\n    force_ocr: false\n    ocr_lang: []\n    do_table_structure: true\n    table_mode: accurate\n    table_cell_matching: true\n    images_scale: 2.0\n\nproviders:\n  ollama:\n    base_url: http://localhost:11434\n\n  docling_serve:\n    base_url: http://localhost:5001\n    api_key: \"\"\n    timeout: 300\n</code></pre>"},{"location":"configuration/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>When using haiku.rag as a Python library, you can pass configuration directly to the <code>HaikuRAG</code> client:</p> <pre><code>from haiku.rag.config import AppConfig\nfrom haiku.rag.config.models import EmbeddingModelConfig, ModelConfig, QAConfig, EmbeddingsConfig\nfrom haiku.rag.client import HaikuRAG\n\n# Create custom configuration\ncustom_config = AppConfig(\n    qa=QAConfig(\n        model=ModelConfig(\n            provider=\"openai\",\n            name=\"gpt-4o\",\n            temperature=0.7\n        )\n    ),\n    embeddings=EmbeddingsConfig(\n        model=EmbeddingModelConfig(\n            provider=\"ollama\",\n            name=\"qwen3-embedding:4b\",\n            vector_dim=2560\n        )\n    ),\n    processing={\"chunk_size\": 512}\n)\n\n# Pass configuration to the client\nclient = HaikuRAG(config=custom_config)\n</code></pre> <p>If you don't pass a config, the client uses the global configuration loaded from your YAML file or defaults.</p> <p>This is useful for: - Jupyter notebooks - Python scripts - Testing with different configurations - Applications that need multiple clients with different configurations</p>"},{"location":"configuration/#configuration-topics","title":"Configuration Topics","text":"<p>For detailed configuration of specific topics, see:</p> <ul> <li>Providers - Model settings and provider-specific configuration (embeddings, reranking)</li> <li>Search and Question Answering - Search settings, question answering, and research workflows</li> <li>Document Processing - Document conversion, chunking, and file monitoring</li> <li>Storage - Database, remote storage, and vector indexing</li> <li>Prompts - Customize agent prompts for your domain</li> </ul>"},{"location":"configuration/processing/","title":"Document Processing &amp; Monitoring","text":"<p>This guide covers how haiku.rag converts, chunks, and monitors documents.</p>"},{"location":"configuration/processing/#document-processing","title":"Document Processing","text":"<p>Configure how documents are converted and chunked:</p> <pre><code>processing:\n  # Chunking configuration\n  chunk_size: 256                            # Maximum tokens per chunk\n\n  # Converter selection\n  converter: docling-local                   # docling-local or docling-serve\n\n  # Chunker selection and configuration\n  chunker: docling-local                     # docling-local or docling-serve\n  chunker_type: hybrid                       # hybrid or hierarchical\n  chunking_tokenizer: \"Qwen/Qwen3-Embedding-0.6B\"  # HuggingFace model for tokenization\n  chunking_merge_peers: true                 # Merge undersized successive chunks\n  chunking_use_markdown_tables: false        # Use markdown tables vs narrative format\n\n  # Automatic title generation\n  auto_title: false                          # Auto-generate titles on ingestion\n  title_model:                               # LLM for title generation (fallback)\n    provider: ollama\n    name: gpt-oss\n    enable_thinking: false\n\n  # Conversion options (works with both local and remote converters)\n  conversion_options:\n    # OCR settings\n    do_ocr: true                             # Enable OCR for bitmap content\n    force_ocr: false                         # Replace existing text with OCR\n    ocr_engine: auto                         # OCR engine: auto, easyocr, rapidocr, tesseract, tesserocr, ocrmac\n    ocr_lang: []                             # OCR languages (e.g., [\"en\", \"fr\", \"de\"])\n\n    # Table extraction\n    do_table_structure: true                 # Extract table structure\n    table_mode: accurate                     # fast or accurate\n    table_cell_matching: true                # Match table cells back to PDF cells\n\n    # Image settings\n    images_scale: 2.0                        # Image scale factor\n    generate_page_images: true               # Include rendered page images (for visualize_chunk)\n    generate_picture_images: false           # Include embedded figure/diagram images\n\n    # VLM picture description (optional)\n    picture_description:\n      enabled: false                         # Enable VLM image descriptions\n      model:\n        provider: ollama\n        name: ministral-3\n</code></pre>"},{"location":"configuration/processing/#conversion-options","title":"Conversion Options","text":"<p>The <code>conversion_options</code> section allows fine-grained control over document conversion. These options work with both <code>docling-local</code> and <code>docling-serve</code> converters.</p>"},{"location":"configuration/processing/#ocr-settings","title":"OCR Settings","text":"<pre><code>conversion_options:\n  do_ocr: true          # Enable OCR for bitmap/scanned content\n  force_ocr: false      # Replace all text with OCR output\n  ocr_engine: auto      # OCR engine selection\n  ocr_lang: []          # List of OCR languages, e.g., [\"en\", \"fr\", \"de\"]\n</code></pre> <ul> <li>do_ocr: When <code>true</code>, applies OCR to images and scanned pages. Disable for faster processing if documents contain only native text.</li> <li>force_ocr: When <code>true</code>, replaces existing text layers with OCR output. Useful for documents with poor text extraction.</li> <li>ocr_engine: Select the OCR engine to use. Options:</li> <li><code>auto</code> (default): Automatically select the best available engine</li> <li><code>easyocr</code>: EasyOCR - supports many languages, good accuracy</li> <li><code>rapidocr</code>: RapidOCR - fast processing</li> <li><code>tesseract</code>: Tesseract OCR</li> <li><code>tesserocr</code>: Tesseract via tesserocr Python binding</li> <li><code>ocrmac</code>: macOS native OCR (macOS only)</li> <li>ocr_lang: List of language codes for OCR. Empty list uses default language detection. Examples: <code>[\"en\"]</code>, <code>[\"en\", \"fr\", \"de\"]</code>.</li> </ul>"},{"location":"configuration/processing/#table-extraction","title":"Table Extraction","text":"<pre><code>conversion_options:\n  do_table_structure: true    # Extract structured table data\n  table_mode: accurate        # fast or accurate\n  table_cell_matching: true   # Match cells back to PDF\n</code></pre> <ul> <li>do_table_structure: When <code>true</code>, extracts table structure. Disable for faster processing if tables aren't important.</li> <li>table_mode:</li> <li><code>accurate</code>: Better table structure recognition (slower)</li> <li><code>fast</code>: Faster processing with simpler table detection</li> <li>table_cell_matching: When <code>true</code>, matches detected table cells back to PDF cells. Disable if tables have merged cells across columns.</li> </ul>"},{"location":"configuration/processing/#image-settings","title":"Image Settings","text":"<pre><code>conversion_options:\n  images_scale: 2.0               # Image resolution scale factor\n  generate_page_images: true      # Include rendered page images\n  generate_picture_images: false  # Include embedded figure/diagram images\n</code></pre> <ul> <li>images_scale: Scale factor for extracted images. Higher values = better quality but larger size. Typical range: 1.0-3.0.</li> <li>generate_page_images: When <code>true</code> (default), rendered images of each PDF page are included in the document. Required for <code>visualize_chunk()</code> to show visual grounding. When <code>false</code>, page images are excluded to reduce document size.</li> <li>generate_picture_images: When <code>true</code>, embedded images (figures, diagrams) are included as base64-encoded data in the document. When <code>false</code> (default), images are excluded to reduce chunk size and avoid context bloat.</li> </ul> <p>Note: With <code>docling-serve</code>, <code>generate_picture_images</code> has limited support - picture image data may not be returned in the JSON response. Page images work correctly with both local and remote converters.</p>"},{"location":"configuration/processing/#picture-description-vlm","title":"Picture Description (VLM)","text":"<p>Use a Vision Language Model (VLM) to automatically describe images in documents. Descriptions become searchable text, improving RAG retrieval for visual content.</p> <pre><code>conversion_options:\n  picture_description:\n    enabled: true                  # Enable VLM picture description\n    model:\n      provider: ollama             # ollama, openai, or custom\n      name: ministral-3            # VLM model name\n    timeout: 90                    # Request timeout in seconds\n    max_tokens: 200                # Maximum tokens in response\n</code></pre> <p>Configuration options:</p> <ul> <li>enabled: When <code>true</code>, each embedded image is sent to a VLM for description. Requires <code>generate_picture_images</code> to be <code>true</code> (automatically enabled).</li> <li>model: Standard model configuration</li> <li><code>provider</code>: <code>ollama</code> (default), <code>openai</code>, or use <code>base_url</code> for custom endpoints</li> <li><code>name</code>: Model name (e.g., <code>ministral-3</code>, <code>granite3.2-vision</code>, <code>gpt-4-vision</code>)</li> <li><code>base_url</code>: Optional custom API endpoint for vLLM, LM Studio, etc.</li> <li>timeout: Request timeout in seconds</li> <li>max_tokens: Maximum tokens in the VLM response</li> </ul> <p>Note: Requires an OpenAI-compatible <code>/v1/chat/completions</code> endpoint. Providers with different API formats (e.g., Anthropic Claude) are not supported.</p> <p>Default prompt (configured in <code>prompts.picture_description</code>):</p> <pre><code>Describe this image for a blind user. State the image type\n(screenshot, chart, photo, etc.), what it depicts, any visible text,\nand key visual details. Be concise and accurate.\n</code></pre> <p>To customize the prompt globally:</p> <pre><code>prompts:\n  picture_description: \"Your custom prompt here...\"\n</code></pre> <p>Using with Ollama:</p> <pre><code>conversion_options:\n  picture_description:\n    enabled: true\n    model:\n      provider: ollama\n      name: ministral-3\n</code></pre> <p>Requires Ollama running with a vision-capable model:</p> <pre><code>ollama pull ministral-3\nollama serve\n</code></pre> <p>Using with vLLM or custom endpoints:</p> <pre><code>conversion_options:\n  picture_description:\n    enabled: true\n    model:\n      provider: openai           # Use OpenAI-compatible API format\n      name: granite-vision\n      base_url: http://my-vllm-server:8000\n</code></pre> <p>How it works:</p> <ol> <li>During PDF conversion, docling extracts embedded images</li> <li>Each image is sent to the configured VLM for description</li> <li>Descriptions are added as annotations on the image</li> <li>When exported to markdown, descriptions appear as searchable text</li> </ol> <p>Using with docling-serve:</p> <p>When using <code>converter: docling-serve</code>, the VLM calls are made by the docling-serve instance, not by haiku.rag. You must:</p> <ol> <li>Set <code>DOCLING_SERVE_ENABLE_REMOTE_SERVICES=true</code> when running docling-serve</li> <li>Ensure the VLM endpoint is accessible from where docling-serve is running</li> </ol> <p>Docker networking: If docling-serve runs in Docker and your VLM runs on the host, use <code>host.docker.internal</code> instead of <code>localhost</code>:</p> <pre><code>picture_description:\n  enabled: true\n  model:\n    provider: ollama\n    name: ministral-3\n    base_url: http://host.docker.internal:11434  # NOT localhost!\n</code></pre> <p>See VLM Picture Description with docling-serve for a complete example.</p>"},{"location":"configuration/processing/#automatic-title-generation","title":"Automatic Title Generation","text":"<p>Enable automatic title generation during document ingestion:</p> <pre><code>processing:\n  auto_title: true\n  title_model:\n    provider: ollama\n    name: gpt-oss\n    enable_thinking: false\n</code></pre> <p>When <code>auto_title</code> is enabled, haiku.rag attempts to extract a title for each document during ingestion using a two-tier approach:</p> <ol> <li>Structural extraction (free, no model calls): Scans the DoclingDocument for semantic labels \u2014 HTML <code>&lt;title&gt;</code> tags, <code>&lt;h1&gt;</code> headings, PDF title blocks, and section headers</li> <li>LLM fallback: When no structural title is found (e.g., plain text), generates a title using the configured <code>title_model</code></li> </ol> <p>Priority order: HTML <code>&lt;title&gt;</code> (furniture layer) \u2192 h1/PDF title (body layer) \u2192 first section header \u2192 LLM generation.</p> <p>Explicit titles passed via <code>title=</code> parameter always take precedence and are never overridden. When updating documents, existing titles are preserved \u2014 auto-generation only applies to untitled documents.</p> <p>To generate titles for existing untitled documents, use <code>rebuild --title-only</code>.</p>"},{"location":"configuration/processing/#local-vs-remote-processing","title":"Local vs Remote Processing","text":"<p>Local processing (default):</p> <ul> <li>Uses <code>docling</code> library locally</li> <li>No external dependencies</li> <li>Good for development and small workloads</li> </ul> <p>Remote processing (docling-serve):</p> <ul> <li>Offloads processing to docling-serve API</li> <li>Better for heavy workloads and production</li> <li>Requires docling-serve instance (see Remote processing setup)</li> </ul> <p>To use remote processing:</p> <pre><code>processing:\n  converter: docling-serve\n  chunker: docling-serve\n\nproviders:\n  docling_serve:\n    base_url: http://localhost:5001\n    api_key: \"your-api-key\"  # Optional\n</code></pre> <p>Conversion options work identically for both local and remote processing.</p> <p>Note: When using <code>chunker: docling-serve</code>, OCR options (<code>do_ocr</code>, <code>force_ocr</code>, <code>ocr_engine</code>, <code>ocr_lang</code>) from <code>conversion_options</code> are passed to the chunking API. This is useful when running docling-serve in a read-only container where OCR model downloads fail\u2014set <code>do_ocr: false</code> to disable OCR entirely.</p>"},{"location":"configuration/processing/#chunking-strategies","title":"Chunking Strategies","text":"<p>Hybrid chunking (default): - Structure-aware chunking - Respects document boundaries - Best for most use cases</p> <p>Hierarchical chunking: - Creates hierarchical chunk structure - Preserves document hierarchy - Useful for complex documents</p>"},{"location":"configuration/processing/#table-serialization","title":"Table Serialization","text":"<p>Control how tables are represented in chunks:</p> <pre><code>processing:\n  chunking_use_markdown_tables: false  # Default: narrative format\n</code></pre> <ul> <li><code>false</code>: Tables as narrative text (\"Value A, Column 2 = Value B\")</li> <li><code>true</code>: Tables as markdown (preserves table structure)</li> </ul>"},{"location":"configuration/processing/#chunk-size","title":"Chunk Size","text":"<pre><code>processing:\n  chunk_size: 256  # Maximum tokens per chunk\n</code></pre> <p>Context expansion settings (for enriching search results with surrounding content) are configured in the <code>search</code> section. See Search Settings.</p>"},{"location":"configuration/processing/#file-monitoring","title":"File Monitoring","text":"<p>Set directories to monitor for automatic indexing:</p> <pre><code>monitor:\n  directories:\n    - /path/to/documents\n    - /another_path/to/documents\n</code></pre>"},{"location":"configuration/processing/#filtering-monitored-files","title":"Filtering Monitored Files","text":"<p>Use gitignore-style patterns to control which files are monitored:</p> <pre><code>monitor:\n  directories:\n    - /path/to/documents\n\n  # Exclude specific files or directories\n  ignore_patterns:\n    - \"*draft*\"         # Ignore files with \"draft\" in the name\n    - \"temp/\"           # Ignore temp directory\n    - \"**/archive/**\"   # Ignore all archive directories\n    - \"*.backup\"        # Ignore backup files\n\n  # Only include specific files (whitelist mode)\n  include_patterns:\n    - \"*.md\"            # Only markdown files\n    - \"*.pdf\"           # Only PDF files\n    - \"**/docs/**\"      # Only files in docs directories\n</code></pre> <p>How patterns work:</p> <ol> <li>Extension filtering - Only supported file types are considered</li> <li>Include patterns - If specified, only matching files are included (whitelist)</li> <li>Ignore patterns - Matching files are excluded (blacklist)</li> <li>Combining both - Include patterns are applied first, then ignore patterns</li> </ol> <p>Common patterns:</p> <pre><code># Only monitor markdown documentation, but ignore drafts\nmonitor:\n  include_patterns:\n    - \"*.md\"\n  ignore_patterns:\n    - \"*draft*\"\n    - \"*WIP*\"\n\n# Monitor all supported files except in specific directories\nmonitor:\n  ignore_patterns:\n    - \"node_modules/\"\n    - \".git/\"\n    - \"**/test/**\"\n    - \"**/temp/**\"\n</code></pre> <p>Patterns follow gitignore syntax:</p> <ul> <li><code>*</code> matches anything except <code>/</code></li> <li><code>**</code> matches zero or more directories</li> <li><code>?</code> matches any single character</li> <li><code>[abc]</code> matches any character in the set</li> </ul>"},{"location":"configuration/prompts/","title":"Prompt Customization","text":"<p>Customize the prompts used by haiku.rag's AI agents to better match your domain and use case.</p>"},{"location":"configuration/prompts/#configuration","title":"Configuration","text":"<pre><code>prompts:\n  # Prepended to all agent prompts\n  domain_preamble: |\n    You are answering questions about our internal documentation.\n    Technical terms like \"time travel\" refer to database versioning features.\n\n  # Full replacement for QA agent prompt (optional)\n  qa: null\n\n  # Full replacement for research synthesis prompt (optional)\n  synthesis: null\n\n  # VLM prompt for image description during conversion (optional)\n  picture_description: null  # Uses default prompt\n</code></pre>"},{"location":"configuration/prompts/#domain-preamble","title":"Domain Preamble","text":"<p>The <code>domain_preamble</code> field is prepended to all agent prompts (QA, research planning, search, evaluation, and synthesis). Use this to:</p> <ul> <li>Add domain context that clarifies terminology</li> <li>Set the tone or personality of responses</li> <li>Specify what the knowledge base contains</li> </ul> <p>Example:</p> <pre><code>prompts:\n  domain_preamble: |\n    You are a technical support assistant for Acme Corp products.\n    The knowledge base contains product documentation, FAQs, and troubleshooting guides.\n    Always be helpful and professional.\n</code></pre>"},{"location":"configuration/prompts/#custom-qa-prompt","title":"Custom QA Prompt","text":"<p>Replace the default QA agent prompt entirely by setting <code>prompts.qa</code>. The prompt should instruct the agent how to:</p> <ol> <li>Use the <code>search_documents</code> tool to find relevant content</li> <li>Interpret search results with scores and metadata</li> <li>Cite sources using chunk IDs</li> <li>Handle insufficient information</li> </ol> <p>Example:</p> <pre><code>prompts:\n  qa: |\n    You are a concise technical assistant. Answer questions using only the knowledge base.\n\n    Process:\n    1. Search for relevant documents using the search_documents tool\n    2. Review results ordered by relevance (rank 1 = most relevant)\n    3. Provide a brief, direct answer based on retrieved content\n\n    Guidelines:\n    - Use only information from search results\n    - Include chunk IDs in cited_chunks for sources you use\n    - If information is insufficient, say so clearly\n    - Be concise - avoid unnecessary elaboration\n</code></pre>"},{"location":"configuration/prompts/#custom-synthesis-prompt","title":"Custom Synthesis Prompt","text":"<p>Replace the research report synthesis prompt by setting <code>prompts.synthesis</code>. This controls how the multi-agent research workflow generates its final report.</p> <p>The prompt should produce a <code>ResearchReport</code> with: <code>title</code>, <code>executive_summary</code>, <code>main_findings</code>, <code>conclusions</code>, <code>recommendations</code>, <code>limitations</code>, and <code>sources_summary</code>.</p> <p>Example:</p> <pre><code>prompts:\n  synthesis: |\n    Generate a research report based on the gathered evidence.\n\n    Output format:\n    - title: 5-12 word title\n    - executive_summary: 3-5 sentence overview\n    - main_findings: 4-8 bullet points of key findings\n    - conclusions: 2-4 bullet points\n    - recommendations: 2-5 actionable recommendations\n    - limitations: 1-3 limitations or gaps\n    - sources_summary: Brief description of sources used\n\n    Guidelines:\n    - Base all content strictly on collected evidence\n    - Be specific and objective\n    - Avoid meta-commentary like \"This report covers...\"\n</code></pre>"},{"location":"configuration/prompts/#picture-description-prompt","title":"Picture Description Prompt","text":"<p>Customize the prompt used when generating VLM descriptions for embedded images during document conversion. This prompt is sent to the configured Vision Language Model for each image.</p> <p>Default prompt:</p> <pre><code>Describe this image for a blind user. State the image type (screenshot, chart, photo, etc.),\nwhat it depicts, any visible text, and key visual details. Be concise and accurate.\n</code></pre> <p>Custom example:</p> <pre><code>prompts:\n  picture_description: |\n    Describe this image for a document search system.\n    Focus on: image type, main content, any text, key visual elements.\n    Be concise and factual.\n</code></pre> <p>The prompt is used when <code>processing.conversion_options.picture_description.enabled</code> is <code>true</code>. See Picture Description (VLM) for full configuration.</p>"},{"location":"configuration/prompts/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from haiku.rag.config import AppConfig\nfrom haiku.rag.config.models import PromptsConfig\n\nconfig = AppConfig(\n    prompts=PromptsConfig(\n        domain_preamble=\"You are answering questions about our product documentation.\",\n        qa=None,  # Use default QA prompt\n        synthesis=None,  # Use default synthesis prompt\n        picture_description=\"Describe this image for search indexing.\",\n    )\n)\n</code></pre>"},{"location":"configuration/providers/","title":"Providers","text":"<p>haiku.rag supports multiple AI providers for embeddings, question answering, and reranking. This guide covers provider-specific configuration and setup.</p> <p>Note</p> <p>You can use a <code>.env</code> file in your project directory to set environment variables like <code>OLLAMA_BASE_URL</code> and API keys (e.g., <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>). These will be automatically loaded when running <code>haiku-rag</code> commands.</p>"},{"location":"configuration/providers/#model-settings","title":"Model Settings","text":"<p>Configure model behavior for <code>qa</code> and <code>research</code> workflows. These settings apply to any provider that supports them.</p>"},{"location":"configuration/providers/#basic-settings","title":"Basic Settings","text":"<pre><code>qa:\n  model:\n    provider: ollama\n    name: gpt-oss\n    temperature: 0.7\n    max_tokens: 500\n</code></pre> <p>Available options:</p> <ul> <li>temperature: Sampling temperature (0.0-1.0+)</li> <li>Lower (0.0-0.3): Deterministic, focused responses</li> <li>Medium (0.4-0.7): Balanced</li> <li>Higher (0.8-1.0+): Creative, varied responses</li> <li>max_tokens: Maximum tokens in response</li> <li>enable_thinking: Control reasoning behavior (see below)</li> <li>base_url: Custom endpoint for OpenAI-compatible servers (vLLM, LM Studio, etc.)</li> </ul>"},{"location":"configuration/providers/#thinking-control","title":"Thinking Control","text":"<p>The <code>enable_thinking</code> setting controls whether models use explicit reasoning steps before answering.</p> <pre><code>qa:\n  model:\n    enable_thinking: false  # Faster responses\n\nresearch:\n  model:\n    enable_thinking: true   # Deeper reasoning\n</code></pre> <p>Values: - <code>false</code>: Disable reasoning for faster responses - <code>true</code>: Enable reasoning for complex tasks - Not set: Use model defaults</p> <p>Provider support:</p> <p>See the Pydantic AI thinking documentation for detailed provider support. haiku.rag supports thinking control for:</p> <ul> <li>OpenAI: Reasoning models (o1, o3, gpt-oss)</li> <li>Anthropic: All Claude models</li> <li>Google: Gemini models with thinking support</li> <li>Groq: Models with reasoning capabilities</li> <li>Bedrock: Claude, OpenAI, and Qwen models</li> <li>Ollama: Models supporting reasoning (gpt-oss, etc.)</li> <li>vLLM: Models supporting reasoning (gpt-oss, etc.)</li> <li>LM Studio: Models supporting reasoning (gpt-oss, etc.)</li> </ul> <p>When to use: - Disable for simple queries, RAG workflows, speed-critical applications - Enable for complex reasoning, mathematical problems, research tasks</p>"},{"location":"configuration/providers/#embedding-providers","title":"Embedding Providers","text":"<p>Embedding models require three settings: <code>provider</code>, <code>name</code>, and <code>vector_dim</code>. Optionally, use <code>base_url</code> for OpenAI-compatible servers.</p>"},{"location":"configuration/providers/#ollama-default","title":"Ollama (Default)","text":"<pre><code>embeddings:\n  model:\n    provider: ollama\n    name: mxbai-embed-large\n    vector_dim: 1024\n</code></pre> <p>The Ollama base URL can be configured in your config file or via environment variable:</p> <pre><code>providers:\n  ollama:\n    base_url: http://localhost:11434\n</code></pre> <p>Or via environment variable:</p> <pre><code>export OLLAMA_BASE_URL=http://localhost:11434\n</code></pre> <p>If not configured, it defaults to <code>http://localhost:11434</code>.</p>"},{"location":"configuration/providers/#voyageai","title":"VoyageAI","text":"<p>If you installed <code>haiku.rag</code> (full package), VoyageAI is already included. If you installed <code>haiku.rag-slim</code>, install with VoyageAI extras:</p> <pre><code>uv pip install haiku.rag-slim[voyageai]\n</code></pre> <pre><code>embeddings:\n  model:\n    provider: voyageai\n    name: voyage-3.5\n    vector_dim: 1024\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export VOYAGE_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/providers/#openai","title":"OpenAI","text":"<p>OpenAI embeddings are included in the default installation:</p> <pre><code>embeddings:\n  model:\n    provider: openai\n    name: text-embedding-3-small  # or text-embedding-3-large\n    vector_dim: 1536\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export OPENAI_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/providers/#cohere","title":"Cohere","text":"<p>Cohere embeddings are available via pydantic-ai:</p> <pre><code>embeddings:\n  model:\n    provider: cohere\n    name: embed-v4.0\n    vector_dim: 1024\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export CO_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/providers/#sentencetransformers","title":"SentenceTransformers","text":"<p>For local embeddings using HuggingFace models:</p> <pre><code>embeddings:\n  model:\n    provider: sentence-transformers\n    name: all-MiniLM-L6-v2\n    vector_dim: 384\n</code></pre>"},{"location":"configuration/providers/#openai-compatible-servers-vllm-lm-studio-etc","title":"OpenAI-Compatible Servers (vLLM, LM Studio, etc.)","text":"<p>For local inference servers with OpenAI-compatible APIs, use the <code>openai</code> provider with a custom <code>base_url</code>:</p> <pre><code># vLLM example\nembeddings:\n  model:\n    provider: openai\n    name: mixedbread-ai/mxbai-embed-large-v1\n    vector_dim: 512\n    base_url: http://localhost:8000/v1\n\n# LM Studio example\nembeddings:\n  model:\n    provider: openai\n    name: text-embedding-qwen3-embedding-4b\n    vector_dim: 2560\n    base_url: http://localhost:1234/v1\n</code></pre> <p>Note: The <code>base_url</code> must include the <code>/v1</code> path for OpenAI-compatible endpoints.</p>"},{"location":"configuration/providers/#question-answering-providers","title":"Question Answering Providers","text":"<p>Configure which LLM provider to use for question answering. Any provider and model supported by Pydantic AI can be used.</p>"},{"location":"configuration/providers/#ollama-default_1","title":"Ollama (Default)","text":"<pre><code>qa:\n  model:\n    provider: ollama\n    name: gpt-oss\n</code></pre> <p>The Ollama base URL can be configured via the <code>OLLAMA_BASE_URL</code> environment variable, config file, or defaults to <code>http://localhost:11434</code>:</p> <pre><code>export OLLAMA_BASE_URL=http://localhost:11434\n</code></pre> <p>Or in your config file:</p> <pre><code>providers:\n  ollama:\n    base_url: http://localhost:11434\n</code></pre>"},{"location":"configuration/providers/#openai_1","title":"OpenAI","text":"<p>OpenAI QA is included in the default installation:</p> <pre><code>qa:\n  model:\n    provider: openai\n    name: gpt-4o-mini  # or gpt-4, gpt-3.5-turbo, etc.\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export OPENAI_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/providers/#anthropic","title":"Anthropic","text":"<p>Anthropic QA is included in the default installation:</p> <pre><code>qa:\n  model:\n    provider: anthropic\n    name: claude-3-5-haiku-20241022  # or claude-3-5-sonnet-20241022, etc.\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export ANTHROPIC_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/providers/#openai-compatible-servers-vllm-lm-studio-etc_1","title":"OpenAI-Compatible Servers (vLLM, LM Studio, etc.)","text":"<p>For local inference servers with OpenAI-compatible APIs, use the <code>openai</code> provider with a custom <code>base_url</code>:</p> <pre><code># vLLM example\nqa:\n  model:\n    provider: openai\n    name: Qwen/Qwen3-4B\n    base_url: http://localhost:8002/v1\n\n# LM Studio example\nqa:\n  model:\n    provider: openai\n    name: gpt-oss-20b\n    base_url: http://localhost:1234/v1\n    enable_thinking: false\n</code></pre> <p>Note: The server must be running with a model that supports tool calling. The <code>base_url</code> must include the <code>/v1</code> path.</p>"},{"location":"configuration/providers/#other-providers","title":"Other Providers","text":"<p>Any provider supported by Pydantic AI can be used. Examples:</p> <pre><code># Google Gemini\nqa:\n  model:\n    provider: gemini\n    name: gemini-1.5-flash\n\n# Groq\nqa:\n  model:\n    provider: groq\n    name: llama-3.3-70b-versatile\n\n# Mistral\nqa:\n  model:\n    provider: mistral\n    name: mistral-small-latest\n</code></pre> <p>See the Pydantic AI documentation for the complete list of supported providers and models.</p>"},{"location":"configuration/providers/#reranking-providers","title":"Reranking Providers","text":"<p>Reranking improves search quality by re-ordering the initial search results using specialized models. When enabled, the system retrieves more candidates (10x the requested limit) and then reranks them to return the most relevant results.</p> <p>Reranking is disabled by default (<code>provider: \"\"</code>) for faster searches. You can enable it by configuring one of the providers below.</p>"},{"location":"configuration/providers/#mixedbread-ai","title":"MixedBread AI","text":"<p>If you installed <code>haiku.rag</code> (full package), MxBAI is already included. If you installed <code>haiku.rag-slim</code>, add the mxbai extra:</p> <pre><code>uv pip install haiku.rag-slim[mxbai]\n</code></pre> <p>Then configure:</p> <pre><code>reranking:\n  model:\n    provider: mxbai\n    name: mixedbread-ai/mxbai-rerank-base-v2\n</code></pre>"},{"location":"configuration/providers/#cohere_1","title":"Cohere","text":"<p>If you installed <code>haiku.rag</code> (full package), Cohere is already included. If you installed <code>haiku.rag-slim</code>, add the cohere extra:</p> <pre><code>uv pip install haiku.rag-slim[cohere]\n</code></pre> <p>Then configure:</p> <pre><code>reranking:\n  model:\n    provider: cohere\n    name: rerank-v3.5\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export CO_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/providers/#zero-entropy","title":"Zero Entropy","text":"<p>If you installed <code>haiku.rag</code> (full package), Zero Entropy is already included. If you installed <code>haiku.rag-slim</code>, add the zeroentropy extra:</p> <pre><code>uv pip install haiku.rag-slim[zeroentropy]\n</code></pre> <p>Then configure:</p> <pre><code>reranking:\n  model:\n    provider: zeroentropy\n    name: zerank-1  # Currently the only available model\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export ZEROENTROPY_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/providers/#vllm","title":"vLLM","text":"<p>For high-performance local reranking using dedicated reranking models:</p> <pre><code>reranking:\n  model:\n    provider: vllm\n    name: mixedbread-ai/mxbai-rerank-base-v2\n    base_url: http://localhost:8001\n</code></pre> <p>Note: vLLM reranking uses the <code>/v1/rerank</code> API endpoint. You need to run a vLLM server separately with a reranking model loaded.</p>"},{"location":"configuration/providers/#jina-ai","title":"Jina AI","text":"<p>Jina provides high-quality reranking with two deployment options: API mode and local inference.</p>"},{"location":"configuration/providers/#api-mode","title":"API Mode","text":"<p>Use the Jina Reranker API for cloud-based reranking:</p> <pre><code>reranking:\n  model:\n    provider: jina\n    name: jina-reranker-v3\n</code></pre> <p>Set your API key via environment variable:</p> <pre><code>export JINA_API_KEY=your-api-key\n</code></pre>"},{"location":"configuration/providers/#local-mode","title":"Local Mode","text":"<p>For local inference, install the jina extra:</p> <pre><code>uv pip install haiku.rag-slim[jina]\n</code></pre> <p>Then configure:</p> <pre><code>reranking:\n  model:\n    provider: jina-local\n    name: jinaai/jina-reranker-v3\n</code></pre> <p>Note: The Jina Reranker v3 local model is licensed under CC BY-NC 4.0, which restricts commercial use. For commercial applications, use the API mode instead.</p>"},{"location":"configuration/qa-research/","title":"Search and Question Answering","text":""},{"location":"configuration/qa-research/#search-settings","title":"Search Settings","text":"<p>Configure search behavior and context expansion:</p> <pre><code>search:\n  limit: 5                     # Default number of results to return\n  context_radius: 0            # DocItems before/after to include for text content\n  max_context_items: 10        # Maximum items in expanded context\n  max_context_chars: 10000     # Maximum characters in expanded context\n</code></pre> <ul> <li>limit: Default number of search results to return when no limit is specified. Used by CLI, MCP server, QA, and research workflows. Default: 5</li> <li>context_radius: For text content (paragraphs), includes N DocItems before and after. Set to 0 to disable expansion (default).</li> <li>max_context_items: Limits how many document items (paragraphs, list items, etc.) can be included in expanded context. Default: 10.</li> <li>max_context_chars: Hard limit on total characters in expanded content. Default: 10000.</li> </ul> <p>Structural content (tables, code blocks, lists) uses type-aware expansion that automatically includes the complete structure regardless of how it was chunked.</p> <p>Reranking behavior</p> <p>When a reranker is configured, search automatically retrieves 10x the requested limit, then reranks to return the final count. This improves result quality without requiring you to adjust <code>limit</code>.</p>"},{"location":"configuration/qa-research/#question-answering-configuration","title":"Question Answering Configuration","text":"<p>Configure the QA workflow:</p> <pre><code>qa:\n  model:\n    provider: ollama\n    name: gpt-oss\n    enable_thinking: false\n  max_iterations: 2     # Maximum search iterations\n  max_concurrency: 1    # Concurrent search operations\n</code></pre> <ul> <li>model: LLM configuration (see Providers)</li> <li>max_iterations: Maximum search iterations (default: 2)</li> <li>max_concurrency: Number of concurrent search operations (default: 1)</li> </ul>"},{"location":"configuration/qa-research/#research-configuration","title":"Research Configuration","text":"<p>Configure the multi-agent research workflow:</p> <pre><code>research:\n  model:\n    provider: \"\"            # Empty to use qa settings\n    name: \"\"               # Empty to use qa model\n    enable_thinking: false\n  max_iterations: 3\n  max_concurrency: 1\n</code></pre> <ul> <li>model: LLM configuration. Leave provider/model empty to inherit from <code>qa</code> (see Providers)</li> <li>max_iterations: Maximum planning/search iterations (default: 3)</li> <li>max_concurrency: Concurrent search operations (default: 1)</li> </ul> <p>The research workflow uses an iterative feedback loop: the planner proposes one question at a time, sees the answer, then decides whether to continue or synthesize. This continues until the planner marks research as complete or <code>max_iterations</code> is reached.</p>"},{"location":"configuration/qa-research/#rlm-configuration","title":"RLM Configuration","text":"<p>Configure the RLM (Recursive Language Model) agent:</p> <pre><code>rlm:\n  model:\n    provider: anthropic\n    name: claude-sonnet-4-20250514\n  code_timeout: 60.0      # Max seconds for code execution\n  max_output_chars: 50000 # Truncate output after this many chars\n</code></pre> <ul> <li>model: LLM configuration (see Providers)</li> <li>code_timeout: Maximum seconds for each code execution (default: 60)</li> <li>max_output_chars: Truncate code output after this many characters (default: 50000)</li> </ul> <p>See RLM Agent for usage details.</p>"},{"location":"configuration/storage/","title":"Database and Storage","text":""},{"location":"configuration/storage/#local-storage","title":"Local Storage","text":"<p>By default, <code>haiku.rag</code> uses a local LanceDB database:</p> <pre><code>storage:\n  data_dir: /path/to/data  # Empty = use default platform location\n  auto_vacuum: true  # Enable automatic vacuuming after operations\n  vacuum_retention_seconds: 86400  # Cleanup threshold in seconds\n</code></pre> <ul> <li>data_dir: Directory for local database storage. When empty, uses platform-specific default locations</li> <li>auto_vacuum: When enabled (default), automatically runs vacuum after document create/update operations and database rebuilds. Set to <code>false</code> to disable automatic vacuuming and rely on manual <code>haiku-rag vacuum</code> commands only. Disabling can help avoid potential crashes in high-concurrency scenarios</li> <li>vacuum_retention_seconds: When vacuum runs, old table versions older than this threshold are removed. Default: 86400 seconds (1 day). Set to 0 for aggressive cleanup (removes all old versions immediately)</li> </ul> <p>Vacuum Retention Threshold</p> <p>The <code>vacuum_retention_seconds</code> value should be larger than the typical time it takes to process and write a document. If a concurrent operation is in progress while vacuum runs, setting this value too low can cause race conditions where vacuum removes table versions that an in-flight operation still needs. The default of 86400 seconds (1 day) is conservative and safe for most use cases.</p>"},{"location":"configuration/storage/#remote-storage","title":"Remote Storage","text":"<p>For remote storage, use the <code>lancedb</code> settings with various backends:</p> <pre><code># LanceDB Cloud\nlancedb:\n  uri: db://your-database-name\n  api_key: your-api-key\n  region: us-west-2  # optional\n\n# Amazon S3\nlancedb:\n  uri: s3://my-bucket/my-table\n# Use AWS credentials or IAM roles\n\n# Azure Blob Storage\nlancedb:\n  uri: az://my-container/my-table\n# Use Azure credentials\n\n# Google Cloud Storage\nlancedb:\n  uri: gs://my-bucket/my-table\n# Use GCP credentials\n\n# HDFS\nlancedb:\n  uri: hdfs://namenode:port/path/to/table\n</code></pre> <p>Authentication is handled through standard cloud provider credentials (AWS CLI, Azure CLI, gcloud, etc.) or by setting <code>api_key</code> for LanceDB Cloud.</p> <p>Note: Table optimization is automatically handled by LanceDB Cloud (<code>db://</code> URIs) and is disabled for better performance. For object storage backends (S3, Azure, GCS), optimization is still performed locally.</p>"},{"location":"configuration/storage/#database-creation","title":"Database Creation","text":"<p>Databases must be explicitly created before use:</p> <p>CLI: <pre><code># Create in default location (see Configuration File Locations below)\nhaiku-rag init\n\n# Create at custom path\nhaiku-rag init --db /path/to/database.lancedb\n</code></pre></p> <p>Python: <pre><code># Create at custom path\nasync with HaikuRAG(\"/path/to/database.lancedb\", create=True) as client:\n    ...\n\n# Create in default location\nasync with HaikuRAG(create=True) as client:\n    ...\n</code></pre></p> <p>The default location is platform-specific (e.g., <code>~/Library/Application Support/haiku.rag/</code> on macOS).</p> <p>Operations on non-existent databases raise <code>FileNotFoundError</code>. This prevents accidental database creation from typos or misconfigured paths.</p>"},{"location":"configuration/storage/#vector-indexing","title":"Vector Indexing","text":"<p>Configure vector search settings:</p> <pre><code>search:\n  vector_index_metric: cosine  # cosine, l2, or dot\n  vector_refine_factor: 30     # Re-ranking factor for accuracy\n</code></pre> <p>For search behavior settings (<code>limit</code>, <code>context_radius</code>, <code>max_context_items</code>, <code>max_context_chars</code>), see QA and Research.</p> <ul> <li>vector_index_metric: Distance metric for vector similarity:</li> <li><code>cosine</code>: Cosine similarity (default, best for most embeddings)</li> <li><code>l2</code>: Euclidean distance</li> <li><code>dot</code>: Dot product similarity</li> <li>vector_refine_factor: Improves accuracy when using a vector index by retrieving <code>refine_factor * limit</code> candidates (using approximate search) and re-ranking them with exact distances. Higher values increase accuracy but slow down queries. Default: 30</li> <li>Only applies with a vector index - has no effect on brute-force search, which already returns exact results</li> </ul> <p>Note</p> <p>Vector indexes are only necessary for large datasets with over 100,000 chunks. For smaller datasets, LanceDB's brute-force kNN search provides exact results with good performance. Only create an index if you notice search performance degradation on large datasets.</p> <p>Index creation:</p> <p>Vector indexes are not created automatically during document ingestion to avoid slowing down the process. After you've added documents (at least 256 chunks required), create the index manually:</p> <pre><code>haiku-rag create-index\n</code></pre> <p>This command: - Checks if you have enough data (minimum 256 chunks) - Creates an IVF_PQ index for fast approximate nearest neighbor (ANN) search - Uses LanceDB's automatic parameter calculation based on your dataset size and vector dimensions</p> <p>Re-indexing:</p> <p>Indexes are not automatically updated when you add new documents. After adding a significant amount of new data:</p> <pre><code>haiku-rag create-index  # Rebuilds the index with all data\n</code></pre> <p>Searches still work with stale indexes - LanceDB uses the index for old data (fast ANN) and brute-force kNN for new unindexed rows, then combines the results. However, performance degrades as more unindexed data accumulates.</p> <p>For datasets with fewer than 256 chunks, searches use brute-force kNN scans (exact nearest neighbors, 100% recall) which work well for small datasets but don't scale beyond a few hundred thousand vectors.</p>"},{"location":"skills/","title":"Skills","text":"<p>haiku.rag exposes its RAG capabilities as haiku.skills skills. Skills are self-contained units that bundle tools, instructions, and state \u2014 they can be composed into any pydantic-ai agent via <code>SkillToolset</code>.</p>"},{"location":"skills/#available-skills","title":"Available Skills","text":"Skill Description <code>rag</code> Search, retrieve, and answer questions from the knowledge base <code>rag-rlm</code> Computational analysis via code execution"},{"location":"skills/#discovery","title":"Discovery","text":"<p>Skills are registered as Python entrypoints under <code>haiku.skills</code>. They are discovered automatically by <code>haiku.skills</code>:</p> <pre><code>haiku-skills list --use-entrypoints\n# rag \u2014 Search, retrieve and analyze documents using RAG.\n# rag-rlm \u2014 Analyze documents using code execution in a sandboxed interpreter.\n</code></pre>"},{"location":"skills/#usage","title":"Usage","text":"<pre><code>from haiku.rag.skills.rag import create_skill\nfrom haiku.skills.agent import SkillToolset\nfrom haiku.skills.prompts import build_system_prompt\nfrom pydantic_ai import Agent\n\nskill = create_skill(db_path=db_path, config=config)\ntoolset = SkillToolset(skills=[skill])\n\nagent = Agent(\n    \"openai:gpt-4o\",\n    instructions=build_system_prompt(toolset.skill_catalog),\n    toolsets=[toolset],\n)\n\nresult = await agent.run(\"What documents do we have?\")\n</code></pre>"},{"location":"skills/#database-path-resolution","title":"Database Path Resolution","text":"<p>Both skills resolve the database path in the same order:</p> <ol> <li><code>db_path</code> argument passed to <code>create_skill()</code></li> <li><code>HAIKU_RAG_DB</code> environment variable</li> <li>Config default (<code>config.storage.data_dir / \"haiku.rag.lancedb\"</code>)</li> </ol>"},{"location":"skills/#state-management","title":"State Management","text":"<p>Each skill manages its own state under a dedicated namespace. State is automatically synced via the AG-UI protocol when using <code>AGUIAdapter</code>.</p> <pre><code>rag_state = toolset.get_namespace(\"rag\")\nrlm_state = toolset.get_namespace(\"rlm\")\n</code></pre> <p>See the individual skill pages for state model details.</p>"},{"location":"skills/#ag-ui-streaming","title":"AG-UI Streaming","text":"<p>For web applications, use pydantic-ai's <code>AGUIAdapter</code> to stream tool calls, text, and state deltas:</p> <pre><code>from pydantic_ai.ag_ui import AGUIAdapter\n\nadapter = AGUIAdapter(agent=agent, run_input=run_input)\nevent_stream = adapter.run_stream()\nsse_event_stream = adapter.encode_stream(event_stream)\n</code></pre> <p>See the Web Application for a complete implementation.</p>"},{"location":"skills/rag/","title":"RAG Skill","text":"<p>The RAG skill is the primary way to use haiku.rag tools. It bundles search, Q&amp;A, document browsing, and research into a single skill with managed state.</p>"},{"location":"skills/rag/#create_skilldb_path-config","title":"<code>create_skill(db_path?, config?)</code>","text":"<pre><code>from haiku.rag.skills.rag import create_skill\n\nskill = create_skill(db_path=db_path, config=config)\n</code></pre> Parameter Default Description <code>db_path</code> <code>None</code> Path to LanceDB database. Falls back to <code>HAIKU_RAG_DB</code> env var, then config default. <code>config</code> <code>None</code> <code>AppConfig</code> instance. If None, uses <code>get_config()</code>."},{"location":"skills/rag/#tools","title":"Tools","text":"Tool Purpose <code>search(query, limit?)</code> Hybrid search (vector + full-text) with context expansion <code>list_documents(limit?, offset?, filter?)</code> Paginated document listing <code>get_document(query)</code> Retrieve a document by ID, title, or URI <code>ask(question)</code> Q&amp;A with citations via the QA agent <code>research(question)</code> Deep multi-agent research producing comprehensive reports"},{"location":"skills/rag/#state","title":"State","text":"<p>The skill manages a <code>RAGState</code> under the <code>\"rag\"</code> namespace:</p> <pre><code>class RAGState(BaseModel):\n    citations: list[Citation] = []\n    qa_history: list[QAHistoryEntry] = []\n    document_filter: str | None = None\n    searches: dict[str, list[SearchResult]] = {}\n    documents: list[DocumentInfo] = []\n    reports: list[ResearchEntry] = []\n</code></pre> <ul> <li>citations \u2014 Accumulated citations from <code>ask</code> calls, with sequential indexing across calls.</li> <li>qa_history \u2014 Questions and answers from <code>ask</code> calls. Prior Q&amp;A is used as context for follow-up questions when embeddings are similar.</li> <li>document_filter \u2014 SQL WHERE clause applied to <code>search</code>, <code>ask</code>, and <code>research</code> calls. Set this to scope queries to specific documents.</li> <li>searches \u2014 Search results keyed by query string.</li> <li>documents \u2014 Documents seen via <code>list_documents</code> or <code>get_document</code> (deduplicated by ID).</li> <li>reports \u2014 Research reports from <code>research</code> calls.</li> </ul>"},{"location":"skills/rlm/","title":"RLM Skill","text":"<p>The RLM (Recursive Language Model) skill provides computational analysis via code execution. It writes and runs Python code in a sandboxed interpreter to answer questions that require computation, aggregation, or data traversal.</p>"},{"location":"skills/rlm/#create_skilldb_path-config","title":"<code>create_skill(db_path?, config?)</code>","text":"<pre><code>from haiku.rag.skills.rlm import create_skill\n\nskill = create_skill(db_path=db_path, config=config)\n</code></pre> Parameter Default Description <code>db_path</code> <code>None</code> Path to LanceDB database. Falls back to <code>HAIKU_RAG_DB</code> env var, then config default. <code>config</code> <code>None</code> <code>AppConfig</code> instance. If None, uses <code>get_config()</code>."},{"location":"skills/rlm/#tools","title":"Tools","text":"Tool Purpose <code>analyze(question, document?, filter?)</code> Answer analytical questions using code execution <p>Parameters:</p> <ul> <li><code>question</code> \u2014 The analytical question to answer.</li> <li><code>document</code> \u2014 Optional document ID or title to pre-load for analysis.</li> <li><code>filter</code> \u2014 Optional SQL WHERE clause to filter documents.</li> </ul>"},{"location":"skills/rlm/#state","title":"State","text":"<p>The skill manages an <code>RLMState</code> under the <code>\"rlm\"</code> namespace:</p> <pre><code>class RLMState(BaseModel):\n    analyses: list[AnalysisEntry] = []\n\nclass AnalysisEntry(BaseModel):\n    question: str\n    answer: str\n    program: str | None = None\n</code></pre> <p>Each <code>analyze</code> call appends an <code>AnalysisEntry</code> with the question, answer, and executed program.</p>"},{"location":"skills/rlm/#usage-with-rag-skill","title":"Usage with RAG Skill","text":"<p>Combine both skills to give the agent full RAG + analysis capabilities:</p> <pre><code>from haiku.rag.skills.rag import create_skill as create_rag_skill\nfrom haiku.rag.skills.rlm import create_skill as create_rlm_skill\nfrom haiku.skills.agent import SkillToolset\nfrom haiku.skills.prompts import build_system_prompt\nfrom pydantic_ai import Agent\n\nrag = create_rag_skill(db_path=db_path)\nrlm = create_rlm_skill(db_path=db_path)\ntoolset = SkillToolset(skills=[rag, rlm])\n\nagent = Agent(\n    \"openai:gpt-4o\",\n    instructions=build_system_prompt(toolset.skill_catalog),\n    toolsets=[toolset],\n)\n</code></pre> <p>See the RLM Agent documentation for details on how the underlying agent works.</p>"}]}